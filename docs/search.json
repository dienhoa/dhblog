[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dhblog",
    "section": "",
    "text": "Why and How to Fine-tune CLIP\n\n\n\n\n\n\n\ndeeplearning\n\n\nclip\n\n\nmultimodal\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nDien-Hoa Truong\n\n\n\n\n\n\n  \n\n\n\n\nCryCeleb2023 Diary\n\n\n\n\n\n\n\ndeeplearning\n\n\naudio\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nDien-Hoa Truong\n\n\n\n\n\n\n  \n\n\n\n\nTransformer for timeseries\n\n\n\n\n\n\n\ntimeseries\n\n\ndeeplearning\n\n\ntransformer\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nDien-Hoa Truong\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Size DataLoader\n\n\n\n\n\n\n\ndeeplearning\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\nDien-Hoa Truong\n\n\n\n\n\n\n  \n\n\n\n\nRedesign your Training Loop with CallBacks\n\n\n\n\n\n\n\ndeeplearning\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nDien-Hoa Truong\n\n\n\n\n\n\n  \n\n\n\n\nCaptcha prediction - From CNN to CRNN\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\nDien-Hoa Truong\n\n\n\n\n\n\n  \n\n\n\n\nObject Detection from scratch - Single Shot Detector\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2022\n\n\nDien-Hoa Truong\n\n\n\n\n\n\n  \n\n\n\n\nClassification loss function as comparing 2 vectors\n\n\n\n\n\n\n\ndeeplearning\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2020\n\n\nDien-Hoa Truong\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/classification_loss_func.html",
    "href": "posts/classification_loss_func.html",
    "title": "Classification loss function as comparing 2 vectors",
    "section": "",
    "text": "Walking through a simple Dot-Product to Cross-Entropy then finally a completely new loss function"
  },
  {
    "objectID": "posts/classification_loss_func.html#introduction",
    "href": "posts/classification_loss_func.html#introduction",
    "title": "Classification loss function as comparing 2 vectors",
    "section": "Introduction",
    "text": "Introduction\nIn this blog post, I will try to explain the Classification Loss function from the perspective of comparing 2 vectors. We’ll start with the most simple loss: the dot-product of 2 vectors, with a simple modification we’ll have the Cross-Entropy Loss and then try to beat the result of Cross-Entropy with our one."
  },
  {
    "objectID": "posts/classification_loss_func.html#review-the-topic",
    "href": "posts/classification_loss_func.html#review-the-topic",
    "title": "Classification loss function as comparing 2 vectors",
    "section": "Review the topic",
    "text": "Review the topic\n\nHow to compare 2 vectors\nA vector is just an arrow with a direction and length. So for the binary classification problem, we have an output vector that has 2 elements and sum up to 1. [p1, p2] and p1+p2 = 1\nImagine we want our target vector to be [0,1]. The worst prediction is [0,1] and a good prediction could be [0.99,0.01]\n\n\n\nVector\n\n\nWe notice that \\(cos(\\theta)\\) for \\(\\theta\\) from 0° to 90° decreases strictly from 1 to 0 (from the best to worst prediction) so it can be an indicator for our prediction (And it exists - cosine similarity). Any function that has value increasing/decreasing strictly from the best prediction to the worst prediction can be considered a loss function\nThe dot-product has some relevance to the cosine mentioned above. The dot-product from a geometrical point of view is the projection of a vector to the direction of another vector and multiplying them both. And the projection is calculated by multiplying the cosine of the angle between these 2 vectors. But in this simple case, the projection is just the y value if our predicted vector is (x,y) and the target vector is (0,1). And the y value decreases strictly from 1 to 0 from the vector (1,0) to vector (0,1) . So the dot-product can also be a candidate for our loss function too\n\n\n\nDot-Product\n\n\nIn the multiclass classification problem with the target vector encoded by one-hot vector (Vector has just one 1 value and 0 for all others position). The dot-product calculation is very simple. Taking the value in the predicted vector at its position in the target vector, we have 1. (Dot-product in algebra is just the sum of the element-wise multiplication)\n\nv1 = np.array([0,1,0,0]) # target vector \nv2 = np.array([0.2,0.3,0.1,0.4]) # predicted vector\nprint(sum(v1*v2))\n\n0.3\n\n\nFor the Cross-Entropy Loss Function, instead of multiplying the predicted vector, we multiply the logarithm of the predicted vector\n\nprint(sum(v1*np.log(v2)))\nprint(np.log(0.3))\n\n-1.2039728043259361\n-1.2039728043259361\n\n\nIn the next section, we will experiment the dot-product loss function, the cross-entropy loss function and try to invent of own loss function by changing the function applying the the predicted vector (like logarithm in the case of Cross-Entropy)"
  },
  {
    "objectID": "posts/classification_loss_func.html#getting-data",
    "href": "posts/classification_loss_func.html#getting-data",
    "title": "Classification loss function as comparing 2 vectors",
    "section": "Getting Data",
    "text": "Getting Data\nThis part is simply for data preparation. Putting all the images and their labels into the corresponding dataloader\n\nfrom fastai2.vision.all import *\npath = untar_data(URLs.PETS)\nitems = get_image_files(path/'images')\n\ndef label_func(fname):\n    return \"cat\" if fname.name[0].isupper() else \"dog\"\n\nlabeller = RegexLabeller(pat=r\"(.+)_\\d+.jpg\")\n\n\npets = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                    get_items=get_image_files,\n                    splitter=RandomSplitter(),\n                    get_y = Pipeline([lambda x: getattr(x,'name'), labeller]),\n                    item_tfms=Resize(224), \n                    batch_tfms=aug_transforms(),\n                    )\n\n\ndls = pets.dataloaders(path/'images')\n\n\ndls.c # number of categories in this dataset\n\n37\n\n\n\ndls.show_batch()"
  },
  {
    "objectID": "posts/classification_loss_func.html#experimenting",
    "href": "posts/classification_loss_func.html#experimenting",
    "title": "Classification loss function as comparing 2 vectors",
    "section": "Experimenting",
    "text": "Experimenting\nAll our loss functions will have two parts. The first part is the softmax function - scaling our output to [0,1]. The second part is how we penalize our prediction - high loss if the predicted vector is far from the target.\n\nCross_Entropy loss\n\ndef softmax(x): return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)\ndef nl(input, target): return -input[range(target.shape[0]), target].log().mean()\ndef our_cross_entropy(input, target):\n    pred = softmax(input)\n    loss = nl(pred, target)\n    return loss\n\n\nlearn = cnn_learner(dls, resnet18, loss_func=our_cross_entropy, metrics=error_rate)\n\n\nlearn.fine_tune(1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.621736\n0.353662\n0.110284\n00:31\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.535891\n0.278063\n0.092016\n00:41\n\n\n\n\n\n\n\nDot Product Loss\nThis is a negative dot-production loss function because we multiply the result by -1 to make it increase from best to worst prediction\n\ndef dot_product_loss(input, target):\n    pred = softmax(input)\n    return -(pred[range(target.shape[0]), target]).mean()\n\n\nlearn = cnn_learner(dls, resnet18, loss_func=dot_product_loss, metrics=error_rate)\n\n\nlearn.fine_tune(1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n-0.485698\n-0.798422\n0.179973\n00:31\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n-0.778386\n-0.841704\n0.144790\n00:42\n\n\n\n\n\nWow ! despite the simplicity of the dot-product loss function, we got not so bad result (0.14) after 2 epochs. Our dataset has 37 categories of pets and a random prediction will give us the error rate (1-1/37)=0.97. Can we do it better ?\n\n\nThe difference between cross-entropy loss and dot-product loss\nHow these 2 loss functions penalize the prediction is described as below. The target vector is always [0,1]\n\nx = np.linspace(0.01,0.99,100) # the predicted vector at index 2\ny_dot_product = -x\ny_cross_entropy = -np.log(x)\n\n\nplt.plot(x, y_dot_product, label='dot_prod')\nplt.plot(x, y_cross_entropy, label='cross_entropy')\nplt.legend()\nplt.show()\n\n\n\n\nFrom the plot, we can see that the cross-entropy function penalizes more when we have a wrong prediction (kind of exponential shape)\nIn the next section, we will try others loss functions but the core idea is still based on the dot-product loss function.\n\n\nInverse Loss\nInstead of multiplying by -1, we can inverse the predicted value to make it increase from best to worst prediction. Let’s see the plot below:\n\ny_inv = 1/x\n\n\nplt.plot(x, y_dot_product, label='dot_prod')\nplt.plot(x, y_cross_entropy, label='cross_entropy')\nplt.plot(x, y_inv, label='inverse loss')\n\nplt.legend()\nplt.show()\n\n\n\n\nThe inverse loss penalizes may be too much compared to the 2 previous ones, no tolerance at all might be not so good. But let’s try it anyway\n\ndef inverse_loss(input, target):\n    pred = softmax(input)\n    return (1/((pred[range(target.shape[0]), target]))).mean()\n\n\nlearn = cnn_learner(dls, resnet18, loss_func=inverse_loss, metrics=error_rate)\n\n\nlearn.fine_tune(1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n447.855957\n11.976704\n0.466170\n00:19\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n50.260994\n5.570698\n0.258457\n00:24\n\n\n\n\n\nOk, we have s worst result. But with this idea, we can easily tune the loss function. We can power the denominator with a value &lt; 1 to decrease the penalization. For example 0.2\n\ny_inv_tuning = 1/(x**0.2)\n\n\nplt.plot(x, y_dot_product, label='dot_prod')\nplt.plot(x, y_cross_entropy, label='cross_entropy')\nplt.plot(x, y_inv_tuning, label='inverse loss tuning')\n\nplt.legend()\nplt.show()\n\n\n\n\nLet’s try this new loss function\n\ndef inverse_loss_tunning(input, target):\n    pred = softmax(input)\n    return (1/((pred[range(target.shape[0]), target]).pow(0.2))).mean()\n\n\nlearn = cnn_learner(dls, resnet18, loss_func=inverse_loss_tunning, metrics=error_rate)\n\n\nlearn.fine_tune(1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.583183\n1.099432\n0.125846\n00:31\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.174570\n1.070881\n0.091340\n00:42\n\n\n\n\n\nWe get not so different error rate: 0.091 compared to 0.092 of the cross-entropy loss function."
  },
  {
    "objectID": "posts/cryceleb.html",
    "href": "posts/cryceleb.html",
    "title": "CryCeleb2023 Diary",
    "section": "",
    "text": "My Experiments to winning the CryCeleb Competition (Audio Verification)\nThis blog post is to document my journey to winning the CryCeleb competition hosted by HuggingFace and Ubenwa, which focuses on verifying babies using their cry sound. However, I must admit that:\nWhy?\nAdvice for future competitions:"
  },
  {
    "objectID": "posts/cryceleb.html#final-experiment",
    "href": "posts/cryceleb.html#final-experiment",
    "title": "CryCeleb2023 Diary",
    "section": "Final Experiment",
    "text": "Final Experiment\n\n!pip install -qq speechbrain\n!pip install -qq seaborn\n!pip install -Uqq huggingface_hub\n!pip install -Uqq fastai\n!pip install -Uqq wandb\n\n\nimport speechbrain as sb\nfrom speechbrain.pretrained import SpeakerRecognition, EncoderClassifier\nfrom speechbrain.dataio.dataio import read_audio\nfrom speechbrain.utils.metric_stats import EER\nimport pandas as pd\nimport numpy as np\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom huggingface_hub import hf_hub_download\nfrom tqdm.notebook import tqdm\nimport random\nfrom speechbrain.processing.features import InputNormalization\nfrom speechbrain.lobes.features import Fbank\nimport seaborn as sns\nfrom itertools import combinations, product\n\n\nfrom fastai.vision.all import *\nfrom torch.utils.data import Dataset, DataLoader\n\n\nimport wandb\nfrom fastai.callback.wandb import *\nwandb.init(\"cryceleb\")\n\nwandb: Currently logged in as: dhoa. Use `wandb login --relogin` to force relogin\n\n\nTracking run with wandb version 0.15.5\n\n\nRun data is saved locally in /home/wandb/run-20230710_211533-s4lz49ck\n\n\nSyncing run hearty-bush-20 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/dhoa/uncategorized\n\n\n View run at https://wandb.ai/dhoa/uncategorized/runs/s4lz49ck\n\n\nDisplay W&B run\n\n\n\n# read metadata\nmetadata = pd.read_csv(f'metadata.csv', dtype={'baby_id':str, 'chronological_index':str})\ndev_metadata = metadata.loc[metadata['split']=='dev'].copy()\n# read sample submission\nsample_submission = pd.read_csv(f\"sample_submission.csv\") # scores are unfiorm random\n# read verification pairs\ndev_pairs = pd.read_csv(f\"dev_pairs.csv\", dtype={'baby_id_B':str, 'baby_id_D':str})\ntest_pairs = pd.read_csv(f\"test_pairs.csv\")\n\ndisplay(metadata.head().style.set_caption(f\"metadata\").set_table_styles([{'selector': 'caption','props': [('font-size', '20px')]}]))\ndisplay(dev_pairs.head().style.set_caption(f\"dev_pairs\").set_table_styles([{'selector': 'caption','props': [('font-size', '20px')]}]))\ndisplay(test_pairs.head().style.set_caption(f\"test_pairs\").set_table_styles([{'selector': 'caption','props': [('font-size', '20px')]}]))\ndisplay(sample_submission.head().style.set_caption(f\"sample_submission\").set_table_styles([{'selector': 'caption','props': [('font-size', '20px')]}]))\n\n\n\n\nmetadata\n\n\n \nbaby_id\nperiod\nduration\nsplit\nchronological_index\nfile_name\nfile_id\n\n\n\n\n0\n0694\nB\n1.320000\ndev\n000\naudio/dev/0694/B/0694_B_000.wav\n0694_B_000\n\n\n1\n0694\nB\n0.940000\ndev\n001\naudio/dev/0694/B/0694_B_001.wav\n0694_B_001\n\n\n2\n0694\nB\n0.880000\ndev\n002\naudio/dev/0694/B/0694_B_002.wav\n0694_B_002\n\n\n3\n0694\nB\n1.130000\ndev\n003\naudio/dev/0694/B/0694_B_003.wav\n0694_B_003\n\n\n4\n0694\nB\n1.180000\ndev\n004\naudio/dev/0694/B/0694_B_004.wav\n0694_B_004\n\n\n\n\n\n\n\n\ndev_pairs\n\n\n \nbaby_id_B\nbaby_id_D\nid\nlabel\n\n\n\n\n0\n0133\n0611\n0133B_0611D\n0\n\n\n1\n0593\n0584\n0593B_0584D\n0\n\n\n2\n0094\n0292\n0094B_0292D\n0\n\n\n3\n0563\n0094\n0563B_0094D\n0\n\n\n4\n0122\n0694\n0122B_0694D\n0\n\n\n\n\n\n\n\n\ntest_pairs\n\n\n \nbaby_id_B\nbaby_id_D\nid\n\n\n\n\n0\nanonymous027\nanonymous212\nanonymous027B_anonymous212D\n\n\n1\nanonymous035\nanonymous225\nanonymous035B_anonymous225D\n\n\n2\nanonymous029\nanonymous288\nanonymous029B_anonymous288D\n\n\n3\nanonymous001\nanonymous204\nanonymous001B_anonymous204D\n\n\n4\nanonymous075\nanonymous244\nanonymous075B_anonymous244D\n\n\n\n\n\n\n\n\nsample_submission\n\n\n \nid\nscore\n\n\n\n\n0\nanonymous027B_anonymous212D\n0.548814\n\n\n1\nanonymous035B_anonymous225D\n0.715189\n\n\n2\nanonymous029B_anonymous288D\n0.602763\n\n\n3\nanonymous001B_anonymous204D\n0.544883\n\n\n4\nanonymous075B_anonymous244D\n0.423655"
  },
  {
    "objectID": "posts/cryceleb.html#ubenwa-baseline",
    "href": "posts/cryceleb.html#ubenwa-baseline",
    "title": "CryCeleb2023 Diary",
    "section": "Ubenwa Baseline",
    "text": "Ubenwa Baseline\nI reproduced the baseline from code, excluding the config file. This facilitated easier customization of the code according to my needs, and allowed for the use of a more high level framework like fastai. The following are some code snippets from speechbrain for the ecapa model.\n\nclass InputNormalizationFixedSize(InputNormalization):\n    def forward(self, x):\n        N_batches = x.shape[0]\n\n        current_means = []\n        current_stds = []\n\n        for snt_id in range(N_batches):\n\n            # Avoiding padded time steps\n            actual_size = torch.round(torch.tensor(1) * x.shape[1]).int()\n\n            # computing statistics\n            current_mean, current_std = self._compute_current_stats(\n                x[snt_id, 0:actual_size, ...]\n            )\n\n            current_means.append(current_mean)\n            current_stds.append(current_std)\n\n            if self.norm_type == \"sentence\":\n\n                x[snt_id] = (x[snt_id] - current_mean.data) / current_std.data\n        return x\n\n\nnormalizer = InputNormalizationFixedSize(norm_type='sentence', std_norm=False)\n\n\nfeature_maker = Fbank(deltas=False,\n                    n_mels=80,\n                    left_frames=0,\n                    right_frames=0,\n                    )\n\n\ndataset_path = './'\nmetadata = pd.read_csv(\n    f\"{dataset_path}/metadata.csv\", dtype={\"baby_id\": str, \"chronological_index\": str}\n)\ndev_metadata = metadata.loc[metadata[\"split\"] == \"dev\"].copy()\nsample_submission = pd.read_csv(\n    f\"{dataset_path}/sample_submission.csv\"\n) \ndev_pairs = pd.read_csv(\n    f\"{dataset_path}/dev_pairs.csv\", dtype={\"baby_id_B\": str, \"baby_id_D\": str}\n)\ntest_pairs = pd.read_csv(f\"{dataset_path}/test_pairs.csv\")\n\n\nencoder = SpeakerRecognition.from_hparams(\n    source=\"Ubenwa/ecapa-voxceleb-ft-cryceleb\",\n    savedir=f\"ecapa-voxceleb-ft-cryceleb\",\n    run_opts={\"device\":\"cuda\"} #comment out if no GPU available\n)\n\n\nembedding_model = encoder.mods.embedding_model\n\n\ndef shuffle_group_and_concat(x, n=8):\n    \"\"\" Shuffle sound data per row and concat \"\"\"\n    concatenated_results = []\n    for _ in range(n):\n        shuffled_values = x.values.copy()\n        random.shuffle(shuffled_values)\n        concatenated = np.concatenate(shuffled_values)\n        tensor_length = concatenated.shape[0]\n        \n        if tensor_length &lt; 16000*3:\n            raw_audio = np.tile(concatenated, math.ceil(16000*7 / tensor_length))\n\n        concatenated = concatenated[:random.randint(16000*3, 16000*15)]\n        concatenated_results.append(concatenated)\n    return concatenated_results\n\n\ndef compute_cosine_similarity_score(row, cry_dict):\n    \"\"\" Average scores for all possible pairs \"\"\"\n    cos = torch.nn.CosineSimilarity(dim=-1)\n    encoded_cry_B = cry_dict[(row['baby_id_B'], 'B')]['cry_encoded']\n    encoded_cry_D = cry_dict[(row['baby_id_D'], 'D')]['cry_encoded']\n    \n    similarity_scores = []\n    for tensor_B in encoded_cry_B:\n        for tensor_D in encoded_cry_D:\n            similarity_score = cos(tensor_B, tensor_D)\n            similarity_scores.append(similarity_score.item())\n    return sum(similarity_scores) / len(similarity_scores)\n\n\ndev_metadata = metadata.loc[metadata['split']=='dev'].copy()\ndev_metadata['cry'] = dev_metadata.apply(lambda row: read_audio(row['file_name']).numpy(), axis=1)\ngrouped_data = dev_metadata.groupby(['baby_id', 'period'])['cry']\ncry_dict = {}\nfor key, group in grouped_data:\n    cry_dict[key] = {'cry': shuffle_group_and_concat(group, 7)}\n\n\ndef encode(embedding_model, item):\n    \"\"\" Encoding audio for ECAPA model including: Feature_maker, Normalizer, Embedding Model \"\"\"\n    is_training = embedding_model.training\n    if is_training: embedding_model.eval()\n    item = item.unsqueeze(0)\n    feats = feature_maker(item.cuda())\n    feats = normalizer(feats)\n    embeddings = embedding_model(feats)\n    \n    if is_training: embedding_model.train()\n    return embeddings\n\n\ndef compute_eer_and_plot_verification_scores(pairs_df, plot=True):\n    ''' pairs_df must have 'score' and 'label' columns'''\n    positive_scores = pairs_df.loc[pairs_df['label']==1]['score'].values\n    negative_scores = pairs_df.loc[pairs_df['label']==0]['score'].values\n    eer, threshold = EER(torch.tensor(positive_scores), torch.tensor(negative_scores))\n    if plot:\n        ax = sns.histplot(pairs_df, x='score', hue='label', stat='percent', common_norm=False)\n        ax.set_title(f'EER={round(eer, 4)} - Thresh={round(threshold, 4)}')\n        plt.axvline(x=[threshold], color='red', ls='--');\n        print(eer)\n    return eer, threshold\n\n\ndef get_eer(embedding_model, cry_dict, df, tta_nb = 8, plot=True, ):\n    \n    embedding_model.eval()\n    for key, group in grouped_data:\n        cry_dict[key] = {'cry': shuffle_group_and_concat(group, tta_nb)}\n    with torch.no_grad():\n        embedding_model = embedding_model\n        if plot:\n            loop = tqdm(cry_dict.items())\n        else:\n            loop = cry_dict.items()            \n        for (baby_id, period), d in loop:\n            cry_array = d['cry']\n            cry_encoded_list = []\n            for row in cry_array:\n                encoded_row = encode(embedding_model.cuda(), torch.tensor(row).cuda())\n                encoded_row = encoded_row.cpu()\n                cry_encoded_list.append(encoded_row)\n\n            d['cry_encoded'] = cry_encoded_list\n    df['score'] = dev_pairs.apply(lambda row: compute_cosine_similarity_score(row=row, cry_dict=cry_dict), axis=1)\n    eer, threshold = compute_eer_and_plot_verification_scores(pairs_df=df, plot=plot)\n    embedding_model.train()\n\n    return eer, threshold\n\n\nget_eer(embedding_model, cry_dict, dev_pairs)\n\n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/SpectralOps.cpp:862.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n\n\n0.22499999403953552\n\n\n(0.22499999403953552, 0.07767139354837127)"
  },
  {
    "objectID": "posts/cryceleb.html#dataset",
    "href": "posts/cryceleb.html#dataset",
    "title": "CryCeleb2023 Diary",
    "section": "Dataset",
    "text": "Dataset\n\ndef get_pair_item(item):\n    if item.name == 'B':\n        pair_name = 'D'\n    else:\n        pair_name = 'B'\n    return item.parent / pair_name\n\n\nfiles = metadata['file_name'].values\nfolders = [Path(file).parent for file in files]\nfolders = list(set(folders))\n\n\nlen(folders)\n\n1334\n\n\n\ndef read_audio_from_list(paths):\n    raw_audios_aug = []\n    random.shuffle(paths)\n    raw_audio = torch.concat([read_audio(str(filename)) for filename in paths])\n    tensor_length = raw_audio.shape[0]\n    if tensor_length &lt; 16000*3:\n        raw_audio = raw_audio.repeat(math.ceil(16000*5 / tensor_length))\n    return raw_audio\n\n\ntrain_folders = [folder for folder in folders if folder.parent.parent.name in ['train']]\ndev_folders = [folder for folder in folders if folder.parent.parent.name in ['dev']]\nfull_folders = train_folders + dev_folders\npair_folders = [folder for folder in train_folders if get_pair_item(folder).exists()]\n\n\nlen(full_folders) ,len(train_folders), len(dev_folders), len(pair_folders)\n\n(1014, 934, 80, 696)\n\n\n\n### get both id from train and dev\nbaby_ids = metadata[(metadata['split'] == 'train')]['baby_id'].unique()\nbaby_ids = np.sort(baby_ids)\nn_classes = len(baby_ids)\nprint(n_classes)\n\n586\n\n\n\nid2idx = {baby_id: index for index, baby_id in enumerate(baby_ids)}\n\n\nfiles = metadata['file_name'].values\nfolders = [Path(file).parent for file in files]\nfolders = list(set(folders))\n\n\nfull_folders = [folder for folder in folders if folder.parent.parent.name in ['train', 'dev']]\ntrain_folders_pairs = [folder for folder in train_folders if get_pair_item(folder).exists()]\nbaby_ids_pairs = list(set([folder.parent.name for folder in train_folders_pairs]))\nvalid_folders = [Path('audio/train')/_id/ random.choice(['B','D']) for _id in baby_ids_pairs ]\ntrain_folders = [folder for folder in train_folders if folder not in valid_folders]\n\n\nlen(full_folders) ,len(train_folders), len(dev_folders), len(pair_folders)\n\n(1014, 586, 80, 696)\n\n\n\ntrain_meta = metadata[metadata['split'] == 'train']\ndev_meta = metadata[metadata['split'] == 'dev']\ntest_meta = metadata[metadata['split'] == 'test']\n\n\n# train_folders_not_pairs = [folder for folder in train_folders if not get_pair_item(folder).exists()]\n\n\ndef read_audio_from_folder(folder, shuffle = True):\n    if shuffle == True:\n        files = [str(filename) for filename in folder.ls().shuffle() if filename.suffix == '.wav']\n    else:\n        files = [str(filename) for filename in folder.ls() if filename.suffix == '.wav']\n    raw_audio = torch.concat([read_audio(str(filename)) for filename in files])\n    tensor_length = raw_audio.shape[0]\n    if tensor_length &lt; 16000*3:\n        raw_audio = raw_audio.repeat(math.ceil(16000*6 / tensor_length))\n    return raw_audio\n\n\ndef get_label(folder):\n    return id2idx[folder.parent.name]\n\n\nclass CryCelebDset(Dataset):\n    def __init__(self,\n                 items):\n        super(CryCelebDset, self).__init__()\n        self.items = items\n        \n    def __len__(self):\n        return len(self.items)\n    \n    def __getitem__(self, i):\n        item = self.items[i]\n        audio = read_audio_from_folder(item)\n        label = get_label(item)\n        return audio, torch.Tensor([label]).long()\n\n\ndef collate_fn(batch):\n    audios, labels = zip(*batch)\n    target_lenth = min(audio.shape[0] for audio in audios)\n    \n    target_lenth = target_lenth if target_lenth &lt; 16000*3 else min(target_lenth, random.randint(16000*3, 16000*8))\n\n    audios = [audio[:target_lenth] for audio in audios]\n    \n    return torch.stack(audios), torch.stack(labels)\n\n\n# train_dset = CryCelebDset(train_folders)\ntrain_dset = CryCelebDset(train_folders)\nvalid_dset = CryCelebDset(train_folders[:2]) # Validation is no use here, it is just a hack to make fastai work\n\n\nlen(train_dset), len(valid_dset)\n\n(586, 2)\n\n\n\ntrain_loader = DataLoader(train_dset, batch_size=16, shuffle=True,  collate_fn=collate_fn)\nvalid_loader = DataLoader(valid_dset, batch_size=32, shuffle=False,  collate_fn=collate_fn)\n\n\ndls = DataLoaders(train_loader, valid_loader)"
  },
  {
    "objectID": "posts/cryceleb.html#model",
    "href": "posts/cryceleb.html#model",
    "title": "CryCeleb2023 Diary",
    "section": "Model",
    "text": "Model\nThe model here shares the same architecture as the ECAPA model, but there are some differences in the hyperparameters, such as n_mels = 150, lin_neurons = 250. My intention was to experiment with a larger version of the default ECAPA model.\nThere is a minor modification in the loss function: I added label_smoothing with a value of 0.05.\n\nimport torch.nn.functional as F\nfrom torch import nn\n\nclass Classifier(nn.Module):\n\n    def __init__(\n        self,\n        input_size,\n        device=\"cpu\",\n        lin_neurons=192,\n        out_neurons=1211,\n        dropout_rate=0.5,  # add a dropout_rate parameter\n    ):\n\n        super().__init__()\n\n        self.linear = nn.Linear(input_size, lin_neurons)\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Final Layer\n        self.weight = nn.Parameter(\n            torch.FloatTensor(out_neurons, lin_neurons).to(device)\n        )\n        nn.init.xavier_uniform_(self.weight)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.dropout(x)\n\n        # Need to be normalized\n        x = F.linear(F.normalize(x.squeeze(1)), F.normalize(self.weight))\n        return x.unsqueeze(1)\n\n\ndef is_model_frozen(model):\n    return all(not param.requires_grad for param in model.parameters())\n\ndef unfreeze_model(model):\n    for param in model.parameters():\n        param.requires_grad = True\n\nunfreeze_model(embedding_model)\nprint(is_model_frozen(embedding_model))  \n\nFalse\n\n\n\nn_mels = 150\nlin_neurons = 250\n\n# n_mels = 80\n# lin_neurons = 192\n\nfeature_maker = sb.lobes.features.Fbank(deltas=False,\n                                        n_mels=n_mels,\n                                        left_frames=0,\n                                        right_frames=0,\n                                        )\n\nembedding_model = sb.lobes.models.ECAPA_TDNN.ECAPA_TDNN(input_size=n_mels,\n                                                        channels=[1024, 1024, 1024, 1024, 3072],\n                                                        kernel_sizes=[5, 3, 3, 3, 1],\n                                                        dilations=[1, 2, 3, 4, 1],\n                                                        groups=[1, 1, 1, 1, 1],\n                                                        attention_channels=128,\n                                                        lin_neurons=lin_neurons\n                                                        )\n\nclassifier = sb.lobes.models.ECAPA_TDNN.Classifier(input_size=lin_neurons,\n                        out_neurons=n_classes,\n                      )\nclass Model(nn.Module):\n    def __init__(self, feature_maker, normalizer, embedding_model, classifier):\n        super(Model, self).__init__()\n        self.feature_maker = feature_maker\n        self.normalizer = normalizer\n        self.embedding_model = embedding_model\n        self.classifier = classifier\n    \n    def forward(self, x):\n        feats = self.feature_maker(x)\n        feats = self.normalizer(feats)\n        embeddings = self.embedding_model(feats)\n        classifier_outputs = self.classifier(embeddings)\n        return classifier_outputs\n    \nmodel = Model(feature_maker = feature_maker,\n              normalizer=normalizer,\n              embedding_model=embedding_model,\n              classifier=classifier)\n\n\nclass LogSoftmaxWrapperSmoothing(nn.Module):\n    def __init__(self, loss_fn, smoothing=0.05):  # add a smoothing parameter\n        super(LogSoftmaxWrapperSmoothing, self).__init__()\n        self.loss_fn = loss_fn\n        self.criterion = torch.nn.KLDivLoss(reduction=\"sum\")\n        self.smoothing = smoothing  # store the smoothing value\n\n    def forward(self, outputs, targets, length=None):\n        outputs = outputs.squeeze(1)\n        targets = targets.squeeze(1)\n        targets = F.one_hot(targets.long(), outputs.shape[1]).float()\n\n        # Apply label smoothing\n        targets = (1 - self.smoothing) * targets + self.smoothing / outputs.shape[1]\n\n        try:\n            predictions = self.loss_fn(outputs, targets)\n        except TypeError:\n            predictions = self.loss_fn(outputs)\n\n        predictions = F.log_softmax(predictions, dim=1)\n        loss = self.criterion(predictions, targets) / targets.sum()\n        return loss\n    \nloss_base = sb.nnet.losses.AdditiveAngularMargin(margin=0.2, scale=30)\ncrit = LogSoftmaxWrapperSmoothing(loss_base)\ndef loss_fn(preds, targets):\n    return crit(preds, targets)\n\n\ndef eer_metric(preds, targs):\n    # The eer metric here is not related to the validation set but the dev set\n    eer, threshold = get_eer(embedding_model, cry_dict, dev_pairs, plot=False)\n    return eer\n\n\nlearner = Learner(dls, model, loss_func=loss_fn, metrics=[eer_metric], cbs=WandbCallback())\n\n\nlearner.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=9.120108734350652e-05)\n\n\n\n\n\n\nlearner.fit_one_cycle(150, 2e-4)\n\nCould not gather input dimensions\nWandbCallback was not able to prepare a DataLoader for logging prediction samples -&gt; 'DataLoader' object has no attribute 'new'\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\neer_metric\ntime\n\n\n\n\n0\n12.819111\n9.852398\n0.397115\n00:21\n\n\n1\n11.732524\n9.378596\n0.369231\n00:20\n\n\n2\n10.952858\n6.739351\n0.375000\n00:19\n\n\n3\n10.304280\n8.235709\n0.350000\n00:18\n\n\n4\n9.576555\n6.657913\n0.375000\n00:18\n\n\n5\n8.874806\n7.752380\n0.350962\n00:18\n\n\n6\n8.028608\n4.957489\n0.325000\n00:17\n\n\n7\n7.302641\n5.177752\n0.301923\n00:17\n\n\n8\n6.496997\n4.656638\n0.350000\n00:17\n\n\n9\n5.806657\n5.101495\n0.325000\n00:17\n\n\n10\n4.984231\n2.045478\n0.350000\n00:18\n\n\n11\n4.510298\n1.314584\n0.327244\n00:18\n\n\n12\n3.988340\n0.315158\n0.326603\n00:17\n\n\n13\n3.477479\n5.352105\n0.325000\n00:17\n\n\n14\n3.133040\n0.237394\n0.320833\n00:17\n\n\n15\n2.831013\n1.140466\n0.350000\n00:17\n\n\n16\n2.446104\n0.798410\n0.352244\n00:18\n\n\n17\n2.167508\n0.486705\n0.325000\n00:17\n\n\n18\n2.059879\n1.214511\n0.325000\n00:17\n\n\n19\n1.854602\n2.260158\n0.350000\n00:17\n\n\n20\n1.853107\n0.357835\n0.300000\n00:17\n\n\n21\n1.740630\n1.374100\n0.325000\n00:17\n\n\n22\n1.724865\n0.170533\n0.325000\n00:18\n\n\n23\n1.595109\n0.166785\n0.297756\n00:17\n\n\n24\n1.476745\n0.489945\n0.300000\n00:17\n\n\n25\n1.364003\n0.830093\n0.347436\n00:17\n\n\n26\n1.368073\n0.172278\n0.325000\n00:17\n\n\n27\n1.400348\n0.264813\n0.350000\n00:17\n\n\n28\n1.357518\n0.213733\n0.354487\n00:17\n\n\n29\n1.350938\n0.121906\n0.306090\n00:17\n\n\n30\n1.298094\n0.797013\n0.275000\n00:17\n\n\n31\n1.163949\n0.125197\n0.325000\n00:18\n\n\n32\n0.992551\n0.494040\n0.303846\n00:18\n\n\n33\n0.929511\n0.315667\n0.350000\n00:17\n\n\n34\n0.970226\n0.303382\n0.326603\n00:17\n\n\n35\n1.001400\n0.403191\n0.325000\n00:17\n\n\n36\n1.044940\n0.573968\n0.325000\n00:17\n\n\n37\n0.906172\n0.145333\n0.300000\n00:17\n\n\n38\n0.842299\n0.281313\n0.304167\n00:17\n\n\n39\n0.871641\n0.217045\n0.350000\n00:17\n\n\n40\n0.777124\n0.131117\n0.300000\n00:17\n\n\n41\n0.705075\n0.207996\n0.276282\n00:17\n\n\n42\n0.711332\n0.606886\n0.300000\n00:17\n\n\n43\n0.756493\n0.504772\n0.325000\n00:17\n\n\n44\n0.703369\n0.255737\n0.325000\n00:18\n\n\n45\n0.670267\n1.306089\n0.276282\n00:17\n\n\n46\n0.645874\n0.156841\n0.325000\n00:17\n\n\n47\n0.648763\n0.117524\n0.325000\n00:17\n\n\n48\n0.561911\n0.229732\n0.300000\n00:17\n\n\n49\n0.529512\n0.191458\n0.295192\n00:17\n\n\n50\n0.462782\n0.167080\n0.277244\n00:18\n\n\n51\n0.496077\n0.106041\n0.275000\n00:17\n\n\n52\n0.501537\n0.292876\n0.277244\n00:17\n\n\n53\n0.470838\n0.200016\n0.306090\n00:17\n\n\n54\n0.492161\n0.140463\n0.256090\n00:17\n\n\n55\n0.490079\n0.254963\n0.256090\n00:17\n\n\n56\n0.452705\n0.186381\n0.277885\n00:17\n\n\n57\n0.422567\n0.180647\n0.274359\n00:18\n\n\n58\n0.458263\n0.916696\n0.272756\n00:17\n\n\n59\n0.445988\n0.176482\n0.250000\n00:17\n\n\n60\n0.449376\n0.134938\n0.300000\n00:17\n\n\n61\n0.398278\n0.454310\n0.274359\n00:18\n\n\n62\n0.378900\n0.136589\n0.300000\n00:18\n\n\n63\n0.397175\n0.266560\n0.346795\n00:17\n\n\n64\n0.419594\n0.166931\n0.275000\n00:17\n\n\n65\n0.408431\n0.162984\n0.322436\n00:18\n\n\n66\n0.429785\n0.227120\n0.344872\n00:17\n\n\n67\n0.380197\n0.282507\n0.325000\n00:17\n\n\n68\n0.361336\n0.403791\n0.299359\n00:18\n\n\n69\n0.348556\n0.501471\n0.277244\n00:18\n\n\n70\n0.360931\n0.358086\n0.275000\n00:17\n\n\n71\n0.336917\n0.388072\n0.350000\n00:17\n\n\n72\n0.335198\n0.326303\n0.275000\n00:18\n\n\n73\n0.318928\n0.230708\n0.350000\n00:17\n\n\n74\n0.337089\n0.224751\n0.300000\n00:17\n\n\n75\n0.347762\n0.228684\n0.295192\n00:17\n\n\n76\n0.347869\n0.283976\n0.275000\n00:17\n\n\n77\n0.328355\n0.148460\n0.300000\n00:17\n\n\n78\n0.301195\n0.314673\n0.300000\n00:18\n\n\n79\n0.309706\n0.219821\n0.330769\n00:18\n\n\n80\n0.313028\n0.351390\n0.375000\n00:17\n\n\n81\n0.311351\n0.203537\n0.374038\n00:17\n\n\n82\n0.278428\n0.227288\n0.326603\n00:17\n\n\n83\n0.300168\n0.120307\n0.300000\n00:17\n\n\n84\n0.291518\n0.273817\n0.305128\n00:18\n\n\n85\n0.286973\n0.276371\n0.324679\n00:17\n\n\n86\n0.292313\n0.201308\n0.300000\n00:18\n\n\n87\n0.286953\n0.333281\n0.279167\n00:17\n\n\n88\n0.278925\n0.292981\n0.321154\n00:17\n\n\n89\n0.254643\n0.312703\n0.254487\n00:18\n\n\n90\n0.256090\n0.102150\n0.275000\n00:17\n\n\n91\n0.256988\n0.243337\n0.250000\n00:17\n\n\n92\n0.247612\n1.446797\n0.250000\n00:18\n\n\n93\n0.240916\n0.104841\n0.300000\n00:17\n\n\n94\n0.239867\n0.203033\n0.300000\n00:18\n\n\n95\n0.250558\n0.213792\n0.300000\n00:17\n\n\n96\n0.225012\n0.146971\n0.325000\n00:18\n\n\n97\n0.250202\n0.144672\n0.322436\n00:18\n\n\n98\n0.252809\n0.223730\n0.300000\n00:18\n\n\n99\n0.247364\n0.127707\n0.303846\n00:17\n\n\n100\n0.236257\n0.200906\n0.300000\n00:17\n\n\n101\n0.215899\n0.222088\n0.275000\n00:18\n\n\n102\n0.211499\n0.264478\n0.325000\n00:17\n\n\n103\n0.220157\n0.327042\n0.278846\n00:17\n\n\n104\n0.217344\n0.180306\n0.251282\n00:17\n\n\n105\n0.211273\n0.191087\n0.325000\n00:18\n\n\n106\n0.225987\n0.226452\n0.324679\n00:18\n\n\n107\n0.234045\n0.162532\n0.300000\n00:17\n\n\n108\n0.216443\n0.187482\n0.275000\n00:18\n\n\n109\n0.211505\n0.156684\n0.276603\n00:18\n\n\n110\n0.210457\n0.178876\n0.275000\n00:17\n\n\n111\n0.203583\n0.097995\n0.280128\n00:17\n\n\n112\n0.200213\n0.150823\n0.300000\n00:18\n\n\n113\n0.205306\n0.091030\n0.275000\n00:17\n\n\n114\n0.195609\n0.419262\n0.253205\n00:17\n\n\n115\n0.184044\n0.157910\n0.269231\n00:18\n\n\n116\n0.182212\n0.235523\n0.275000\n00:18\n\n\n117\n0.177163\n0.177653\n0.275000\n00:18\n\n\n118\n0.181401\n0.236190\n0.269551\n00:17\n\n\n119\n0.175148\n0.269990\n0.269231\n00:17\n\n\n120\n0.176068\n0.224078\n0.319872\n00:18\n\n\n121\n0.185972\n0.157348\n0.275000\n00:17\n\n\n122\n0.174520\n0.167444\n0.300000\n00:18\n\n\n123\n0.177205\n0.214876\n0.300000\n00:18\n\n\n124\n0.166557\n0.090736\n0.297756\n00:18\n\n\n125\n0.167413\n0.265629\n0.325000\n00:17\n\n\n126\n0.164617\n0.261985\n0.325000\n00:18\n\n\n127\n0.167717\n0.223148\n0.256090\n00:17\n\n\n128\n0.160176\n0.159316\n0.325000\n00:17\n\n\n129\n0.155260\n0.159374\n0.300000\n00:18\n\n\n130\n0.154295\n0.176301\n0.272115\n00:17\n\n\n131\n0.149302\n0.210373\n0.275000\n00:17\n\n\n132\n0.156900\n0.168842\n0.300000\n00:17\n\n\n133\n0.150621\n0.171651\n0.274038\n00:17\n\n\n134\n0.150258\n0.184598\n0.278526\n00:18\n\n\n135\n0.152160\n0.148298\n0.325000\n00:17\n\n\n136\n0.150087\n0.219485\n0.274038\n00:17\n\n\n137\n0.147386\n0.202868\n0.302244\n00:17\n\n\n138\n0.147530\n0.129326\n0.273077\n00:17\n\n\n139\n0.145539\n0.096450\n0.296474\n00:17\n\n\n140\n0.145980\n0.111623\n0.300000\n00:18\n\n\n141\n0.145903\n0.138313\n0.300000\n00:17\n\n\n142\n0.150013\n0.089761\n0.275000\n00:18\n\n\n143\n0.154749\n0.164044\n0.275000\n00:17\n\n\n144\n0.149258\n0.143600\n0.280449\n00:17\n\n\n145\n0.145224\n0.149808\n0.300000\n00:18\n\n\n146\n0.140290\n0.104018\n0.277885\n00:18\n\n\n147\n0.142366\n0.178131\n0.293910\n00:17\n\n\n148\n0.149232\n0.173077\n0.275000\n00:17\n\n\n149\n0.145681\n0.137975\n0.297115\n00:17"
  },
  {
    "objectID": "posts/cryceleb.html#submission",
    "href": "posts/cryceleb.html#submission",
    "title": "CryCeleb2023 Diary",
    "section": "Submission",
    "text": "Submission\n\nembedding_model.eval()\ntest_metadata = metadata.loc[metadata['split']=='test'].copy()\ntest_metadata['cry'] = test_metadata.apply(lambda row: read_audio(row['file_name']).numpy(), axis=1)\ngrouped_data = test_metadata.groupby(['baby_id', 'period'])['cry']\ncry_dict_test = {}\nfor key, group in grouped_data:\n    cry_dict_test[key] = {'cry': shuffle_group_and_concat(group, 7)}\n\nwith torch.no_grad():\n\n    for (baby_id, period), d in tqdm(cry_dict_test.items()):\n        cry_array = d['cry']\n        cry_encoded_list = []\n\n        for row in cry_array:\n            encoded_row = encode(embedding_model.cuda(), torch.tensor(row).cuda())\n            cry_encoded_list.append(encoded_row)\n\n        d['cry_encoded'] = cry_encoded_list\n    \ntest_pairs['score'] = test_pairs.apply(lambda row: compute_cosine_similarity_score(row=row, cry_dict=cry_dict_test), axis=1)\n\n\n\n\n\n#submission must match the 'sample_submission.csv' format exactly\nmy_submission= test_pairs[['id', 'score']]\nmy_submission.to_csv('my_submission.csv', index=False)\ndisplay(my_submission.head())\n\n\n\n\n\n\n\n\nid\nscore\n\n\n\n\n0\nanonymous027B_anonymous212D\n-0.120739\n\n\n1\nanonymous035B_anonymous225D\n-0.063723\n\n\n2\nanonymous029B_anonymous288D\n0.026014\n\n\n3\nanonymous001B_anonymous204D\n-0.170518\n\n\n4\nanonymous075B_anonymous244D\n0.125677"
  },
  {
    "objectID": "posts/finetune_clip.html",
    "href": "posts/finetune_clip.html",
    "title": "Why and How to Fine-tune CLIP",
    "section": "",
    "text": "Why and How to Fine-tune CLIP for remote sensing images\nTraditional Image Classification models sometimes struggle with generalization in real-life situations. Moreover, labeling is a significant challenge when training these models, making it difficult for them to generalize across all cases. Enter CLIP (Text-Image pairing model), which benefits from recent developments in NLP (using Transformers) and the billions of image captions available on the Internet.\nCLIP demonstrates less accuracy degradation in real-world scenarios compared to previous methods and introduces various exciting applications, such as searching for images using text, zero-shot learning classification, and more.\nHowever, like many preceding deep learning models, even when CLIP is trained on an enormous dataset, it can encounter difficulties if there’s a mismatch between the domain of the inference data and the training data.\nIn this blog post, you’ll discover that CLIP, by default, doesn’t perform very well on remote-sensing datasets (images captured by satellites) and how we fine-tune the CLIP model using this new dataset.\n!pip install -Uqq accelerate\n!pip install -Uqq transformers\nimport transformers\ntransformers.__version__\n\n'4.33.2'"
  },
  {
    "objectID": "posts/finetune_clip.html#how-clip-works",
    "href": "posts/finetune_clip.html#how-clip-works",
    "title": "Why and How to Fine-tune CLIP",
    "section": "How CLIP works",
    "text": "How CLIP works\nIn a nutshell, the CLIP model leverages two pretrained models for text and image. It fine-tunes them in such a way that their embedding outputs for similar concepts become as close as possible\n\n\n\nHow CLIP model works"
  },
  {
    "objectID": "posts/finetune_clip.html#imports",
    "href": "posts/finetune_clip.html#imports",
    "title": "Why and How to Fine-tune CLIP",
    "section": "Imports",
    "text": "Imports\n\nimport os\nimport datasets\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport matplotlib.pyplot as plt\nimport requests\nimport random\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom PIL import Image\nfrom torchvision.io import ImageReadMode, read_image\nfrom torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize\nfrom torchvision.transforms.functional import InterpolationMode\nfrom pdb import set_trace\n\nimport transformers\nfrom transformers import (\n    VisionTextDualEncoderProcessor,\n    VisionTextDualEncoderModel,\n    AutoImageProcessor,\n    AutoModel,\n    AutoTokenizer,\n    HfArgumentParser,\n    Trainer,\n    TrainingArguments,\n    set_seed,\n)\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom transformers.utils import check_min_version, send_example_telemetry\nfrom transformers.utils.versions import require_version\n\nimport warnings\nwarnings.filterwarnings('ignore', category=UserWarning, module='torchvision')\n\n# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\ncheck_min_version(\"4.31.0.dev0\")\nrequire_version(\"datasets&gt;=1.8.0\", \"To fix: pip install -r examples/pytorch/contrastive-image-text/requirements.txt\")"
  },
  {
    "objectID": "posts/finetune_clip.html#arguments",
    "href": "posts/finetune_clip.html#arguments",
    "title": "Why and How to Fine-tune CLIP",
    "section": "Arguments",
    "text": "Arguments\nWe define two argument classes: ModelArguments and HfArgumentParser. This allows us to utilize Hugging Face’s HfArgumentParser in conjunction with the default TrainingArguments from Hugging Face.\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    image_processor_name: str = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n    cache_dir: Optional[str] = field(\n        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n                \"with private models).\"\n            )\n        },\n    )\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    data_dir: Optional[str] = field(default=None, metadata={\"help\": \"The data directory containing input files.\"})\n    image_column: Optional[str] = field(\n        default=\"image_path\",\n        metadata={\"help\": \"The name of the column in the datasets containing the full image file paths.\"},\n    )\n    caption_column: Optional[str] = field(\n        default=\"caption\",\n        metadata={\"help\": \"The name of the column in the datasets containing the image captions.\"},\n    )\n    max_seq_length: Optional[int] = field(\n        default=128,\n        metadata={\n            \"help\": (\n                \"The maximum total input sequence length after tokenization. Sequences longer \"\n                \"than this will be truncated, sequences shorter will be padded.\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n\n\nargs_dict = {'output_dir': './clip-roberta-finetuned',\n 'model_name_or_path': './clip-roberta',\n 'data_dir': './data',\n 'dataset_name': 'arampacha/rsicd',\n 'image_column': 'image',\n 'caption_column': 'captions',\n 'remove_unused_columns': False,\n 'per_device_train_batch_size': 64,\n 'per_device_eval_batch_size': 64,\n 'learning_rate': 5e-05,\n 'warmup_steps': 0,\n 'weight_decay': 0.1,\n 'overwrite_output_dir': True,\n 'push_to_hub': False}\n\nparser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\nmodel_args, data_args, training_args = parser.parse_dict(args_dict)\n\n\nmodel_args, data_args\n\n(ModelArguments(model_name_or_path='./clip-roberta', config_name=None, tokenizer_name=None, image_processor_name=None, cache_dir=None, model_revision='main', use_fast_tokenizer=True, use_auth_token=False),\n DataTrainingArguments(dataset_name='arampacha/rsicd', data_dir='./data', image_column='image', caption_column='captions', max_seq_length=128, overwrite_cache=False, preprocessing_num_workers=None))"
  },
  {
    "objectID": "posts/finetune_clip.html#dataset-preparation",
    "href": "posts/finetune_clip.html#dataset-preparation",
    "title": "Why and How to Fine-tune CLIP",
    "section": "Dataset Preparation",
    "text": "Dataset Preparation\n\nclass Transform(torch.nn.Module):\n    def __init__(self, image_size, mean, std):\n        super().__init__()\n        self.transforms = torch.nn.Sequential(\n            Resize([image_size], interpolation=InterpolationMode.BICUBIC, antialias=True),\n            CenterCrop(image_size),\n            ConvertImageDtype(torch.float),\n            Normalize(mean, std),\n        )\n    def forward(self, x) -&gt; torch.Tensor:\n        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n        with torch.no_grad():\n            x = self.transforms(x)\n        return x\n\n\ndef collate_fn(examples):\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n    return {\n        \"pixel_values\": pixel_values,\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"return_loss\": True,\n    }\n\nBelow is the remote sensing dataset that we use in this blog post\n\ndataset = datasets.load_dataset(\"arampacha/rsicd\")\n\nFound cached dataset parquet (/home/.cache/huggingface/datasets/arampacha___parquet/arampacha--rsicd-56e24d6cc63cb9d9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n\n\n\n\n\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['filename', 'captions', 'image'],\n        num_rows: 8734\n    })\n    test: Dataset({\n        features: ['filename', 'captions', 'image'],\n        num_rows: 1093\n    })\n    valid: Dataset({\n        features: ['filename', 'captions', 'image'],\n        num_rows: 1094\n    })\n})\n\n\nLet’s see examples of this dataset\n\ndef show_images(dset, num_images=8, without_caption=True,num_columns=2,img_size=(4, 4)):\n    num_rows = -(-num_images // num_columns)  # Ceiling division\n    fig = plt.figure(figsize=(img_size[0] * num_columns, img_size[1] * num_rows))\n\n    _list = list(range(len(dset)))\n    for i in range(num_images):\n        index = _list[i]\n        ax = fig.add_subplot(num_rows, num_columns, i+1)\n        image = dset[index]['image']\n        plt.imshow(image)\n        \n        # Set title as the first caption\n        if without_caption:\n            caption = dset[index]['captions'][0]\n            ax.set_title(caption, fontsize=10)\n        \n        # Remove axis\n        plt.axis('off')\n    plt.tight_layout()\n    plt.subplots_adjust(wspace=0.5, hspace=0.01)  # Adjust these values as needed\n\n    plt.show()\n\n\nshow_images(dataset['train'], num_images=8, without_caption=True)"
  },
  {
    "objectID": "posts/finetune_clip.html#model-preparation",
    "href": "posts/finetune_clip.html#model-preparation",
    "title": "Why and How to Fine-tune CLIP",
    "section": "Model Preparation",
    "text": "Model Preparation\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n)\n\n\nimage_processor = AutoImageProcessor.from_pretrained(\n    model_args.image_processor_name or model_args.model_name_or_path,\n    cache_dir=model_args.cache_dir,\n    revision=model_args.model_revision,\n    use_auth_token=True if model_args.use_auth_token else None,\n)\n\nmodel = AutoModel.from_pretrained(\n    model_args.model_name_or_path,\n    cache_dir=model_args.cache_dir,\n    revision=model_args.model_revision,\n    use_auth_token=True if model_args.use_auth_token else None,\n)\nconfig = model.config\n\nTo ensure reproducible output, we should set the seed.\n\nset_seed(training_args.seed)\n\n\nimage_transformations = Transform(\n    config.vision_config.image_size, image_processor.image_mean, image_processor.image_std\n)\nimage_transformations = torch.jit.script(image_transformations)\n\n\ndef tokenize_captions(examples):\n    captions = [example[0] for example in examples[data_args.caption_column]]\n    text_inputs = tokenizer(captions, max_length=data_args.max_seq_length, padding=\"max_length\", truncation=True)\n    examples[\"input_ids\"] = text_inputs.input_ids\n    examples[\"attention_mask\"] = text_inputs.attention_mask\n    return examples\n\ndef transform_images(examples):\n    images = [torch.tensor(np.array(image)).permute(2, 0, 1) for image in examples[data_args.image_column]]\n    examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n    return examples\n\ndef filter_corrupt_images(examples):\n    \"\"\"remove problematic images\"\"\"\n    valid_images = []\n    for image_file in examples[data_args.image_column]:\n        try:\n            Image.open(image_file)\n            valid_images.append(True)\n        except Exception:\n            valid_images.append(False)\n    return valid_images\n\n\ntrain_dataset = dataset[\"train\"]\ntrain_dataset = train_dataset.map(\n    function=tokenize_captions,\n    batched=True,\n    num_proc=data_args.preprocessing_num_workers,\n    load_from_cache_file=not data_args.overwrite_cache,\n    desc=\"Running tokenizer on train dataset\",\n)\ntrain_dataset.set_transform(transform_images)\n\nLoading cached processed dataset at /home/.cache/huggingface/datasets/arampacha___parquet/arampacha--rsicd-56e24d6cc63cb9d9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-01af25c2d3c15faa.arrow\nParameter 'transform'=&lt;function transform_images&gt; of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['filename', 'captions', 'image', 'input_ids', 'attention_mask'],\n    num_rows: 8734\n})\n\n\n\neval_dataset = dataset[\"valid\"]\neval_dataset = eval_dataset.map(\n    function=tokenize_captions,\n    batched=True,\n    num_proc=data_args.preprocessing_num_workers,\n    load_from_cache_file=not data_args.overwrite_cache,\n    desc=\"Running tokenizer on validation dataset\",\n)\neval_dataset.set_transform(transform_images)\n\n\n\n\n\ntrain_dataset, eval_dataset\n\n(Dataset({\n     features: ['filename', 'captions', 'image', 'input_ids', 'attention_mask'],\n     num_rows: 8734\n }),\n Dataset({\n     features: ['filename', 'captions', 'image', 'input_ids', 'attention_mask'],\n     num_rows: 1094\n }))\n\n\n\nprocessor =  VisionTextDualEncoderProcessor(image_processor, tokenizer)\n\nWe have a straightforward example below (sourced from Hugging Face) to quickly demonstrate how the CLIP model works by default. There are two images: the first one is of a cat and the second is of a dog. We will use the text “a photo of a cat” and determine which picture has the highest probability.\n\nurls = [\n    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n    \"https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg\",\n]\nimages = [Image.open(requests.get(url, stream=True).raw) for url in urls]\ninputs = processor(\n    text=[\"a photo of a cat\"], images=images, return_tensors=\"pt\", padding=True\n)\ninputs['input_ids'] = inputs['input_ids'].cuda()\ninputs['attention_mask'] = inputs['attention_mask'].cuda()\ninputs['pixel_values'] = inputs['pixel_values'].cuda()\nmodel = model.cuda()\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image \n\n\nlogits_per_image\n\ntensor([[-0.9982],\n        [-0.5772]], device='cuda:0', grad_fn=&lt;PermuteBackward0&gt;)\n\n\nAs you can see, the first picture is more likely to be of a cat (and that’s correct).\n\nimages[0]\n\n\n\n\n\nimages[1]"
  },
  {
    "objectID": "posts/finetune_clip.html#finetuning-clip",
    "href": "posts/finetune_clip.html#finetuning-clip",
    "title": "Why and How to Fine-tune CLIP",
    "section": "Finetuning CLIP",
    "text": "Finetuning CLIP\nTake a look at how the default model performs with these remote-sensing images, which aren’t predominant in the training set. From a randomly selected set of 8 images, identify the first 3 images that correspond to the prompt “green trees.”\n\nnp.random.seed(0)\nindices = np.random.choice(len(dataset['valid']), 8, replace=False)\npatches = dataset['valid'].select(indices.tolist())\n\n\nshow_images(patches, 8, without_caption=False, num_columns=4,img_size=(3, 3))\n\n\n\n\n\ndef show_result(model, patches, text, top_n = 3):\n    images = [patch['image'] for patch in patches]\n    inputs = processor(text=[text], images=images, return_tensors=\"pt\", padding=True)\n    inputs['input_ids'] = inputs['input_ids'].cuda()\n    inputs['attention_mask'] = inputs['attention_mask'].cuda()\n    inputs['pixel_values'] = inputs['pixel_values'].cuda()\n    \n    model = model.cuda()\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    sorted_idx = (torch.sort(logits_per_image, dim=0, descending=True)[1][:,0]).tolist()\n    sorted_idx = sorted_idx[:top_n]\n    patches_sorted = patches.select(sorted_idx)\n    show_images(patches_sorted, num_images=len(patches_sorted), without_caption=False, num_columns=1, img_size=(3,3))\n\n\nshow_result(model, patches, 'green trees')\n\n\n\n\nWithout fine-tuning, the performance isn’t optimal. As you can see, the first 3 images don’t showcase many trees.\n\n# 8. Initalize our trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=collate_fn,\n)\n\n# 9. Training\ntrain_result = trainer.train()\ntrainer.log_metrics(\"train\", train_result.metrics)\nmetrics = trainer.evaluate()\ntrainer.log_metrics(\"eval\", metrics)\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501: UserWarning: operator() profile_node %380 : int = prim::profile_ivalue(%out_dtype.1)\n does not have profile information (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/third_party/nvfuser/csrc/graph_fuser.cpp:104.)\n  return forward_call(*args, **kwargs)\n\n\n\n    \n      \n      \n      [411/411 06:42, Epoch 3/3]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n\n\n\n\n***** train metrics *****\n  epoch                    =        3.0\n  total_flos               =  3258157GF\n  train_loss               =     1.7008\n  train_runtime            = 0:06:44.15\n  train_samples_per_second =     64.832\n  train_steps_per_second   =      1.017\n***** eval metrics *****\n  epoch                   =        3.0\n  eval_loss               =     3.8048\n  eval_runtime            = 0:00:07.26\n  eval_samples_per_second =    150.574\n  eval_steps_per_second   =      2.477\n\n\n\n    \n      \n      \n      [18/18 00:06]\n    \n    \n\n\n\nshow_result(model, patches, 'green trees')\n\n\n\n\nAfter finetuning the result is much better!! There are trees in all 3 images"
  },
  {
    "objectID": "posts/SSD_base.html",
    "href": "posts/SSD_base.html",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "",
    "text": "Building an Object Detection from scratch with fastai v2\nRecently, I had a project that needs to modify an Object Detection Architecture. However, when I searched for related repositories, I found it quite difficult to understand. We have a lot of libraries for use out of the box but hard to make changes to the source code.\nThis blog is the implementation of Single Shot Detector Architecture using fast.ai in literate programming style so the readers can follow and run each line of code themselves in case needed to deepen their knowledge.\nThe original idea was taken from the fastai 2018 course. Readers are recommended to watch this lecture. 2018 Lecture\nSome useful notes taken by students: - Cedrick Note - Francesco Note\nDataset used: Pascal 2017\nWhat we can learn from this notebook:\n%load_ext autoreload\n%autoreload 2\nfrom fastai.vision.all import *\n\n/home/ubuntu/miniconda3/envs/blog/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "posts/SSD_base.html#object-detection-dataloaders",
    "href": "posts/SSD_base.html#object-detection-dataloaders",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Object Detection Dataloaders",
    "text": "Object Detection Dataloaders\nFor objection detection, you have:\n\n1 independent variable (X): Image\n2 dependents variables (Ys): Bounding box and Class\n\nIn this part, we will use fastai DataBlock to build Object Detection Dataloaders. The idea is from each image file name, we will have:\n\nAn Image\nBounding Boxes getting from the annotations file\nLabels correspond to each bounding box\n\n\n\n\n\n\n\nNote\n\n\n\n\nZero padding: Each image have a different number of objects. Then, to make it possible to gather multiple images to one batch, the number of bounding boxes per image is the maximum in that batch (the padding value by default is 0) bb_pad\nBackground class: In Object Detection, we need to have a class that represents the background. fastai do it automatically for you by adding #na# at index 0\nThe coordinates of bounding box is rescaled to ~ -1 -&gt; 1 in fastai/vision/core.py _scale_pnts\n\n\n\n( Check out some outputs below for details )\n\n\n\nList of Files to Data\n\n\n\npath = untar_data(URLs.PASCAL_2007)\n\n\npath.ls()\n\n(#8) [Path('/home/ubuntu/.fastai/data/pascal_2007/train.json'),Path('/home/ubuntu/.fastai/data/pascal_2007/test.json'),Path('/home/ubuntu/.fastai/data/pascal_2007/test'),Path('/home/ubuntu/.fastai/data/pascal_2007/train.csv'),Path('/home/ubuntu/.fastai/data/pascal_2007/segmentation'),Path('/home/ubuntu/.fastai/data/pascal_2007/valid.json'),Path('/home/ubuntu/.fastai/data/pascal_2007/train'),Path('/home/ubuntu/.fastai/data/pascal_2007/test.csv')]\n\n\n\nimgs, lbl_bbox = get_annotations(path/'train.json') \n\n\nimgs[0], lbl_bbox[0]\n\n('000012.jpg', ([[155, 96, 351, 270]], ['car']))\n\n\n\nimg2bbox = dict(zip(imgs, lbl_bbox))\n\n\nfirst = {k: img2bbox[k] for k in list(img2bbox)[:1]}; first\n\n{'000012.jpg': ([[155, 96, 351, 270]], ['car'])}\n\n\n\ngetters = [lambda o: path/'train'/o, lambda o: img2bbox[o][0], lambda o: img2bbox[o][1]]\n\n\nitem_tfms = [Resize(224, method='squish'),]\nbatch_tfms = [Rotate(), Flip(), Dihedral()]\n\n\npascal = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),\n                 splitter=RandomSplitter(),\n                 getters=getters,\n                 item_tfms=item_tfms,\n                 batch_tfms=batch_tfms,\n                 n_inp=1)\n\n\ndls = pascal.dataloaders(imgs, bs = 128)\n\n\n\n\n\n\n\nNote\n\n\n\n#na# is the background class as defined in BBoxLblBlock\n\n\n\ndls.vocab\n\n['#na#', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n\n\n\nlen(dls.vocab)\n\n21\n\n\n\ndls.show_batch()\n\n\n\n\n\none_batch = dls.one_batch()\n\n\n\n\n\n\n\nNote\n\n\n\nThe coordinates of boudning box is rescaled to ~ -1 -&gt; 1 in fastai/vision/core.py\n\n\n\none_batch[1][0][0]\n\nTensorBBox([-0.0440, -0.2171,  0.2200,  0.5046], device='cuda:0')\n\n\n\n# Zero Padding\none_batch[2]\n\nTensorMultiCategory([[13, 15,  0,  ...,  0,  0,  0],\n                     [12, 15, 15,  ...,  0,  0,  0],\n                     [18,  5,  5,  ...,  0,  0,  0],\n                     ...,\n                     [15,  8,  8,  ...,  0,  0,  0],\n                     [ 7,  0,  0,  ...,  0,  0,  0],\n                     [ 8,  0,  0,  ...,  0,  0,  0]], device='cuda:0')"
  },
  {
    "objectID": "posts/SSD_base.html#model-architecture",
    "href": "posts/SSD_base.html#model-architecture",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Model Architecture",
    "text": "Model Architecture\n\n\n\nSSD Architecture\n\n\nIn a nutshell, Object Detection Model is a model that does 2 jobs at the same time:\n\na regressor with 4 outputs for bounding box\na classifier with c classes.\n\nTo handle multiple objects, here comes the grid cell. For each cell, you will have an atomic prediction for the object that dominates a part of the image ( This is the idea of the receptive field that you will see in the next part )\n\n\n\n\n\n\nMy Intuition\n\n\n\nIn Machine Learning, it is better to improve from something rather than start from scratch. You can see this in: Image Classification Architecture - Resnet with the Skip Connections, or Gradient Boosting in Tree-based Model. There is a common point in the grid-cell SSD architecture, the model will try to improve from an anchor box rather than searching through the whole image.\n\n\nWe should better leverage a well-known pretrained classification model to be used as a backbone / or body ( resnet in this tutorial ) if the object is similar to the Imagenet dataset. The head part will follow to adapt to the necessary dimension\nTo easily develop the idea - visualize and debug, we will start with a simple 4x4 grid\n\ndef flatten_conv(x,k):\n    # Flatten the 4x4 grid to dim16 vectors\n    bs,nf,gx,gy = x.size()\n    x = x.permute(0,2,3,1).contiguous()\n    return x.view(bs,-1,nf//k)\n\n\nclass OutConv(nn.Module):\n    # Output Layers for SSD-Head. Contains oconv1 for Classification and oconv2 for Detection\n    def __init__(self, k, nin, bias):\n        super().__init__()\n        self.k = k\n        self.oconv1 = nn.Conv2d(nin, (len(dls.vocab))*k, 3, padding=1)\n        self.oconv2 = nn.Conv2d(nin, 4*k, 3, padding=1)\n        self.oconv1.bias.data.zero_().add_(bias)\n        \n    def forward(self, x):\n        return [flatten_conv(self.oconv1(x), self.k),\n                flatten_conv(self.oconv2(x), self.k)]\n\n\nclass StdConv(nn.Module):\n    # Standard Convolutional layers \n    def __init__(self, nin, nout, stride=2, drop=0.1):\n        super().__init__()\n        self.conv = nn.Conv2d(nin, nout, 3, stride=stride, padding=1)\n        self.bn = nn.BatchNorm2d(nout)\n        self.drop = nn.Dropout(drop)\n        \n    def forward(self, x): return self.drop(self.bn(F.relu(self.conv(x))))\n\n\nclass SSD_Head(nn.Module):\n    def __init__(self, k, bias):\n        super().__init__()\n        self.drop = nn.Dropout(0.25)\n        self.sconv0 = StdConv(512,256, stride=1)\n        self.sconv2 = StdConv(256,256)\n        self.out = OutConv(k, 256, bias)\n        \n    def forward(self, x):\n        x = self.drop(F.relu(x))\n        x = self.sconv0(x)\n        x = self.sconv2(x)\n        return self.out(x)\n\nWe start with k = 1 which is the number of alterations for each anchor box ( we have a lot of anchor boxes later )\n\nk=1\n\n\nhead_reg4 = SSD_Head(k, -3.)\n\n\nbody = create_body(resnet34(True))\nmodel = nn.Sequential(body, head_reg4)\n\n/home/ubuntu/miniconda3/envs/blog/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n  warnings.warn(\n/home/ubuntu/miniconda3/envs/blog/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nTo understand and verify that everything works ok, you can take out a batch and run the model on it\n\nout0 = body(one_batch[0].cpu())\n\n\nout1 = head_reg4(out0)\n\n\nout1[0].shape, out1[1].shape\n\n(torch.Size([128, 16, 21]), torch.Size([128, 16, 4]))\n\n\nShape explanation:\n\n128: batch size\n16: number of anchor boxes\n21: number of classes\n4: number of bounding box coordinates"
  },
  {
    "objectID": "posts/SSD_base.html#x4-anchor-boxes-and-receptive-field",
    "href": "posts/SSD_base.html#x4-anchor-boxes-and-receptive-field",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "4x4 Anchor boxes and Receptive Field",
    "text": "4x4 Anchor boxes and Receptive Field\nAs mentioned before, we will start with a 4x4 grid to better visualize the idea. The size will be normalized to [0,1]\nThe idea of why, after the Body, we use Conv2d and not Linear Layer to make a 4x4x(4+c) output dimension instead of 16x(4+c) shape is - Receptive Field. This way, each cell will have information that comes directly from the location corresponding to the anchor box. The illustration is below.\n\n\n\nSSD vs YOLO\n\n\n\n\n\nReceptive Field\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe very careful about the bounding box format when working with Object Detection. There are many different formats out there. For example:\n\npascal_voc: [x_min, y_min, x_max, y_max]\ncoco: [x_min, y_min, width, height]\nYOLO: [x_center, y_center, width, height]\n\nThe bounding box format in this tutorial is [x_min, y_min, x_max, y_max]\n\n\nCheck out Bounding Boxes Augmentation for more details:\nWe define the anchors coordinates as below\n\nanc_grid = 4 # Start with only 4x4 grid and no variation for each cell\nk = 1 # Variation of each anchor box\nanc_offset = 1/(anc_grid*2)\nanc_x = np.repeat(np.linspace(anc_offset, 1-anc_offset, anc_grid), anc_grid) # Center of anc in x\nanc_y = np.tile(np.linspace(anc_offset, 1-anc_offset, anc_grid), anc_grid) # Center f anc in y\n\n\nanc_x\n\narray([0.125, 0.125, 0.125, 0.125, 0.375, 0.375, 0.375, 0.375, 0.625,\n       0.625, 0.625, 0.625, 0.875, 0.875, 0.875, 0.875])\n\n\n\nanc_y\n\narray([0.125, 0.375, 0.625, 0.875, 0.125, 0.375, 0.625, 0.875, 0.125,\n       0.375, 0.625, 0.875, 0.125, 0.375, 0.625, 0.875])\n\n\n\nanc_ctrs = np.tile(np.stack([anc_x,anc_y], axis=1), (k,1)) # Anchor centers\nanc_sizes = np.array([[1/anc_grid,1/anc_grid] for i in range(anc_grid*anc_grid)])\n\n\nanc_ctrs\n\narray([[0.125, 0.125],\n       [0.125, 0.375],\n       [0.125, 0.625],\n       [0.125, 0.875],\n       [0.375, 0.125],\n       [0.375, 0.375],\n       [0.375, 0.625],\n       [0.375, 0.875],\n       [0.625, 0.125],\n       [0.625, 0.375],\n       [0.625, 0.625],\n       [0.625, 0.875],\n       [0.875, 0.125],\n       [0.875, 0.375],\n       [0.875, 0.625],\n       [0.875, 0.875]])\n\n\n\nanc_sizes\n\narray([[0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25]])\n\n\n\nanchors = torch.tensor(np.concatenate([anc_ctrs, anc_sizes], axis=1), requires_grad=False).cuda()\n# Coordinates with format: center_x, center_y, W, H\n\n\nanchors\n\ntensor([[0.1250, 0.1250, 0.2500, 0.2500],\n        [0.1250, 0.3750, 0.2500, 0.2500],\n        [0.1250, 0.6250, 0.2500, 0.2500],\n        [0.1250, 0.8750, 0.2500, 0.2500],\n        [0.3750, 0.1250, 0.2500, 0.2500],\n        [0.3750, 0.3750, 0.2500, 0.2500],\n        [0.3750, 0.6250, 0.2500, 0.2500],\n        [0.3750, 0.8750, 0.2500, 0.2500],\n        [0.6250, 0.1250, 0.2500, 0.2500],\n        [0.6250, 0.3750, 0.2500, 0.2500],\n        [0.6250, 0.6250, 0.2500, 0.2500],\n        [0.6250, 0.8750, 0.2500, 0.2500],\n        [0.8750, 0.1250, 0.2500, 0.2500],\n        [0.8750, 0.3750, 0.2500, 0.2500],\n        [0.8750, 0.6250, 0.2500, 0.2500],\n        [0.8750, 0.8750, 0.2500, 0.2500]], device='cuda:0',\n       dtype=torch.float64)\n\n\n\ngrid_sizes = torch.tensor(np.array([1/anc_grid]), requires_grad=False).unsqueeze(1).cuda()\n\n\ngrid_sizes\n\ntensor([[0.2500]], device='cuda:0', dtype=torch.float64)"
  },
  {
    "objectID": "posts/SSD_base.html#visualization-utils",
    "href": "posts/SSD_base.html#visualization-utils",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Visualization Utils",
    "text": "Visualization Utils\nIt is very helpful (to understand/ debug) when you can visualize data of every step. Many subtle tiny details happen in this Object Detection Problem. One careless implementation can lead to hours (or even days) to debug. Sometimes, you just wish that the code throws you some bugs that you can trackback.\n\n\n\n\n\n\nWarning\n\n\n\nThere are some details that you need to double check\n\nAre your ground truth bounding boxes, anchor boxes, bounding box activations are in the same scale ( -1 -&gt; 1 or 0 -&gt; 1 ) ?\nDo the background class is handled correctly? ( This is a bug when I develop this notebook that the old version of the fastai course set the index of background as number_of_classes but in the latest version, it is 0 )\nDo you map correctly each Anchor Box to the ground-true object? (This will be shown in the next session)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDon’t hesitate to take out one batch from your dataloader and verify every single detail. When I start to use fast.ai, I made a big mistake that thinking these data are already processed and we can not show things directly from there. This data is very important, it is the input of your model. It must be carefully double-checked.\n\n\nBelow we will try to plot some images from a batch with their bounding boxes and classes, to see that we did not missing anything\n\nimport matplotlib.colors as mcolors\nimport matplotlib.cm as cmx\nfrom matplotlib import patches, patheffects\n\n\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.set_xticks(np.linspace(0, 224, 8))\n    ax.set_yticks(np.linspace(0, 224, 8))\n    ax.grid()\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n    return ax\n\n\ndef draw_outline(o, lw):\n    o.set_path_effects([patheffects.Stroke(\n        linewidth=lw, foreground='black'), patheffects.Normal()])\n\n\ndef draw_text(ax, xy, txt, sz=14, color='white'):\n    text = ax.text(*xy, txt,\n        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n    draw_outline(text, 1)\n\n\ndef draw_rect(ax, b, color='white'):\n    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n    draw_outline(patch, 4)\n\n\ndef bb_hw(a): return np.array([a[1],a[0],a[3]-a[1]+1,a[2]-a[0]+1])\n\n\ndef get_cmap(N):\n    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba\n\n\nnum_colr = 12\ncmap = get_cmap(num_colr)\ncolr_list = [cmap(float(x)) for x in range(num_colr)]\n\n\ndef show_ground_truth(ax, im, bbox, clas=None, prs=None, thresh=0.3):\n    bb = [bb_hw(o) for o in bbox.reshape(-1,4)]\n    if prs is None:  prs  = [None]*len(bb)\n    if clas is None: clas = [None]*len(bb)\n    ax = show_img(im, ax=ax)\n    k=0\n    for i,(b,c,pr) in enumerate(zip(bb, clas, prs)):\n        if((b[2]&gt;1) and (pr is None or pr &gt; thresh)):\n            k+=1\n            draw_rect(ax, b, color=colr_list[i%num_colr])\n            txt = f'{k}: '\n            if c is not None: txt += ('bg' if c==0 else dls.vocab[c])\n            if pr is not None: txt += f' {pr:.2f}'\n            draw_text(ax, b[:2], txt, color=colr_list[i%num_colr])\n\n\ndef torch_gt(ax, ima, bbox, clas, prs=None, thresh=0.4):\n    return show_ground_truth(ax, ima, to_np((bbox*224).long()),\n         to_np(clas), to_np(prs) if prs is not None else None, thresh)\n\n\nShowing one batch\n\nidx = 5\n\n\nimg = one_batch[0][idx].permute(2,1,0).cpu()\n\n\nplt.imshow(img)\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\nExtracting one batch for your dataloader and see if the data is OK\n\nx = one_batch[0].permute(0,3,2,1).cpu()\n\n\ny = one_batch[1:]\n\nBecause the bounding box in the dataloader is scaled to -1 -&gt; 1, it needs to be rescaled to 0 -&gt; 1 for drawing by doing (bb+1)/2*Size\n\n## Bounding Box after dataloader should Rescale\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\nfor i,ax in enumerate(axes.flat):\n    show_ground_truth(ax, x[i].cpu(), ((y[0][i]+1)/2*224).cpu(), y[1][i].cpu())\nplt.tight_layout()\n\n\n\n\nEverything looks fine! We have correct bounding boxes and their corresponding classes"
  },
  {
    "objectID": "posts/SSD_base.html#map-to-ground-truth-and-loss-function",
    "href": "posts/SSD_base.html#map-to-ground-truth-and-loss-function",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Map to Ground-Truth and Loss function",
    "text": "Map to Ground-Truth and Loss function\nAs you might guess, There are 2 components forming the Object Detection Loss: Classification Loss (For the class) and Localization Loss (For the bounding box)\nThe idea is, for each image, we will: - Calculate the Intersection-over-Union (IoU) of each predefined Anchor Box with the Object Bounding Box. - Assign the label for each cell (Map to ground truth) according to the IoUs. Background will be assigned to Cell which overlaps with no object - Calculate the Classification Loss for all Cells - Calculate the Bounding Box Location Loss only for Cells responsible to Objects (no Background) - Take the sum of these 2 losses\n\n\n\n\n\n\nNote\n\n\n\nCurrently, we will loop for each image in a batch to calculate its loss and then sum them all. I think we might have a better way to vectorize these operations, or, calculate everything in one shot directly with a batch tensor\n\n\n\n\n\nMap to Grouth Truth\n\n\n\ndef get_y(bbox,clas):\n    \"\"\"\n    Remove the zero batching from a batch\n    \n    Because the number of object in each image are different so\n    we need to zero padding for batching \n    \"\"\"\n    bbox = bbox.view(-1,4)\n    clas = clas.view(-1,1)\n    bb_keep = ((bbox[:,2]-bbox[:,0])&gt;0).nonzero()[:,0]\n    return TensorBase(bbox)[bb_keep],TensorBase(clas)[bb_keep]\n\n\none_batch[2][idx]\n\nTensorMultiCategory([16, 16, 16, 16, 14,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n                      0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n                      0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n\n\n\nget_y(one_batch[1][idx], one_batch[2][idx])\n\n(TensorBBox([[ 0.0966, -1.0172,  0.4870, -0.4764],\n             [-0.3311, -1.0029,  0.0835, -0.4559],\n             [-0.3511, -1.0028,  0.0783, -0.4872],\n             [ 0.1286, -1.0201,  0.5700, -0.5041],\n             [ 0.4902,  0.1488,  1.0261,  0.9663],\n             [-0.8546, -0.6447, -0.2425, -0.2718]], device='cuda:0'),\n TensorBBox([[16],\n             [16],\n             [16],\n             [16],\n             [14],\n             [ 7]], device='cuda:0'))\n\n\nWe can see that all the zero values are removed before continuing to process\n\ndef hw2corners(ctr, hw): \n    # Function to convert BB format: (centers and dims) -&gt; corners\n    return torch.cat([ctr-hw/2, ctr+hw/2], dim=1)\n\nThe Activations are passed to a Tanh function to rescale their values to -1 -&gt; 1. Then they are processed to make coherent with the Grid Coordinates:\n\nThe center of each cell’s prediction stays in the cell\nThe size of each cell’s prediction can be varied from 1/2 to 3/2 cell’s size to give more flexibility\n\n\n\n\n\n\n\nTip\n\n\n\nThe bounding box activations are in [x_center, y_center, width, height] format to easily define the min/max scale to the anchor box\n\n\n\ndef actn_to_bb(actn, anchors):\n    actn_bbs = torch.tanh(actn)\n    actn_centers = (actn_bbs[:,:2]/2 * grid_sizes) + anchors[:,:2]\n    actn_hw = (actn_bbs[:,2:]/2+1) * anchors[:,2:]\n    return hw2corners(actn_centers, actn_hw)\n\n\ndef one_hot_embedding(labels, num_classes):\n    return torch.eye(num_classes)[labels].cuda()\n\n\ndef intersect(box_a, box_b):\n    \"\"\"\n    Intersect area between to bounding boxes\n    \"\"\"\n    max_xy = torch.min(box_a[:, None, 2:], box_b[None, :, 2:])\n    min_xy = torch.max(box_a[:, None, :2], box_b[None, :, :2])\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef box_sz(b): return ((b[:, 2]-b[:, 0]) * (b[:, 3]-b[:, 1]))\n\n\ndef jaccard(box_a, box_b):\n    \"\"\"\n    Jaccard or Intersection over Union\n    \"\"\"\n    inter = intersect(box_a, box_b)\n    union = box_sz(box_a).unsqueeze(1) + box_sz(box_b).unsqueeze(0) - inter\n    return inter / union\n\nMap to Ground Truth (Visualization below). The idea is looping through all anchor boxes and calculating the overlaps with the Ground Truth bounding boxes, then assigning each Anchor Box to the corresponding class\n\ndef map_to_ground_truth(overlaps):\n    prior_overlap, prior_idx = overlaps.max(1) # 3\n    gt_overlap, gt_idx = overlaps.max(0) # 16\n    gt_overlap[prior_idx] = 1.99\n    for i,o in enumerate(prior_idx): gt_idx[o] = i\n    return gt_overlap,gt_idx\n\nFor calculating loss, we will loop through every images in a batch and calculate loss for each image (ssd_1_loss), then summing the result with ssd_loss. The Classification Loss (loss_f) currently is left empty as we will discussion it later in the next section.\n\ndef ssd_1_loss(b_c,b_bb,bbox,clas):\n    bbox,clas = get_y(bbox,clas)\n    bbox = (bbox+1)/2\n    a_ic = actn_to_bb(b_bb, anchors)\n    overlaps = jaccard(bbox.data, anchor_cnr.data)\n    gt_overlap,gt_idx = map_to_ground_truth(overlaps)\n    gt_clas = clas[gt_idx]\n    pos = gt_overlap &gt; 0.4\n    pos_idx = torch.nonzero(pos)[:,0]\n    gt_clas[~pos] = 0  # Assign the background to idx 0\n    gt_bbox = bbox[gt_idx]\n    loc_loss = ((TensorBase(a_ic[TensorBase(pos_idx)]) - TensorBase(gt_bbox[TensorBase(pos_idx)])).abs()).mean()\n    clas_loss  = loss_f(b_c, gt_clas)\n    return loc_loss, clas_loss\n\n\nanchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:]).cuda()\n\n\nShowing Map To Ground Truth\nAs mentioned earlier, Map-to-Ground-Truth is a very important step for calculating loss. We should show it to make sure everything looks fine\n\nidx = 0\nbbox = one_batch[1][idx].cuda()\nclas = one_batch[2][idx].cuda()\n\n\nbbox,clas = get_y(bbox,clas)\nbbox = (bbox+1)/2\n# a_ic = actn_to_bb(b_bb, anchors)\noverlaps = jaccard(bbox.data, anchor_cnr.data)\ngt_overlap,gt_idx = map_to_ground_truth(overlaps)\ngt_clas = clas[gt_idx]\npos = gt_overlap &gt; 0.4\npos_idx = torch.nonzero(pos)[:,0]\ngt_clas[~pos] = 0  # Assign the background to idx 0\ngt_bbox = bbox[gt_idx]\n\n\nima = one_batch[0][idx].permute(2,1,0).cpu()\n\n\nfig, ax = plt.subplots(figsize=(7,7))\ntorch_gt(ax, ima, bbox, clas)\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(7,7))\ntorch_gt(ax, ima, anchor_cnr, gt_clas)\n\n\n\n\n\nsz = 224\n\n\n\nClassificaton Loss: Binary Cross Entropy and why Focal Loss\n2 tricks can be used for Classification Loss:\n\nBinary Cross-Entropy Loss without background\nFurther improve Binary Cross-Entropy Loss with Focal Loss\n\n\nBinary Cross-Entropy\n\n\nIf we treat the Background Class as one class and ask the Model to understand what is a Background, it might be too difficult. We can translate it to a set of easier questions: Is it a Cat? Is it a Dog? … through all the classes, which is exactly what Binary Cross-Entropy does\n\n\nFocal Loss\n\n\nThe classification task in object detection is very imbalance that we have a lot of background objects (check the Match to Ground-Truth image above). If we just use Binary Cross-Entropy Loss function, it will try all efforts to improve background classification\n\n\n\n\nFocal Loss vs Binary Cross Entropy Loss\n\n\nQuote from fastai2018 course:\nThe blue line is the binary cross entropy loss. If the answer is not a motorbike, and I said “I think it’s not a motorbike and I am 60% sure” with the blue line, the loss is still about 0.5 which is pretty bad. So if we want to get our loss down, then for all these things which are actually back ground, we have to be saying “I am sure that is background”, “I am sure it’s not a motorbike, or a bus, or a person” — because if I don’t say we are sure it is not any of these things, then we still get loss.\nThat is why the motorbike example did not work. Because even when it gets to lower right corner and it wants to say “I think it’s a motorbike”, there is no payoff for it to say so. If it is wrong, it gets killed. And the vast majority of the time, it is background. Even if it is not background, it is not enough just to say “it’s not background” — you have to say which of the 20 things it is.\nSo the trick is to trying to find a different loss function that looks more like the purple line. Focal loss is literally just a scaled cross entropy loss. Now if we say “I’m .6 sure it’s not a motorbike” then the loss function will say “good for you! no worries”.\n\nclass BCE_Loss(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.num_classes = num_classes\n\n    def forward(self, pred, targ):\n        t = one_hot_embedding(targ.squeeze(), self.num_classes)\n        t = t[:,1:] # Start from 1 to exclude the Background\n        x = pred[:,1:]\n        w = self.get_weight(x,t)\n        return F.binary_cross_entropy_with_logits(x, t, w.detach(), reduction='sum')/self.num_classes\n    \n    def get_weight(self,x,t): return None\n\n\nclass FocalLoss(BCE_Loss):\n    def get_weight(self,x,t):\n        alpha,gamma = 0.25,1\n        p = x.sigmoid()\n        pt = p*t + (1-p)*(1-t)\n        w = alpha*t + (1-alpha)*(1-t)\n        return w * (1-pt).pow(gamma)\n\nThe ssd_loss will loop through every image in a batch and accumulate loss\n\ndef ssd_loss(pred, bbox, clas):\n    lcs, lls = 0., 0.\n    W = 30\n    for b_c, b_bb, bbox, clas in zip(*pred, bbox, clas):\n        loc_loss, clas_loss = ssd_1_loss(b_c, b_bb, bbox, clas)\n        lls += loc_loss\n        lcs += clas_loss\n    return lls + lcs\n\n\nloss_f = FocalLoss(len(dls.vocab))"
  },
  {
    "objectID": "posts/SSD_base.html#training-simple-model",
    "href": "posts/SSD_base.html#training-simple-model",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Training Simple Model",
    "text": "Training Simple Model\n\nmodel = nn.Sequential(body, head_reg4)\n\n\nlearner = Learner(dls, model, loss_func=ssd_loss)\n\n\nlearner.fit_one_cycle(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n34.889286\n28.454723\n00:25\n\n\n1\n32.127403\n29.533695\n00:23\n\n\n2\n30.588394\n26.637667\n00:23\n\n\n3\n29.455709\n25.630453\n00:23\n\n\n4\n28.651590\n25.509596\n00:23\n\n\n\n\n\nThe loss decreases, and the model can learn something. Looking at the results shown below, we can see that the predictions are not so bad but not particularly good either. In the next session, we can see how to improve the results with more anchor boxes\n\nShow Results\n\none_batch = dls.valid.one_batch()\nlearner.model.eval();\npred = learner.model(one_batch[0])\nb_clas, b_bb = pred\nx = one_batch[0]\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\nfor idx,ax in enumerate(axes.flat):\n    ima = x.permute(0,3,2,1).cpu()[idx]\n#     ima=md.val_ds.ds.denorm(x)[idx]\n    bbox,clas = get_y(y[0][idx], y[1][idx])\n    a_ic = actn_to_bb(b_bb[idx], anchors)\n    torch_gt(ax, ima, a_ic, b_clas[idx].max(1)[1], b_clas[idx].max(1)[0].sigmoid(), 0.21)\n#plt.tight_layout()\nplt.subplots_adjust(wspace=0.15, hspace=0.15)"
  },
  {
    "objectID": "posts/SSD_base.html#more-anchors",
    "href": "posts/SSD_base.html#more-anchors",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "More anchors",
    "text": "More anchors\nAs said earlier, the anchor box is a hint for the model to not go too far and focus on a part of the image. So obviously, 4x4 grid is not enough to predict an object of any size. In this part, by adding more Conv2d layers, we will have 3 grids: 4x4, 2x2, 1x1 and each cell will have 9 variations: 3-zooms and 3-ratios\nThe total number of anchors is: (16 + 4 + 1) x 9 = 189 anchors\n\n\n\nimage8.png\n\n\n\n# This is for release the GPU memrory while experimenting. I guess it is not enough. Please tell me if you know a better way\ndel learner\ndel model\nimport gc; gc.collect()\ntorch.cuda.empty_cache()\n\n\nanc_grids = [4,2,1]\nanc_zooms = [0.7, 1., 1.3]\nanc_ratios = [(1.,1.), (1.,0.5), (0.5,1.)]\nanchor_scales = [(anz*i,anz*j) for anz in anc_zooms for (i,j) in anc_ratios]\nk = len(anchor_scales)\nanc_offsets = [1/(o*2) for o in anc_grids]\nk\n\n9\n\n\n\nanc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)\n                        for ao,ag in zip(anc_offsets,anc_grids)])\nanc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)\n                        for ao,ag in zip(anc_offsets,anc_grids)])\nanc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)\n\n\nanc_x\n\narray([0.125, 0.125, 0.125, 0.125, 0.375, 0.375, 0.375, 0.375, 0.625,\n       0.625, 0.625, 0.625, 0.875, 0.875, 0.875, 0.875, 0.25 , 0.25 ,\n       0.75 , 0.75 , 0.5  ])\n\n\n\nanc_sizes  =   np.concatenate([np.array([[o/ag,p/ag] for i in range(ag*ag) for o,p in anchor_scales])\n               for ag in anc_grids])\ngrid_sizes = torch.tensor(np.concatenate([np.array([ 1/ag       for i in range(ag*ag) for o,p in anchor_scales])\n               for ag in anc_grids]), requires_grad=False).unsqueeze(1).cuda()\nanchors = torch.tensor(np.concatenate([anc_ctrs, anc_sizes], axis=1), requires_grad=False).float().cuda()\nanchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:]).cuda()\n\n\nanchor_cnr.shape\n\ntorch.Size([189, 4])\n\n\nWe need to adjust the SSD head a little bit. We will add more Conv2D layer with StdConv (to create 2x2 and 1x1 grids). After each StdConv is an OutConv to handle the Classification prediction and Localization prediction\n\nclass SSD_MultiHead(nn.Module):\n    def __init__(self, k, bias):\n        super().__init__()\n        self.drop = nn.Dropout(drop)\n        self.sconv0 = StdConv(512,256, stride=1, drop=drop)\n        self.sconv1 = StdConv(256,256, drop=drop)\n        self.sconv2 = StdConv(256,256, drop=drop)\n        self.sconv3 = StdConv(256,256, drop=drop)\n        self.out0 = OutConv(k, 256, bias)\n        self.out1 = OutConv(k, 256, bias)\n        self.out2 = OutConv(k, 256, bias)\n        self.out3 = OutConv(k, 256, bias)\n\n    def forward(self, x):\n        x = self.drop(F.relu(x))\n        x = self.sconv0(x)\n        x = self.sconv1(x)\n        o1c,o1l = self.out1(x)\n        x = self.sconv2(x)\n        o2c,o2l = self.out2(x)\n        x = self.sconv3(x)\n        o3c,o3l = self.out3(x)\n        return [torch.cat([o1c,o2c,o3c], dim=1),\n                torch.cat([o1l,o2l,o3l], dim=1)]\n\n\ndrop=0.4\n\n\nhead_reg4 = SSD_MultiHead(k, -4.)\n\n\nbody = create_body(resnet34(True))\nmodel = nn.Sequential(body, head_reg4)\n\n\nlearner = Learner(dls, model, loss_func=ssd_loss)\n\n\n# learner.lr_find()\n\n\nlearner.fit_one_cycle(20, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n79.482658\n65.257332\n00:24\n\n\n1\n77.919846\n64.182114\n00:24\n\n\n2\n75.337402\n69.358673\n00:24\n\n\n3\n70.927734\n73.576935\n00:24\n\n\n4\n65.866829\n58.502281\n00:24\n\n\n5\n61.796001\n51.171406\n00:24\n\n\n6\n58.571583\n47.785007\n00:24\n\n\n7\n55.809723\n45.772766\n00:24\n\n\n8\n53.606243\n45.726265\n00:25\n\n\n9\n51.751816\n45.473743\n00:24\n\n\n10\n49.946224\n43.707134\n00:24\n\n\n11\n48.457012\n42.950340\n00:25\n\n\n12\n46.938705\n40.909351\n00:24\n\n\n13\n45.661766\n40.690815\n00:24\n\n\n14\n44.419174\n40.372437\n00:25\n\n\n15\n43.232628\n39.393692\n00:24\n\n\n16\n42.119759\n38.884872\n00:24\n\n\n17\n41.290310\n38.704178\n00:24\n\n\n18\n40.546024\n38.666664\n00:24\n\n\n19\n39.970467\n38.707432\n00:24\n\n\n\n\n\n\nShow results\n\none_batch = dls.valid.one_batch()\nlearner.model.eval();\npred = learner.model(one_batch[0])\nb_clas, b_bb = pred\nx = one_batch[0]\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\nfor idx,ax in enumerate(axes.flat):\n    ima = x.permute(0,3,2,1).cpu()[idx]\n#     ima=md.val_ds.ds.denorm(x)[idx]\n    bbox,clas = get_y(y[0][idx], y[1][idx])\n    a_ic = actn_to_bb(b_bb[idx], anchors)\n    torch_gt(ax, ima, a_ic, b_clas[idx].max(1)[1], b_clas[idx].max(1)[0].sigmoid(), thresh=0.21)\n#plt.tight_layout()\nplt.subplots_adjust(wspace=0.15, hspace=0.15)\n\n\n\n\nThe result looks better than the simple version above"
  },
  {
    "objectID": "posts/SSD_base.html#non-maximum-suppression-nms",
    "href": "posts/SSD_base.html#non-maximum-suppression-nms",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Non Maximum Suppression (NMS)",
    "text": "Non Maximum Suppression (NMS)\nYou can see in the previous results, that having a lot of Anchor Boxes leads to many overlaps. You can use Non Maximum Suppression, a technique to choose one bounding box out of many overlapping ones\n\ndef nms(boxes, scores, overlap=0.5, top_k=100):\n    keep = scores.new(scores.size(0)).zero_().long()\n    if boxes.numel() == 0: return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    count = 0\n    while idx.numel() &gt; 0:\n        i = idx[-1]  # index of current largest val\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1: break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU &lt;= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n\n\ndef show_nmf(idx):\n    ima = one_batch[0][idx].permute(2,1,0).cpu()\n    bbox = one_batch[1][idx].cuda()\n    clas = one_batch[2][idx].cuda()\n    bbox,clas = get_y(bbox,clas)\n    \n    a_ic = actn_to_bb(b_bb[idx], anchors)\n    clas_pr, clas_ids = b_clas[idx].max(1)\n    clas_pr = clas_pr.sigmoid()\n\n    conf_scores = b_clas[idx].sigmoid().t().data\n\n    out1,out2,cc = [],[],[]\n    for cl in range(1, len(conf_scores)):\n        c_mask = conf_scores[cl] &gt; 0.25\n        if c_mask.sum() == 0: continue\n        scores = conf_scores[cl][c_mask]\n        l_mask = c_mask.unsqueeze(1).expand_as(a_ic)\n        boxes = a_ic[l_mask].view(-1, 4)\n        ids, count = nms(boxes.data, scores, 0.4, 50)\n        ids = ids[:count]\n        out1.append(scores[ids])\n        out2.append(boxes.data[ids])\n        cc.append([cl]*count)\n    if not cc:\n        print(f\"{i}: empty array\")\n        return\n    cc = torch.tensor(np.concatenate(cc))\n    out1 = torch.cat(out1)\n    out2 = torch.cat(out2)\n\n    fig, ax = plt.subplots(figsize=(8,8))\n    torch_gt(ax, ima, out2, cc, out1, 0.1)\n\n\nfor i in range(25, 35): show_nmf(i)\n\n25: empty array\n28: empty array\n31: empty array\n32: empty array"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, I’m Dien-Hoa Truong, Data Scientist, vietnamese currently living in the french riviera. I love programming, and math and am a big fan of fast.ai. Here is where I share all the knowledge I discover along my AI journey\nSome achievements: - fast.ai regular member - winner of community Kaggle competition"
  },
  {
    "objectID": "posts/tst_from_scratch.html",
    "href": "posts/tst_from_scratch.html",
    "title": "Transformer for timeseries",
    "section": "",
    "text": "Developing an Intuition for Transformers and Applying Them to Time Series Classification\nStruggling to learn a new deep learning architecture, such as Transformer, can be quite challenging. However, it doesn’t have to be so daunting. In this blog post, I will demonstrate a practical approach to start using a new architecture, specifically the Transformer. We will construct a basic Transformer architecture and progressively fine-tune it to achieve the performance of the TST architecture (A Transformer-based Framework for Multivariate Time Series Representation Learning).\nContent:\nReferences:\nYou’ll need fastai and tsai to run the code in this blog-post\n!pip install -Uqq tsai\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch import nn\nfrom fastai.data.core import DataLoader, DataLoaders\nfrom fastai.learner import Learner\nfrom fastai.losses import LabelSmoothingCrossEntropyFlat\nfrom fastai.metrics import RocAucBinary, accuracy\nfrom fastai.torch_core import Module\nfrom fastai.layers import Flatten\nfrom tsai.models.TST import TST\nfrom tsai.models.RNN import LSTM\nfrom tsai.data.external import get_UCR_data\nfrom tsai.callback.core import ShowGraph as ShowGraphCallback2\nfrom tsai.learner import plot_metrics\nfrom tsai.imports import default_device\nimport numpy as np\nfrom torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder"
  },
  {
    "objectID": "posts/tst_from_scratch.html#dataset-facedetection",
    "href": "posts/tst_from_scratch.html#dataset-facedetection",
    "title": "Transformer for timeseries",
    "section": "Dataset: FaceDetection",
    "text": "Dataset: FaceDetection\n\n\n\n\n\n\nNote\n\n\n\nWhy Time Series? Although the Transformer originates from the NLP domain and outperforms all previous architectures, I believe that, for those not yet familiar with NLP, it is more advantageous to start with a domain that requires less preprocessing, such as Time Series. This way, we can focus our attention on understanding the architecture itself.\n\n\nIn this tutorial, we will be using a dataset from the well-known UEA & UCR Time Series repository. Although we won’t delve into the details of this dataset in this blog post, it’s worth mentioning its purpose. The objective is to classify whether a given MEG signal (Magnetoencephalography) represents a face or not. The input dimension is 144, and the sequence length is 62.\nI chose this dataset because it contains a reasonable amount of data (5,890 training instances and 3,524 testing instances) and has been used in a Transformer tutorial in the tsai repository. This ensures that we have a reliable reference model to aim to outperform.\nWe will utilize utility functions from the tsai and fastai libraries to facilitate our work and streamline the process.\n\nbatch_size, c_in, c_out, seq_len = 64, 144, 2, 62\nX, y, splits = get_UCR_data('FaceDetection', return_split=False)\n\nX_train = X[splits[0]]\ny_train = y[splits[0]]\nX_valid = X[splits[1]]\ny_valid = y[splits[1]]\n\nmean_trn = np.mean(X_train, axis=(0,2), keepdims=True)\nstd_trn = np.std(X_train, axis=(0,2), keepdims=True)\n\n\nclass TSDataset(Dataset):\n    \"\"\"TimeSeries DataSet for FaceDetection\"\"\"\n    def __init__(self, X, y):\n        super(TSDataset, self).__init__()\n        self.X = torch.tensor(X)\n        self.Y = torch.concat([torch.tensor([_y == '0'], dtype=int) for _y in y])\n    \n    def __len__(self): return len(self.X)\n    \n    def __getitem__(self, i):\n        return self.X[i], self.Y[i]\n\nThe following code demonstrates how to create data loaders for the training and validation sets:\n\ndset_train = TSDataset(X_train, y_train)\ndset_valid = TSDataset(X_valid, y_valid)\n\ndl_train = DataLoader(dset_train, batch_size=batch_size, shuffle=True)\ndl_valid = DataLoader(dset_valid, batch_size=batch_size, shuffle=False)\n\ndls = DataLoaders(dl_train, dl_valid) \ndls = dls.cuda()\n\n\nx, y = next(iter(dl_train))\n\n\nx.shape, y.shape\n\n(torch.Size([64, 144, 62]), torch.Size([64]))"
  },
  {
    "objectID": "posts/tst_from_scratch.html#reference-model",
    "href": "posts/tst_from_scratch.html#reference-model",
    "title": "Transformer for timeseries",
    "section": "Reference Model",
    "text": "Reference Model\nThe reference model we will be using is the TST (Transformer-based Framework for Multivariate Time Series Representation Learning) and implemented by the tsai library.\n\ndef evaluate_model(model, n_epoch=30):\n    learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropyFlat(), \n                    metrics=[RocAucBinary(), accuracy],  cbs=ShowGraphCallback2())\n    learn.fit_one_cycle(n_epoch, 1e-4) \n\n\nmodel = TST(c_in, c_out, seq_len, dropout=0.3, fc_dropout=0.9, n_heads=1, n_layers=1)\n\n\nevaluate_model(model)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nroc_auc_score\naccuracy\ntime\n\n\n\n\n0\n0.943482\n0.710201\n0.503936\n0.505675\n00:02\n\n\n1\n0.959244\n0.705551\n0.515424\n0.515607\n00:01\n\n\n2\n0.940436\n0.697859\n0.534342\n0.532350\n00:01\n\n\n3\n0.900420\n0.692391\n0.563204\n0.543133\n00:01\n\n\n4\n0.872391\n0.682138\n0.601740\n0.567537\n00:01\n\n\n5\n0.873357\n0.674679\n0.632084\n0.591090\n00:01\n\n\n6\n0.827063\n0.668141\n0.662991\n0.618331\n00:01\n\n\n7\n0.802059\n0.658147\n0.682879\n0.635925\n00:01\n\n\n8\n0.753277\n0.653781\n0.693962\n0.641884\n00:01\n\n\n9\n0.744286\n0.647927\n0.698235\n0.643303\n00:01\n\n\n10\n0.727256\n0.646205\n0.705161\n0.654370\n00:01\n\n\n11\n0.699470\n0.644024\n0.707230\n0.654654\n00:01\n\n\n12\n0.707701\n0.639246\n0.713946\n0.663167\n00:01\n\n\n13\n0.677575\n0.637510\n0.716578\n0.663734\n00:01\n\n\n14\n0.690399\n0.635938\n0.720142\n0.667991\n00:01\n\n\n15\n0.651648\n0.635806\n0.720622\n0.667991\n00:01\n\n\n16\n0.639079\n0.634356\n0.722736\n0.665153\n00:01\n\n\n17\n0.672199\n0.633124\n0.724921\n0.665437\n00:01\n\n\n18\n0.639468\n0.631378\n0.728070\n0.668558\n00:01\n\n\n19\n0.638936\n0.629368\n0.728399\n0.668558\n00:01\n\n\n20\n0.625763\n0.627779\n0.731610\n0.672247\n00:01\n\n\n21\n0.619361\n0.626804\n0.732947\n0.671680\n00:01\n\n\n22\n0.633200\n0.626732\n0.732423\n0.673666\n00:01\n\n\n23\n0.633324\n0.625232\n0.735347\n0.673099\n00:01\n\n\n24\n0.639257\n0.625497\n0.733850\n0.672531\n00:01\n\n\n25\n0.626306\n0.625547\n0.733988\n0.671396\n00:01\n\n\n26\n0.616693\n0.625246\n0.734917\n0.674234\n00:01\n\n\n27\n0.627592\n0.625567\n0.733930\n0.671396\n00:01\n\n\n28\n0.616872\n0.625370\n0.734323\n0.671680\n00:01\n\n\n29\n0.617217\n0.624772\n0.734979\n0.672815\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the simplicity for the reader, I do not use any normalization technique and training in lesser number of epochs than the original reference notebook. After 100 epochs, they reach an accuracy arount 0.70.1"
  },
  {
    "objectID": "posts/tst_from_scratch.html#baseline",
    "href": "posts/tst_from_scratch.html#baseline",
    "title": "Transformer for timeseries",
    "section": "Baseline",
    "text": "Baseline\nWe’ll begin our journey by exploring an LSTM model, which was commonly used for sequence classification in the pre-transformer era.\n\n\n\n\n\n\nNote\n\n\n\nNote: Observing the validation loss may lead you to believe that the model is overfitting. However, this is not the case, as the final metric (accuracy) continues to increase.\n\n\n\nmodel = LSTM(144,2,rnn_dropout=0.3, fc_dropout=0.3)\n\n\nevaluate_model(model)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nroc_auc_score\naccuracy\ntime\n\n\n\n\n0\n0.698941\n0.696725\n0.522427\n0.517026\n00:02\n\n\n1\n0.696502\n0.695753\n0.526428\n0.516175\n00:01\n\n\n2\n0.694726\n0.694219\n0.533478\n0.519013\n00:01\n\n\n3\n0.691055\n0.692617\n0.541788\n0.529228\n00:01\n\n\n4\n0.691306\n0.691157\n0.549403\n0.538309\n00:01\n\n\n5\n0.684514\n0.689195\n0.559889\n0.545119\n00:01\n\n\n6\n0.680696\n0.686942\n0.571706\n0.545970\n00:01\n\n\n7\n0.668403\n0.685266\n0.581042\n0.554767\n00:01\n\n\n8\n0.659610\n0.685288\n0.585615\n0.558456\n00:01\n\n\n9\n0.656271\n0.686517\n0.588135\n0.557889\n00:01\n\n\n10\n0.651706\n0.688422\n0.589830\n0.557889\n00:01\n\n\n11\n0.630258\n0.691245\n0.590714\n0.557605\n00:01\n\n\n12\n0.620531\n0.697241\n0.591923\n0.557605\n00:01\n\n\n13\n0.606660\n0.700341\n0.597773\n0.571793\n00:01\n\n\n14\n0.595829\n0.706305\n0.598964\n0.574347\n00:01\n\n\n15\n0.581478\n0.708097\n0.604908\n0.579455\n00:01\n\n\n16\n0.570084\n0.709881\n0.610240\n0.580590\n00:01\n\n\n17\n0.560884\n0.712225\n0.612305\n0.583144\n00:01\n\n\n18\n0.570109\n0.713588\n0.616308\n0.583995\n00:01\n\n\n19\n0.558659\n0.713829\n0.616927\n0.581725\n00:01\n\n\n20\n0.545606\n0.714969\n0.618493\n0.580874\n00:01\n\n\n21\n0.541346\n0.715872\n0.620730\n0.582293\n00:01\n\n\n22\n0.538415\n0.716904\n0.622545\n0.580590\n00:01\n\n\n23\n0.535115\n0.717429\n0.623803\n0.581725\n00:01\n\n\n24\n0.531399\n0.718325\n0.623906\n0.582577\n00:01\n\n\n25\n0.540339\n0.718253\n0.624539\n0.582009\n00:01\n\n\n26\n0.539049\n0.718644\n0.624380\n0.582293\n00:01\n\n\n27\n0.529248\n0.718712\n0.624360\n0.583144\n00:01\n\n\n28\n0.524167\n0.718756\n0.624453\n0.583712\n00:01\n\n\n29\n0.524851\n0.718751\n0.624465\n0.583712\n00:01"
  },
  {
    "objectID": "posts/tst_from_scratch.html#our-tst",
    "href": "posts/tst_from_scratch.html#our-tst",
    "title": "Transformer for timeseries",
    "section": "Our TST",
    "text": "Our TST\n\n\n\nTransformer Architecture\n\n\nThe diagram above illustrates the Transformer architecture as presented in the “Attention is All You Need” paper. The breakthrough in this architecture is the Multi-Head Attention. The idea behind Attention is that if your model can focus on the most important parts of a long sequence, it can perform better without being affected by noise.\nHow does it work? Well, in my experience, when we are not very familiar with a new architecture, we shouldn’t focus too much on understanding every detail of the architecture. I spent a lot of time reading various tutorials, trying to grasp the clever idea behind this, only to realize that I still didn’t know how to apply it to a real case. I will attempt to cover building Self-Attention from scratch in a future blog post. However, in this one, we will start by learning how to use the Transformer module from PyTorch.\nWhat do we need to pay attention to here? We will mainly focus on the shapes of the input and output. The input maintains its shape after passing through the Transformer Encoder. Subsequently, the output is flattened and passed through a linear layer, which generates the appropriate number of classes for the given classification task\nOur first model as below is a very simple architecture with just one TransformerEncoder Layer and one Linear Layer\n\n\n\nSimple TST Artchitecture\n\n\n\nbatch_size, c_in, d_model, c_out, seq_len, dropout, fc_dropout  = 64, 144, 128, 2, 62, 0.7, 0.9\n\n\nclass OurTST(Module):\n    def __init__(self, c_in, c_out, seq_len, dropout):\n        self.c_in, self.c_out, self.seq_len = c_in, c_out, seq_len\n        encoder_layer = TransformerEncoderLayer(d_model=c_in, nhead=1, dropout=dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=1)\n        self.head = nn.Linear(seq_len*c_in, c_out)\n    def forward(self, x):\n        o = x.swapaxes(1,2) # [bs,c_in,seq_len] -&gt; [bs,seq_len,c_in]\n        o = self.transformer_encoder(o) # [bs,c_in,seq_len] -&gt; [bs,c_in,seq_len]\n        o = o.reshape(o.shape[0], -1) # [bs,c_in,seq_len] -&gt; [bs,c_in x seq_len]\n        o = self.head(o) # [bs,c_in x seq_len] -&gt; [bs,]\n        return o\n\n\nmodel = OurTST(c_in, c_out, seq_len, 0.9)\n\n\nmodel\n\nOurTST(\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n        )\n        (linear1): Linear(in_features=144, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.9, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=144, bias=True)\n        (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.9, inplace=False)\n        (dropout2): Dropout(p=0.9, inplace=False)\n      )\n    )\n  )\n  (head): Linear(in_features=8928, out_features=2, bias=True)\n)\n\n\n\nevaluate_model(model, 30)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nroc_auc_score\naccuracy\ntime\n\n\n\n\n0\n0.771023\n0.765103\n0.501380\n0.503405\n00:01\n\n\n1\n0.767232\n0.743445\n0.526820\n0.517877\n00:01\n\n\n2\n0.760522\n0.713145\n0.577938\n0.552781\n00:01\n\n\n3\n0.718567\n0.686044\n0.636998\n0.599603\n00:01\n\n\n4\n0.707388\n0.671847\n0.679890\n0.627128\n00:01\n\n\n5\n0.667916\n0.675715\n0.702388\n0.650114\n00:01\n\n\n6\n0.646124\n0.692107\n0.713854\n0.657492\n00:01\n\n\n7\n0.626370\n0.714405\n0.720573\n0.665153\n00:01\n\n\n8\n0.623622\n0.739222\n0.721124\n0.671396\n00:01\n\n\n9\n0.597921\n0.755921\n0.722508\n0.669410\n00:01\n\n\n10\n0.595011\n0.766809\n0.725383\n0.672815\n00:01\n\n\n11\n0.574535\n0.771822\n0.729465\n0.675936\n00:01\n\n\n12\n0.573907\n0.776058\n0.732046\n0.671112\n00:01\n\n\n13\n0.570190\n0.785060\n0.733693\n0.673950\n00:01\n\n\n14\n0.552551\n0.789218\n0.734351\n0.674234\n00:01\n\n\n15\n0.575410\n0.794398\n0.736869\n0.678774\n00:01\n\n\n16\n0.563437\n0.795728\n0.738365\n0.676788\n00:01\n\n\n17\n0.563861\n0.796549\n0.739659\n0.680193\n00:01\n\n\n18\n0.538637\n0.797076\n0.740454\n0.679909\n00:01\n\n\n19\n0.548644\n0.796367\n0.741566\n0.681896\n00:01\n\n\n20\n0.549104\n0.798111\n0.741827\n0.685017\n00:01\n\n\n21\n0.539753\n0.801571\n0.741463\n0.683598\n00:01\n\n\n22\n0.542557\n0.801905\n0.742229\n0.683031\n00:01\n\n\n23\n0.547139\n0.803032\n0.742469\n0.682463\n00:01\n\n\n24\n0.532426\n0.802947\n0.743231\n0.683598\n00:01\n\n\n25\n0.524330\n0.803357\n0.743226\n0.683314\n00:01\n\n\n26\n0.525560\n0.803959\n0.743130\n0.683598\n00:01\n\n\n27\n0.534251\n0.804135\n0.743205\n0.682747\n00:01\n\n\n28\n0.541934\n0.804315\n0.743135\n0.682747\n00:01\n\n\n29\n0.527424\n0.804290\n0.743156\n0.682747\n00:01\n\n\n\n\n\n\n\n\n\n\n\nWell, our model outperforms the LSTM model and even better than the TST model after 30 epochs"
  },
  {
    "objectID": "posts/tst_from_scratch.html#upgrades",
    "href": "posts/tst_from_scratch.html#upgrades",
    "title": "Transformer for timeseries",
    "section": "Upgrades",
    "text": "Upgrades\nIn this section, I will discuss how we can build upon our basic Transformer architecture to achieve even greater results. We will explore several ideas inspired by the original paper and general deep learning concepts.\n1- Feature Standardizing:\nTo enhance neural network training, it is recommended that input data have a zero mean and unit standard deviation ( for more details, refer to this lesson from fast.ai). In line with the original paper, we employ feature standardization, which standardizes each feature separately.\nmean_trn = np.mean(X_train, axis=(0,2), keepdims=True)\nstd_trn = np.std(X_train, axis=(0,2), keepdims=True)\n... # In the Dataset\nself.X = (self.X - mean_trn)/std_trn\n2- Input Projection\nBefore feeding the input into the TransformerEncoder layer, it can be projected into another dimension, allowing us to control the input received by the TransformerEncoder. In general, using suitable techniques, a deeper network can potentially outperform a shallow one.\n    def __init__( ... )\n        self.W_P = nn.Linear(c_in, d_model)\n    def forward(self, x):\n        o = x.swapaxes(1, 2)  \n        o = self.W_P(o)  # Input Projection\n        \n3- Positional Encoding\nTransformers do not inherently capture the positional order of input data, which can be crucial for certain tasks. To embed this information, we can employ techniques such as passing the input through a specific function (e.g., a sinusoidal function) or creating learnable parameters for position (as implemented in our code).\n    def __init__( ... )\n        # Positional encoding\n        W_pos = torch.empty((seq_len, d_model), device=default_device())\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n        self.W_pos = nn.Parameter(W_pos, requires_grad=True)\n    def forward(self, x):\n        o = x.swapaxes(1, 2)  \n        o = self.W_P(o)  \n        o = o + self.W_pos # Positional Encoding\n4- DropOut\nDeep neural networks can be prone to overfitting. To mitigate this issue, we can introduce dropout layers in our model, making it more resistant to overfitting. In our architecture, there are two types of dropout: one within the TransformerEncoder layer and another just before the final Linear layer.\n    def __init__( ... )\n        # Transformer encoder layers\n        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=1, dropout=drop_out) # dropout inside Transformer Layer\n        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=n_layers)\n\n        self.head = nn.Sequential(\n            nn.GELU(),\n            Flatten(),\n            nn.Dropout(fc_dropout), # fully connected dropout\n            nn.Linear(seq_len * d_model, c_out)\n        )\n\nbatch_size, c_in, d_model, c_out, seq_len,fc_dropout = 64, 144, 128, 2, 62, 0.9\nX, y, splits = get_UCR_data('FaceDetection', return_split=False)\n\nX_train = X[splits[0]]\ny_train = y[splits[0]]\nX_valid = X[splits[1]]\ny_valid = y[splits[1]]\n\nmean_trn = np.mean(X_train, axis=(0,2), keepdims=True)\nstd_trn = np.std(X_train, axis=(0,2), keepdims=True)\n\n\nclass TSDataset(Dataset):\n    \"\"\"TimeSeries DataSet for FaceDetection\"\"\"\n    def __init__(self, X, y):\n        super(TSDataset, self).__init__()\n        self.X = torch.tensor(X)\n        self.X = (self.X - mean_trn)/std_trn\n        self.Y = torch.concat([torch.tensor([_y == '0'], dtype=int) for _y in y])\n    \n    def __len__(self): return len(self.X)\n    \n    def __getitem__(self, i):\n        return self.X[i], self.Y[i]\n\n\nclass OurTST(Module):\n    def __init__(self, c_in, c_out, d_model, seq_len, n_layers, drop_out, fc_dropout):\n        self.c_in, self.c_out, self.seq_len = c_in, c_out, seq_len\n        self.W_P = nn.Linear(c_in, d_model)\n\n        # Positional encoding\n        W_pos = torch.empty((seq_len, d_model), device=default_device())\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n        self.W_pos = nn.Parameter(W_pos, requires_grad=True)\n\n        # Transformer encoder layers\n        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=1, dropout=drop_out)\n        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=n_layers)\n\n        self.head = nn.Sequential(\n            nn.GELU(),\n            Flatten(),\n            nn.Dropout(fc_dropout),\n            nn.Linear(seq_len * d_model, c_out)\n        )\n\n    def forward(self, x):\n        o = x.swapaxes(1, 2)  # [bs,c_in,seq_len] -&gt; [bs,seq_len,c_in]\n        o = self.W_P(o)  # [bs,seq_len,c_in] -&gt; [bs,seq_len,d_model]\n        o = o + self.W_pos\n        o = self.transformer_encoder(o)  # [bs, seq_len, d_model] -&gt; [bs, seq_len, d_model]\n        o = o.contiguous()\n        o = self.head(o)  # [bs,seq_len x d_model] -&gt; [bs,c_out]\n        return o\n\n\nmodel = OurTST(c_in, c_out, d_model, seq_len, 3, 0.4 ,0.9)\n\n\nevaluate_model(model, n_epoch=30)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nroc_auc_score\naccuracy\ntime\n\n\n\n\n0\n0.925684\n0.714104\n0.499395\n0.500000\n00:02\n\n\n1\n0.941063\n0.708397\n0.513332\n0.510499\n00:02\n\n\n2\n0.910563\n0.697380\n0.547459\n0.532066\n00:02\n\n\n3\n0.875563\n0.684338\n0.586375\n0.553916\n00:02\n\n\n4\n0.788294\n0.675705\n0.634576\n0.583144\n00:02\n\n\n5\n0.729792\n0.669882\n0.668022\n0.613224\n00:02\n\n\n6\n0.698079\n0.661823\n0.690192\n0.640182\n00:02\n\n\n7\n0.678686\n0.650137\n0.703406\n0.646141\n00:02\n\n\n8\n0.669151\n0.639466\n0.713121\n0.653235\n00:02\n\n\n9\n0.640111\n0.631399\n0.720097\n0.656924\n00:02\n\n\n10\n0.636223\n0.625919\n0.728919\n0.664586\n00:02\n\n\n11\n0.609655\n0.623980\n0.735162\n0.663734\n00:02\n\n\n12\n0.600260\n0.626050\n0.739055\n0.669694\n00:02\n\n\n13\n0.595127\n0.623003\n0.741405\n0.671396\n00:02\n\n\n14\n0.592127\n0.624625\n0.742487\n0.675369\n00:02\n\n\n15\n0.572983\n0.630688\n0.746745\n0.677923\n00:02\n\n\n16\n0.573267\n0.628453\n0.747862\n0.682463\n00:02\n\n\n17\n0.564986\n0.627425\n0.749735\n0.680761\n00:02\n\n\n18\n0.567010\n0.626230\n0.751817\n0.684733\n00:02\n\n\n19\n0.555792\n0.628040\n0.751030\n0.680761\n00:02\n\n\n20\n0.546230\n0.633149\n0.751046\n0.683031\n00:02\n\n\n21\n0.547146\n0.631326\n0.752979\n0.684166\n00:02\n\n\n22\n0.548416\n0.632791\n0.752416\n0.684449\n00:02\n\n\n23\n0.548463\n0.634959\n0.752966\n0.682747\n00:02\n\n\n24\n0.541696\n0.634788\n0.753698\n0.683598\n00:02\n\n\n25\n0.541297\n0.634837\n0.753692\n0.684733\n00:02\n\n\n26\n0.530234\n0.635058\n0.753956\n0.683314\n00:02\n\n\n27\n0.539544\n0.635267\n0.753939\n0.683031\n00:02\n\n\n28\n0.546123\n0.635198\n0.753946\n0.683314\n00:02\n\n\n29\n0.535531\n0.635230\n0.753916\n0.683314\n00:02\n\n\n\n\n\n\n\n\n\n\n\nLet’s try training with more epochs. In the following example, we will train for 100 epochs, which is the same as in this tutorial from tsai\n\nmodel = OurTST(c_in, c_out, d_model, seq_len, 3, 0.6 ,0.9)\nevaluate_model(model, n_epoch=100)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nroc_auc_score\naccuracy\ntime\n\n\n\n\n0\n0.986113\n0.723611\n0.495537\n0.501986\n00:02\n\n\n1\n0.985938\n0.715433\n0.500191\n0.498581\n00:02\n\n\n2\n0.959804\n0.714567\n0.505571\n0.503121\n00:02\n\n\n3\n0.935837\n0.711624\n0.512527\n0.509081\n00:02\n\n\n4\n0.949967\n0.708520\n0.520180\n0.511351\n00:02\n\n\n5\n0.938500\n0.703618\n0.531381\n0.521283\n00:02\n\n\n6\n0.914983\n0.698704\n0.546194\n0.528944\n00:02\n\n\n7\n0.916452\n0.693311\n0.562272\n0.545687\n00:02\n\n\n8\n0.863562\n0.688229\n0.581601\n0.557605\n00:02\n\n\n9\n0.825720\n0.684290\n0.603750\n0.570658\n00:02\n\n\n10\n0.814094\n0.675768\n0.626344\n0.585982\n00:02\n\n\n11\n0.770269\n0.671013\n0.646063\n0.598751\n00:02\n\n\n12\n0.734332\n0.666361\n0.663611\n0.621169\n00:02\n\n\n13\n0.728247\n0.666351\n0.673837\n0.612372\n00:02\n\n\n14\n0.703636\n0.659145\n0.688090\n0.640749\n00:02\n\n\n15\n0.695590\n0.652872\n0.696829\n0.652667\n00:02\n\n\n16\n0.685371\n0.648839\n0.701994\n0.639330\n00:02\n\n\n17\n0.677378\n0.640827\n0.706331\n0.654370\n00:02\n\n\n18\n0.659291\n0.637410\n0.711023\n0.658059\n00:02\n\n\n19\n0.654945\n0.636273\n0.717630\n0.659478\n00:02\n\n\n20\n0.642616\n0.639057\n0.721688\n0.658343\n00:02\n\n\n21\n0.629476\n0.639091\n0.728980\n0.667991\n00:02\n\n\n22\n0.618588\n0.642530\n0.735525\n0.673099\n00:02\n\n\n23\n0.612620\n0.642298\n0.741208\n0.671680\n00:02\n\n\n24\n0.606063\n0.644125\n0.745705\n0.677639\n00:02\n\n\n25\n0.597042\n0.653301\n0.746442\n0.673099\n00:02\n\n\n26\n0.591410\n0.649610\n0.749507\n0.680761\n00:02\n\n\n27\n0.599989\n0.644790\n0.752807\n0.684449\n00:02\n\n\n28\n0.580830\n0.655860\n0.755328\n0.683598\n00:02\n\n\n29\n0.579203\n0.667195\n0.754020\n0.683314\n00:02\n\n\n30\n0.576821\n0.666351\n0.757188\n0.679342\n00:02\n\n\n31\n0.571501\n0.670810\n0.757713\n0.688138\n00:02\n\n\n32\n0.577291\n0.670262\n0.760897\n0.689841\n00:02\n\n\n33\n0.565669\n0.672775\n0.759541\n0.687003\n00:02\n\n\n34\n0.581174\n0.681387\n0.757111\n0.685868\n00:02\n\n\n35\n0.571533\n0.672656\n0.760768\n0.689841\n00:02\n\n\n36\n0.568005\n0.684501\n0.759986\n0.685868\n00:02\n\n\n37\n0.558923\n0.689911\n0.758634\n0.687287\n00:02\n\n\n38\n0.544595\n0.691652\n0.760238\n0.692111\n00:02\n\n\n39\n0.547767\n0.690527\n0.760010\n0.690976\n00:02\n\n\n40\n0.547662\n0.695285\n0.760551\n0.694949\n00:02\n\n\n41\n0.545006\n0.692685\n0.761823\n0.692111\n00:02\n\n\n42\n0.557433\n0.701815\n0.761501\n0.696084\n00:02\n\n\n43\n0.556936\n0.702578\n0.758627\n0.687003\n00:02\n\n\n44\n0.542515\n0.713648\n0.757356\n0.688422\n00:02\n\n\n45\n0.540261\n0.718203\n0.757706\n0.686152\n00:02\n\n\n46\n0.531610\n0.718681\n0.758163\n0.692111\n00:02\n\n\n47\n0.525029\n0.722788\n0.760487\n0.685868\n00:02\n\n\n48\n0.531557\n0.714598\n0.761604\n0.695516\n00:02\n\n\n49\n0.528427\n0.720070\n0.757874\n0.688422\n00:02\n\n\n50\n0.533936\n0.731660\n0.760347\n0.694381\n00:02\n\n\n51\n0.532107\n0.734147\n0.758814\n0.687571\n00:02\n\n\n52\n0.528784\n0.726680\n0.761935\n0.690125\n00:02\n\n\n53\n0.525845\n0.736096\n0.760733\n0.694665\n00:02\n\n\n54\n0.535664\n0.743276\n0.758939\n0.689841\n00:02\n\n\n55\n0.521018\n0.735471\n0.761122\n0.691544\n00:02\n\n\n56\n0.523641\n0.729754\n0.761421\n0.692963\n00:02\n\n\n57\n0.525150\n0.735508\n0.761838\n0.689274\n00:02\n\n\n58\n0.516105\n0.739418\n0.763721\n0.699205\n00:02\n\n\n59\n0.511782\n0.742465\n0.761415\n0.694098\n00:02\n\n\n60\n0.523468\n0.742341\n0.760741\n0.695800\n00:02\n\n\n61\n0.520403\n0.743021\n0.761011\n0.694381\n00:02\n\n\n62\n0.514355\n0.741946\n0.762210\n0.700624\n00:02\n\n\n63\n0.511770\n0.744806\n0.762579\n0.694949\n00:02\n\n\n64\n0.514207\n0.747035\n0.761403\n0.694665\n00:02\n\n\n65\n0.504732\n0.747706\n0.760959\n0.689841\n00:02\n\n\n66\n0.507337\n0.746426\n0.761844\n0.693530\n00:02\n\n\n67\n0.502984\n0.753452\n0.761258\n0.696084\n00:02\n\n\n68\n0.503284\n0.748423\n0.762783\n0.694381\n00:02\n\n\n69\n0.510511\n0.748741\n0.762635\n0.697219\n00:02\n\n\n70\n0.502065\n0.757950\n0.761098\n0.690692\n00:02\n\n\n71\n0.499528\n0.758117\n0.760646\n0.696084\n00:02\n\n\n72\n0.508516\n0.758758\n0.759330\n0.694665\n00:02\n\n\n73\n0.497975\n0.761433\n0.759172\n0.694098\n00:02\n\n\n74\n0.497121\n0.762476\n0.758746\n0.695233\n00:02\n\n\n75\n0.494813\n0.761791\n0.759932\n0.696368\n00:02\n\n\n76\n0.496914\n0.761701\n0.760781\n0.698922\n00:02\n\n\n77\n0.492758\n0.762113\n0.760283\n0.697219\n00:02\n\n\n78\n0.495429\n0.760835\n0.760252\n0.698354\n00:02\n\n\n79\n0.500510\n0.766248\n0.759687\n0.695800\n00:02\n\n\n80\n0.491652\n0.764638\n0.760198\n0.693530\n00:02\n\n\n81\n0.495746\n0.766025\n0.760069\n0.694098\n00:02\n\n\n82\n0.492518\n0.767861\n0.759896\n0.694381\n00:02\n\n\n83\n0.492122\n0.767575\n0.759625\n0.695800\n00:02\n\n\n84\n0.495317\n0.768247\n0.758977\n0.695233\n00:02\n\n\n85\n0.503044\n0.767743\n0.759250\n0.696935\n00:02\n\n\n86\n0.488295\n0.768618\n0.759364\n0.696368\n00:02\n\n\n87\n0.495461\n0.769804\n0.759051\n0.694381\n00:02\n\n\n88\n0.499347\n0.769520\n0.758997\n0.695233\n00:02\n\n\n89\n0.511932\n0.769175\n0.758995\n0.695233\n00:02\n\n\n90\n0.504696\n0.769521\n0.758882\n0.694949\n00:02\n\n\n91\n0.491524\n0.769784\n0.758762\n0.694098\n00:02\n\n\n92\n0.498932\n0.769638\n0.758810\n0.694665\n00:02\n\n\n93\n0.493265\n0.770175\n0.758754\n0.694665\n00:02\n\n\n94\n0.497908\n0.770099\n0.758764\n0.694665\n00:02\n\n\n95\n0.486489\n0.769981\n0.758742\n0.694665\n00:02\n\n\n96\n0.496604\n0.770035\n0.758724\n0.694665\n00:02\n\n\n97\n0.485646\n0.769963\n0.758733\n0.694381\n00:02\n\n\n98\n0.499033\n0.769941\n0.758741\n0.694381\n00:02\n\n\n99\n0.490694\n0.769939\n0.758742\n0.694381\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere may be differences between the implementation of the Transformer in pytorch and tsai (for example, pytorch uses LayerNorm in the TransformerEncoder layer, which is popular in NLP, while tsai employs BatchNorm)"
  },
  {
    "objectID": "posts/Learner.html",
    "href": "posts/Learner.html",
    "title": "Redesign your Training Loop with CallBacks",
    "section": "",
    "text": "Building a Flexible Training Loop via a system of callbacks - fastai course part 2 2022"
  },
  {
    "objectID": "posts/Learner.html#minimalist-training-loop",
    "href": "posts/Learner.html#minimalist-training-loop",
    "title": "Redesign your Training Loop with CallBacks",
    "section": "Minimalist Training Loop",
    "text": "Minimalist Training Loop\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb,yb in train_dl:\n            loss = loss_func(model(xb), yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n        model.eval()\n        with torch.no_grad():\n            tot_loss,tot_acc,count = 0.,0.,0\n            for xb,yb in valid_dl:\n                pred = model(xb)\n                n = len(xb)\n                count += n\n                tot_loss += loss_func(pred,yb).item()*n\n                tot_acc  += accuracy (pred,yb).item()*n\n        print(epoch, tot_loss/count, tot_acc/count)\n    return tot_loss/count, tot_acc/count\nIf you are a data scientist, you might be familiar with the code above - a minimalist training loop. It does a couple of things:\n\nTraining through a number of epochs\nIn each epoch, loop through every batch\nIn each batch, depending on if the model is training or validating, having different behaviors\n\nHowever, most of the time, we won’t stop here … What if 🤔 :\n\nThe dataset is unbalanced, we should better adding more metrics than just accuracy, maybe F1, ROCAUC, …\nYou want to log the result: in a file/show it in a realtime graph / push it on WandB\nEarly Stopping, Save Best Model, and Much more …\n\nThen you will add more and more ideas in the training loop, to a point that changing anything becomes a headache. If you want to reactivate an old feature that you’ve tried last week, combine several ideas, … Big chance that you will just create a new notebook with Learner-Copy1, Learner-Copy2, Learner-CopyN, …\nLet’s design a new Flexible Training Loop where we keep it as simple as possible but also having a full power of plugging new ideas via callbacks.\n\n\n\n\n\n\nNote\n\n\n\nCallback here is not a python feature but a design concept. It just mean triggering a function when you’ve done something"
  },
  {
    "objectID": "posts/Learner.html#a-learner-with-callbacks",
    "href": "posts/Learner.html#a-learner-with-callbacks",
    "title": "Redesign your Training Loop with CallBacks",
    "section": "A Learner with CallBacks",
    "text": "A Learner with CallBacks\nThere are 3 core pieces of a training loop\n\nFit\nEpoch\nBatch\n\nWe will wrap around each event here with a before and after methods (so before_batch after_batch before_epoch after_epoch before_fit after_fit) with full access to the Learner (which includes everything: the model, optimizer, dataloader, … ).\nclass Learner():\n    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n\n    def one_batch(self):\n        self.preds = self.model(self.batch[0])\n        self.loss = self.loss_func(self.preds, self.batch[1])\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n            self.opt.zero_grad()\n\n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        try:\n            self.callback('before_epoch')\n            for self.iter,self.batch in enumerate(self.dl):\n                try:\n                    self.callback('before_batch')\n                    self.one_batch()\n                    self.callback('after_batch')\n                except CancelBatchException: pass\n            self.callback('after_epoch')\n        except CancelEpochException: pass\n    \n    def fit(self, n_epochs):\n        self.n_epochs = n_epochs\n        self.epochs = range(n_epochs)\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        try:\n            self.callback('before_fit')\n            for self.epoch in self.epochs:\n                self.one_epoch(True)\n                self.one_epoch(False)\n            self.callback('after_fit')\n        except CancelFitException: pass\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n\n\n\n\n\n\nNote\n\n\n\nThe exception here is for adding more control - exit an event when we need\n\n\nPause for a second and imagine if you want to write a simple DeviceCallBack, what will you do ? …\nSo:\n\nBefore_fit: Model -&gt; CUDA\nBefore_batch: (Input, Label) -&gt; CUDA\n\n# from fastai course 2022 p2\nclass DeviceCB(Callback):\n    def __init__(self, device=def_device): fc.store_attr()\n    def before_fit(self, learn):\n        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)\nWhat about a MetricsCallback based on torcheval (a library from Pytorch for model evaluations). Well, we might need some basic steps below:\n\nBefore_epoch: Reset all metrics\nAfter_batch: Update new values to metrics ( accuracy, loss, … per batch )\nAfter_epoch: Compute the final metrics for this epoch (Ex: Weight Average of Accuracy based on a list of accuracy per batch). Then print the result, push it on WandB, …\n\n# from fastai course 2022 p2\nclass MetricsCB(Callback):\n    def __init__(self, *ms, **metrics):\n        for o in ms: metrics[type(o).__name__] = o\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics['loss'] = self.loss = Mean()\n\n    def _log(self, d): \n        print(d)\n        wandb.log(log)\n        \n    def before_fit(self, learn): learn.metrics = self\n    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n\n    def after_epoch(self, learn):\n        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n        log['epoch'] = learn.epoch\n        log['train'] = 'train' if learn.model.training else 'eval'\n        self._log(log)\n        \n\n    def after_batch(self, learn):\n        x,y,*_ = to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n        self.loss.update(to_cpu(learn.loss), weight=len(x))\n\n\n\nDiagram of how to plug MetricsCB and DeviceCB to Learner\n\n\n\n\n\nWandB Log"
  },
  {
    "objectID": "posts/Learner.html#how-to-go-even-further",
    "href": "posts/Learner.html#how-to-go-even-further",
    "title": "Redesign your Training Loop with CallBacks",
    "section": "How to go even further?",
    "text": "How to go even further?\nWe don’t want to repeat ourselves so the before and after in the previous Learner version can be refactored. We can use @decorator or @contextmanager for wrapping things before and after the event.\nThe fastai course experiment with this idea further with even considering 'predict','get_loss','backward','step','zero_grad' as events. It means there is nothing that we can not control here. An example of when it is useful is experimenting with a Momentum Learner by rewriting the zero_grad\n#|export\nclass MomentumLearner(TrainLearner):\n    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n\n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p.grad *= self.mom\n     \nInstead of forgetting all the previous gradients by assigning them to zero, we can multiply them by a number &lt; 1. So in the next update, we also take into account what we’ve trained before ( or momentum )."
  },
  {
    "objectID": "posts/captcha.html",
    "href": "posts/captcha.html",
    "title": "Captcha prediction - From CNN to CRNN",
    "section": "",
    "text": "3 approaches predicting the captcha with &gt; 95% accuracy\nDo you want a little bit more challenge than a traditional Image Classification? Let’s see if we can classify a sequence of classes rather than a single one ;).\nIn this blog post, I will try to break the captcha using 3 different approaches.\nIn this blog post:\nSpecial thanks to these references below for helping me out during this development: - Fastai Captcha Recognition by Augustas Macijauskas - CRNN-Pytorch repo by GitYCC"
  },
  {
    "objectID": "posts/captcha.html#mid-level-fastai-dataloaders",
    "href": "posts/captcha.html#mid-level-fastai-dataloaders",
    "title": "Captcha prediction - From CNN to CRNN",
    "section": "Mid-Level fastai Dataloaders",
    "text": "Mid-Level fastai Dataloaders\nIn this part, we will use the Mid-Level API fastai to load data. This tool will help us to create Dataloaders which compatible with all the fastai ecosystems\nIn brief, we will create a CaptchaTransform ( similar to a Pytorch Datasets ) which returns something showable (CaptchaImage in this case)\nTake a look at this fastai tutorial for more details\n\nfrom fastai.vision.all import *\nimport PIL\nfrom torch.nn import CTCLoss\nfrom scipy.special import logsumexp\n\n\npath = untar_data('https://github.com/AakashKumarNain/CaptchaCracker/raw/master/captcha_images_v2.zip')\n\n\nimgs = get_image_files(path)\n\n\nimgs\n\n(#1040) [Path('/home/ubuntu/.fastai/data/captcha_images_v2/by5y3.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/efb3f.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/76y6f.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/e2d66.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/c6we6.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/p2m6n.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/d66cn.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/2yggg.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/cffp4.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/5npdn.png')...]\n\n\n\n\n\n\n\n\nNote\n\n\n\nBelow is the mapping from label to index and vice-versa. The index starts from 1 because we save the 0 for UNKNOWN class which is use in the last section CNN + RNN\n\n\n\n# Find all the unique labels\nld = set()\nfor f in imgs:\n    for l in f.stem:\n        ld.add(l)\n\nlabel_mapper = \"\".join(sorted(ld))\nl2i = { label_mapper[i]: i+1 for i in range(len(label_mapper)) } # labels to int + BLANK LABEL\ni2l = { v: k for k, v in l2i.items() } # int to labels\n\n\nl2i, i2l\n\n({'2': 1,\n  '3': 2,\n  '4': 3,\n  '5': 4,\n  '6': 5,\n  '7': 6,\n  '8': 7,\n  'b': 8,\n  'c': 9,\n  'd': 10,\n  'e': 11,\n  'f': 12,\n  'g': 13,\n  'm': 14,\n  'n': 15,\n  'p': 16,\n  'w': 17,\n  'x': 18,\n  'y': 19},\n {1: '2',\n  2: '3',\n  3: '4',\n  4: '5',\n  5: '6',\n  6: '7',\n  7: '8',\n  8: 'b',\n  9: 'c',\n  10: 'd',\n  11: 'e',\n  12: 'f',\n  13: 'g',\n  14: 'm',\n  15: 'n',\n  16: 'p',\n  17: 'w',\n  18: 'x',\n  19: 'y'})\n\n\n\ndef label_func(path): return tensor([l2i[l] for l in path.stem])\n\n\ndef open_image(fname):\n    img = PIL.Image.open(fname).convert('RGB')\n    t = torch.Tensor(np.array(img))\n    return t.permute(2,0,1).double()/255.0\n\n\nclass CaptchaImage(fastuple):\n    def show(self, ctx=None, **kwargs): \n        img,labels = self\n        t = tensor(img)\n        return show_image(t, title=''.join(str(i2l[i.item()]) for i in labels), ctx=ctx, **kwargs)\n\n\nclass CaptchaTransform(Transform):\n    def __init__(self, files):\n        self.files = files\n        \n    def encodes(self, i):\n        file = self.files[i]\n        label = label_func(file)\n        img = open_image(file)\n        return CaptchaImage(TensorImage(img), label)\n\n\nbs = 8\n\n\nsplitter = RandomSplitter(valid_pct=0.1) \ntrain_idx , valid_idx = splitter(imgs) \ntrain_files = imgs[train_idx]\nvalid_files = imgs[valid_idx]\ntrain_tl= TfmdLists(range(len(train_files)), CaptchaTransform(train_files))\nvalid_tl= TfmdLists(range(len(valid_files)), CaptchaTransform(valid_files))\ndls = DataLoaders.from_dsets(train_tl, \n                             valid_tl, \n                             after_item=Resize((50,200), method=ResizeMethod.Squish),\n                             after_batch=[Rotate(max_deg=10),\n                                          Brightness(max_lighting=0.5, p=0.8, batch=False),\n                                          Contrast(max_lighting=0.5, p=0.8, batch=False)], \n                             bs=bs)\n\n\n@typedispatch\ndef show_batch(x:CaptchaImage, y, samples, ctxs=None, max_n=6, nrows=None, ncols=2, figsize=None, **kwargs):\n    if figsize is None: figsize = (ncols*6, max_n//ncols * 3)\n    if ctxs is None: ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize)\n    for i,ctx in enumerate(ctxs): CaptchaImage(x[0][i],x[1][i]).show(ctx=ctx)\n\n\none_batch = dls.one_batch()\n\n\ndls.show_batch(max_n=10)\n\n\n\n\n\nn_chars = len(i2l)\n\n\nn_chars * 5\n\n95"
  },
  {
    "objectID": "posts/captcha.html#an-image-classification-model-with-a-tweak-on-output-dimension",
    "href": "posts/captcha.html#an-image-classification-model-with-a-tweak-on-output-dimension",
    "title": "Captcha prediction - From CNN to CRNN",
    "section": "An Image Classification Model with a tweak on output dimension",
    "text": "An Image Classification Model with a tweak on output dimension\nA simple image classification model is used here with the output dimension: Number_of_vocab x Number_of_classes. Then, while calculating the loss function, we reshape the dimension and calculating the Cross-Entropy loss for each class (The fastai CrossEntropyLossFlat can help you to specify which axis that you want to calculate the Cross-Entropy loss on)\n\n\n\nsimple tweak on output\n\n\n\nn_class = n_chars + 1\n\n\nmodel = create_cnn_model(xresnet34, n_class*5)\n\n\ncrit = LabelSmoothingCrossEntropyFlat()\n\n\ndef loss_captcha(output, target):\n    output = output.view(-1, 5, n_class)\n    return crit(output, target)\n\n\ndef char_accu(inp, targ, axis=-1):\n    inps = inp.reshape(-1, 5, n_class)\n    pred = inps.argmax(dim=-1)\n    return (pred == targ).sum()/(pred.shape[0]*pred.shape[1])\n\n\ndef captcha_accu(inp, targ, axis=-1):\n    inps = inp.reshape(-1, 5, n_class)\n    pred = inps.argmax(dim=-1)\n    return ((pred == targ).all(axis=1)).sum()/targ.shape[0]\n\n\nlearn = Learner(dls, model, loss_captcha, metrics=[char_accu, captcha_accu])\n\n\nmodel = model.cuda()\ndls = dls.cuda()\n\n\nlearn.fit_one_cycle(60, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nchar_accu\ncaptcha_accu\ntime\n\n\n\n\n0\n4.417060\n3.768889\n0.065385\n0.000000\n00:05\n\n\n1\n4.238056\n5.701259\n0.055769\n0.000000\n00:05\n\n\n2\n3.933014\n3.674879\n0.088462\n0.000000\n00:05\n\n\n3\n3.719078\n5.090882\n0.075000\n0.000000\n00:05\n\n\n4\n3.548473\n3.824108\n0.132692\n0.000000\n00:05\n\n\n5\n3.366105\n3.826407\n0.113462\n0.000000\n00:05\n\n\n6\n3.136746\n3.577558\n0.115385\n0.000000\n00:05\n\n\n7\n2.936203\n3.092070\n0.194231\n0.000000\n00:05\n\n\n8\n2.729806\n3.936151\n0.196154\n0.000000\n00:05\n\n\n9\n2.521348\n4.372227\n0.161538\n0.000000\n00:05\n\n\n10\n2.263230\n3.843181\n0.184615\n0.000000\n00:05\n\n\n11\n2.063784\n6.484088\n0.094231\n0.000000\n00:05\n\n\n12\n1.915519\n5.548406\n0.196154\n0.000000\n00:05\n\n\n13\n1.772406\n2.161972\n0.426923\n0.000000\n00:05\n\n\n14\n1.659181\n2.289798\n0.411538\n0.000000\n00:05\n\n\n15\n1.480252\n6.103688\n0.165385\n0.000000\n00:05\n\n\n16\n1.440136\n1.914036\n0.509615\n0.009615\n00:05\n\n\n17\n1.336665\n3.500629\n0.332692\n0.009615\n00:05\n\n\n18\n1.275441\n1.713563\n0.590385\n0.076923\n00:05\n\n\n19\n1.219574\n1.325677\n0.738462\n0.240385\n00:05\n\n\n20\n1.165449\n1.249834\n0.771154\n0.298077\n00:05\n\n\n21\n1.132226\n1.372405\n0.711538\n0.163462\n00:05\n\n\n22\n1.072073\n1.138467\n0.823077\n0.403846\n00:05\n\n\n23\n1.045686\n1.143437\n0.832692\n0.432692\n00:05\n\n\n24\n1.027944\n1.020135\n0.890385\n0.605769\n00:05\n\n\n25\n0.980347\n0.953016\n0.905769\n0.673077\n00:05\n\n\n26\n0.955928\n0.885346\n0.938461\n0.769231\n00:05\n\n\n27\n0.938319\n0.880523\n0.940385\n0.778846\n00:05\n\n\n28\n0.912880\n0.909565\n0.926923\n0.692308\n00:05\n\n\n29\n0.888791\n0.846983\n0.957692\n0.836538\n00:05\n\n\n30\n0.877755\n0.829840\n0.959615\n0.855769\n00:05\n\n\n31\n0.854199\n0.870150\n0.946154\n0.788462\n00:05\n\n\n32\n0.838330\n0.819317\n0.959615\n0.855769\n00:05\n\n\n33\n0.824045\n0.791964\n0.978846\n0.951923\n00:05\n\n\n34\n0.821468\n0.794070\n0.969231\n0.894231\n00:05\n\n\n35\n0.803675\n0.774516\n0.969231\n0.913462\n00:05\n\n\n36\n0.800759\n0.771277\n0.975000\n0.932692\n00:05\n\n\n37\n0.784726\n0.774723\n0.971154\n0.913462\n00:05\n\n\n38\n0.776317\n0.733620\n0.986538\n0.971154\n00:05\n\n\n39\n0.767470\n0.736605\n0.982692\n0.951923\n00:05\n\n\n40\n0.757092\n0.729704\n0.978846\n0.942308\n00:05\n\n\n41\n0.755353\n0.721849\n0.984615\n0.961538\n00:05\n\n\n42\n0.754742\n0.728808\n0.978846\n0.951923\n00:05\n\n\n43\n0.748297\n0.719470\n0.982692\n0.961538\n00:05\n\n\n44\n0.742404\n0.716713\n0.986538\n0.971154\n00:05\n\n\n45\n0.735511\n0.707033\n0.984615\n0.961538\n00:05\n\n\n46\n0.735871\n0.699644\n0.984615\n0.961538\n00:06\n\n\n47\n0.728447\n0.696534\n0.984615\n0.961538\n00:05\n\n\n48\n0.723877\n0.706259\n0.984615\n0.961538\n00:05\n\n\n49\n0.728841\n0.698290\n0.984615\n0.961538\n00:05\n\n\n50\n0.721174\n0.697634\n0.984615\n0.961538\n00:05\n\n\n51\n0.720327\n0.697556\n0.986538\n0.971154\n00:05\n\n\n52\n0.714673\n0.690376\n0.988462\n0.971154\n00:06\n\n\n53\n0.717317\n0.693437\n0.984615\n0.961538\n00:05\n\n\n54\n0.714745\n0.691280\n0.986538\n0.971154\n00:05\n\n\n55\n0.715825\n0.685640\n0.984615\n0.961538\n00:06\n\n\n56\n0.711018\n0.691839\n0.986538\n0.971154\n00:05\n\n\n57\n0.713582\n0.688716\n0.986538\n0.971154\n00:06\n\n\n58\n0.713092\n0.687872\n0.986538\n0.971154\n00:06\n\n\n59\n0.711346\n0.691615\n0.988462\n0.971154\n00:05\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\nWow! With this simple trick, we can reach 97% accuracy for the prediction 5-digits captcha. Very impressive! Let’s see if we can do it better with other techniques"
  },
  {
    "objectID": "posts/captcha.html#remove-the-adaptiveavgpool2d-to-reserve-spatial-information",
    "href": "posts/captcha.html#remove-the-adaptiveavgpool2d-to-reserve-spatial-information",
    "title": "Captcha prediction - From CNN to CRNN",
    "section": "Remove the AdaptiveAvgPool2d to reserve spatial information",
    "text": "Remove the AdaptiveAvgPool2d to reserve spatial information\nIn the model used above, between the body and head, there is an AdaptiveAvgPool2d layer, which blurs all essential spatial information. So let’s remove it and create our own head\n\n\n\nReceptive field with and without Adaptive Average Pooling\n\n\n\n\n\n\n\n\nIntuition\n\n\n\nFrom the Illustration above, we can see that, with AdaptiveAvgPool2d, each element in the feature vector must understand the whole Captcha, to classify correctly. To facilitate the work, by removing the Pooling layer, each feature needs to represent only a part of the Captcha, or in the best case, a letter. Combining all the letter’s features together can give us a Captcha prediction\n\n\n\nbody = create_body(xresnet34)\n\n\nhead = nn.Sequential(\n    Flatten(),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(7168,1000),\n    nn.ReLU(),\n    nn.BatchNorm1d(1000),\n    nn.Dropout(0.5),\n    nn.Linear(1000,n_class*5),\n)\n\n\nmodel = nn.Sequential(body, head)\n\n\nmodel.cuda()\ndls.cuda()\n\n&lt;fastai.data.core.DataLoaders&gt;\n\n\n\nlearn = Learner(dls, model, loss_captcha, metrics=[char_accu, captcha_accu])\n\n\nlearn.fit_one_cycle(60, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nchar_accu\ncaptcha_accu\ntime\n\n\n\n\n0\n3.056647\n2.770755\n0.221154\n0.000000\n00:06\n\n\n1\n2.707805\n2.455260\n0.351923\n0.019231\n00:06\n\n\n2\n2.307795\n2.130244\n0.434615\n0.048077\n00:06\n\n\n3\n1.944143\n1.862135\n0.544231\n0.067308\n00:06\n\n\n4\n1.663925\n1.757791\n0.603846\n0.076923\n00:06\n\n\n5\n1.499317\n1.516997\n0.653846\n0.153846\n00:06\n\n\n6\n1.436849\n1.711082\n0.590385\n0.048077\n00:06\n\n\n7\n1.486420\n1.472152\n0.676923\n0.153846\n00:06\n\n\n8\n1.389985\n1.436426\n0.725000\n0.192308\n00:06\n\n\n9\n1.344463\n1.969319\n0.588462\n0.067308\n00:06\n\n\n10\n1.500018\n1.610145\n0.661538\n0.105769\n00:06\n\n\n11\n1.370128\n1.287863\n0.821154\n0.451923\n00:06\n\n\n12\n1.320343\n1.476295\n0.742308\n0.240385\n00:06\n\n\n13\n1.276946\n3.231544\n0.340385\n0.000000\n00:06\n\n\n14\n1.580945\n1.563601\n0.763462\n0.307692\n00:06\n\n\n15\n1.444904\n1.174396\n0.834615\n0.432692\n00:06\n\n\n16\n1.276948\n1.223626\n0.817308\n0.403846\n00:06\n\n\n17\n1.145310\n1.043204\n0.901923\n0.653846\n00:06\n\n\n18\n1.079106\n1.131395\n0.909615\n0.701923\n00:06\n\n\n19\n1.054410\n1.013260\n0.901923\n0.634615\n00:06\n\n\n20\n1.059869\n1.099213\n0.892308\n0.634615\n00:06\n\n\n21\n1.031296\n0.993608\n0.905769\n0.663462\n00:06\n\n\n22\n0.994466\n0.873521\n0.944231\n0.788462\n00:06\n\n\n23\n1.009694\n0.865026\n0.948077\n0.817308\n00:06\n\n\n24\n0.932827\n1.105105\n0.951923\n0.875000\n00:06\n\n\n25\n0.887062\n0.852510\n0.950000\n0.836538\n00:06\n\n\n26\n0.884278\n0.817573\n0.959615\n0.875000\n00:06\n\n\n27\n0.862702\n0.815369\n0.963462\n0.865385\n00:06\n\n\n28\n0.853542\n0.814780\n0.969231\n0.903846\n00:06\n\n\n29\n0.852293\n0.793552\n0.971154\n0.903846\n00:06\n\n\n30\n0.832754\n1.187254\n0.892308\n0.605769\n00:06\n\n\n31\n0.824426\n0.905626\n0.971154\n0.923077\n00:06\n\n\n32\n0.802780\n0.747221\n0.973077\n0.913462\n00:06\n\n\n33\n0.807151\n1.262912\n0.957692\n0.913462\n00:06\n\n\n34\n0.788464\n0.744047\n0.980769\n0.942308\n00:06\n\n\n35\n0.772782\n0.724258\n0.982692\n0.932692\n00:06\n\n\n36\n0.764042\n0.716053\n0.980769\n0.951923\n00:06\n\n\n37\n0.755922\n0.726421\n0.980769\n0.942308\n00:06\n\n\n38\n0.755969\n0.716299\n0.984615\n0.951923\n00:06\n\n\n39\n0.742237\n0.709827\n0.988461\n0.971154\n00:06\n\n\n40\n0.741012\n0.700651\n0.986538\n0.961538\n00:06\n\n\n41\n0.739579\n0.746366\n0.975000\n0.923077\n00:06\n\n\n42\n0.734317\n0.740812\n0.978846\n0.942308\n00:06\n\n\n43\n0.728524\n0.689804\n0.984615\n0.951923\n00:06\n\n\n44\n0.721895\n0.686216\n0.984615\n0.961538\n00:06\n\n\n45\n0.718292\n0.680776\n0.984615\n0.961538\n00:06\n\n\n46\n0.711961\n0.675663\n0.988462\n0.971154\n00:06\n\n\n47\n0.711802\n0.678798\n0.988461\n0.971154\n00:06\n\n\n48\n0.712833\n0.678948\n0.986538\n0.961538\n00:06\n\n\n49\n0.711009\n0.678042\n0.984615\n0.961538\n00:06\n\n\n50\n0.705792\n0.671570\n0.986538\n0.961538\n00:06\n\n\n51\n0.703747\n0.669645\n0.986538\n0.961538\n00:06\n\n\n52\n0.700930\n0.668842\n0.988462\n0.971154\n00:06\n\n\n53\n0.700268\n0.667778\n0.988461\n0.971154\n00:06\n\n\n54\n0.698437\n0.673563\n0.988461\n0.971154\n00:06\n\n\n55\n0.702755\n0.665972\n0.988462\n0.961538\n00:06\n\n\n56\n0.699661\n0.668209\n0.988461\n0.971154\n00:06\n\n\n57\n0.695964\n0.666184\n0.988461\n0.971154\n00:06\n\n\n58\n0.697385\n0.665079\n0.992308\n0.980769\n00:06\n\n\n59\n0.702616\n0.668129\n0.988462\n0.961538\n00:06\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\nWe have a quite similar result to the previous model after 60 epochs. However, this one learns much faster. After 15 epochs, it attains already 43% captcha accuracy while the With AdaptiveAvgPool2d is still at 0%"
  },
  {
    "objectID": "posts/captcha.html#crnn-ctc-loss",
    "href": "posts/captcha.html#crnn-ctc-loss",
    "title": "Captcha prediction - From CNN to CRNN",
    "section": "CRNN + CTC Loss",
    "text": "CRNN + CTC Loss\nOne can imagine, from the intuition of the last section, if we can extract features from letters and then predict the captcha, How about using a Recurrent Neural Network (RNN)? Is it for solving sequence problems right?\nYes, yes, It is the CRNN.\n\n\n\nCRNN Model\n\n\n\n\n\n\n\n\nNote\n\n\n\nFeel free to run the model step by step through each layer to understand better the dimension\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Sequence Length of the output doesn’t necessarily equal to the Captcha Length (which is 5 in our case) because our loss function CTC Loss knows how to handle it\n\n\nThe CNN-Body model I use here is resnet34, but not the entire one. We cut it after some layers. The reason is, the deeper the image passes through the CNN the more its Width shrink, and it can not be smaller than our captcha length (which is 5). Below you can see I choose to cut after 7 layers so the feature’s Width is still 13 (the last dimension of the tensor)\n\nbody = create_body(resnet34, cut=7)\n\n\nn_class = n_chars + 1\n\n\nn_class\n\n20\n\n\nBy running the CNN body manually, I can know the number of output features, height and width which will be used later for building the CRNN model\n\nbody(one_batch[0].cpu()).shape\n\ntorch.Size([8, 256, 4, 13])\n\n\n\nclass CRNN(nn.Module):\n    def __init__(self, output_channel, H, W, n_class, map_to_seq_hidden=64, rnn_hidden_1=256, rnn_hidden_2=128):\n        super(CRNN, self).__init__()\n        self.body = create_body(resnet34, cut=7)\n        self.map_to_seq = LinBnDrop(output_channel * H, map_to_seq_hidden, p=0.1, bn=False, lin_first=False)\n        self.rnn1 = nn.LSTM(map_to_seq_hidden, rnn_hidden_1, bidirectional=True)\n        self.rnn2 = nn.LSTM(2 * rnn_hidden_1, rnn_hidden_2, bidirectional=True)\n        self.dense = LinBnDrop(2 * rnn_hidden_2, n_class, p=0.1, bn=False, lin_first=False)\n        \n    def forward(self, images):\n        # shape of images: (batch, channel, height, width)\n\n        conv = self.body(images)\n        batch, channel, height, width = conv.size()\n\n        conv = conv.view(batch, channel * height, width)\n        conv = conv.permute(2, 0, 1)  # (width, batch, feature)\n        seq = self.map_to_seq(conv)\n\n        recurrent, _ = self.rnn1(seq)\n        recurrent, _ = self.rnn2(recurrent)\n\n        output = self.dense(recurrent)\n        return output  # shape: (seq_len, batch, num_class)\n\n\noutput_channel = 256\nH = 4\nW = 13\n\n\nmodel = CRNN(output_channel, H, W, n_class)\n\n\nsteps = W\n\nThe loss function we use here is CTC Loss. In brief, it is a loss function that can handle a sequence classification without a specific alignment (Because we don’t have a character-level dataset and the span of each character is random). An Illustration is below (Taken from https://sid2697.github.io/Blog_Sid/algorithm/2019/10/19/CTC-Loss.html). In CTC Loss, it allows a repeated prediction with a character that spans through multiple positions and also a blank character. However, we need a decoder to decode later the output. I will talk about it in the next section\n\n\n\nCTC Loss Illustration\n\n\n\ncriterion = nn.CTCLoss()\n\n\ndef loss_captcha_ctc(output, target):\n    batch_size = target.shape[0]\n    input_lengths = torch.LongTensor([steps] * batch_size)\n    target_lengths = torch.LongTensor([5] * batch_size)\n    log_probs = torch.nn.functional.log_softmax(output, dim=2)\n    return criterion(log_probs, target, input_lengths, target_lengths)\n\n\nDecoder and Metrics\nAs the output of CRNN model is not exactly corresponding to the Groud-Truth, we must have something to decode it and get the final prediction. Check this tutorial for more details https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7. Below there is code for the greedy and beam-search technique\n\nblank = 0\nbeam_size = 10\n\n\nNINF = -1 * float('inf')\nDEFAULT_EMISSION_THRESHOLD = 0.01\n\n\ndef _reconstruct(labels, blank=0):\n    new_labels = []\n    # merge same labels\n    previous = None\n    for l in labels:\n        if l != previous:\n            new_labels.append(l)\n            previous = l\n    # delete blank\n    new_labels = [l for l in new_labels if l != blank]\n    return new_labels\n\n\ndef greedy_decode(emission_log_prob, blank=0, **kwargs):\n    labels = np.argmax(emission_log_prob, axis=-1)\n    labels = _reconstruct(labels, blank=blank)\n    return labels\n\n\ndef beam_search_decode(emission_log_prob, blank=0, **kwargs):\n    beam_size = kwargs['beam_size']\n    emission_threshold = kwargs.get('emission_threshold', np.log(DEFAULT_EMISSION_THRESHOLD))\n\n    length, class_count = emission_log_prob.shape\n\n    beams = [([], 0)]  # (prefix, accumulated_log_prob)\n    for t in range(length):\n        new_beams = []\n        for prefix, accumulated_log_prob in beams:\n            for c in range(class_count):\n                log_prob = emission_log_prob[t, c]\n                if log_prob &lt; emission_threshold:\n                    continue\n                new_prefix = prefix + [c]\n                # log(p1 * p2) = log_p1 + log_p2\n                new_accu_log_prob = accumulated_log_prob + log_prob\n                new_beams.append((new_prefix, new_accu_log_prob))\n\n        # sorted by accumulated_log_prob\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:beam_size]\n\n    # sum up beams to produce labels\n    total_accu_log_prob = {}\n    for prefix, accu_log_prob in beams:\n        labels = tuple(_reconstruct(prefix, blank))\n        # log(p1 + p2) = logsumexp([log_p1, log_p2])\n        total_accu_log_prob[labels] = \\\n            logsumexp([accu_log_prob, total_accu_log_prob.get(labels, NINF)])\n\n    labels_beams = [(list(labels), accu_log_prob)\n                    for labels, accu_log_prob in total_accu_log_prob.items()]\n    labels_beams.sort(key=lambda x: x[1], reverse=True)\n    labels = labels_beams[0][0]\n    return labels\n\nAs we can have a prediction from the decoder that has length &gt; 5, we don’t have character level accuracy for the metrics but only whole captcha accuracy\n\ndef captcha_accu_ctc(pred, targ, axis=-1):\n    log_probs = torch.nn.functional.log_softmax(pred, dim=2)\n    emission_log_probs = np.transpose(log_probs.detach().cpu().numpy(), (1, 0, 2))\n    decoder = beam_search_decode\n    label2char = i2l\n    \n    decoded_list = []\n    for emission_log_prob in emission_log_probs:\n        decoded = decoder(emission_log_prob, blank=blank, beam_size=beam_size)\n        decoded_list.append(decoded)\n    \n    count_ok = 0\n    for decode, gt in zip(decoded_list, targ):\n        if len(decode) == len(gt):\n            count_ok += (torch.tensor(decode).cuda() == gt).all().item()\n            \n    return count_ok/targ.shape[0]\n\n\n# loss_captcha_ctc(pred, one_batch[1])\n\n\ndls = dls.cuda()\nmodel = model.cuda()\n\n\nlearn = Learner(dls, model, loss_func=loss_captcha_ctc, metrics=[captcha_accu_ctc])\n# learn = Learner(dls, model, loss_func=loss_captcha_ctc)\n\n\nlearn.fit_one_cycle(1,1e-6)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ncaptcha_accu_ctc\ntime\n\n\n\n\n0\n0.001245\n0.070678\n0.961538\n00:06\n\n\n\n\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0014454397605732083)\n\n\n\n\n\n\nlearn.fit_one_cycle(60, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ncaptcha_accu_ctc\ntime\n\n\n\n\n0\n3.364362\n3.221772\n0.000000\n00:05\n\n\n1\n3.239760\n3.222240\n0.000000\n00:05\n\n\n2\n3.198520\n3.179216\n0.000000\n00:05\n\n\n3\n3.068399\n3.082180\n0.000000\n00:05\n\n\n4\n2.935494\n2.940528\n0.000000\n00:05\n\n\n5\n2.778541\n2.834429\n0.000000\n00:05\n\n\n6\n2.595863\n2.414017\n0.000000\n00:05\n\n\n7\n2.398474\n2.306193\n0.000000\n00:05\n\n\n8\n2.133149\n2.030112\n0.009615\n00:05\n\n\n9\n1.933357\n2.035026\n0.000000\n00:05\n\n\n10\n1.740602\n3.510546\n0.000000\n00:05\n\n\n11\n1.571230\n1.606472\n0.000000\n00:05\n\n\n12\n1.481388\n1.380839\n0.028846\n00:05\n\n\n13\n1.363306\n1.311289\n0.028846\n00:05\n\n\n14\n1.196686\n1.012929\n0.125000\n00:05\n\n\n15\n0.932915\n0.713615\n0.192308\n00:05\n\n\n16\n0.730192\n0.596288\n0.278846\n00:05\n\n\n17\n0.589729\n0.484889\n0.442308\n00:05\n\n\n18\n0.393728\n0.315344\n0.567308\n00:05\n\n\n19\n0.335306\n0.282954\n0.663462\n00:05\n\n\n20\n0.253060\n0.164563\n0.826923\n00:05\n\n\n21\n0.192399\n0.202485\n0.778846\n00:05\n\n\n22\n0.152099\n0.170849\n0.788462\n00:05\n\n\n23\n0.165517\n0.182350\n0.769231\n00:05\n\n\n24\n0.143058\n0.116091\n0.875000\n00:05\n\n\n25\n0.140082\n0.098341\n0.884615\n00:05\n\n\n26\n0.107128\n0.153724\n0.846154\n00:05\n\n\n27\n0.104198\n0.064582\n0.903846\n00:05\n\n\n28\n0.075773\n0.164378\n0.826923\n00:05\n\n\n29\n0.073986\n0.095392\n0.942308\n00:05\n\n\n30\n0.121426\n0.098069\n0.875000\n00:05\n\n\n31\n0.061526\n0.085097\n0.913462\n00:05\n\n\n32\n0.050841\n0.033660\n0.961538\n00:05\n\n\n33\n0.041742\n0.113056\n0.884615\n00:05\n\n\n34\n0.037825\n0.035997\n0.980769\n00:05\n\n\n35\n0.042165\n0.057766\n0.923077\n00:05\n\n\n36\n0.040758\n0.056158\n0.961538\n00:05\n\n\n37\n0.013569\n0.063323\n0.951923\n00:05\n\n\n38\n0.017145\n0.041725\n0.951923\n00:05\n\n\n39\n0.016637\n0.050826\n0.951923\n00:05\n\n\n40\n0.015019\n0.039701\n0.971154\n00:05\n\n\n41\n0.007477\n0.036252\n0.980769\n00:05\n\n\n42\n0.017243\n0.036068\n0.980769\n00:05\n\n\n43\n0.011176\n0.045191\n0.951923\n00:05\n\n\n44\n0.010634\n0.045656\n0.971154\n00:05\n\n\n45\n0.005593\n0.048920\n0.961538\n00:05\n\n\n46\n0.002363\n0.049634\n0.961538\n00:05\n\n\n47\n0.005349\n0.049498\n0.961538\n00:05\n\n\n48\n0.012180\n0.036904\n0.971154\n00:05\n\n\n49\n0.001877\n0.035886\n0.980769\n00:05\n\n\n50\n0.002923\n0.043079\n0.971154\n00:05\n\n\n51\n0.002380\n0.034152\n0.980769\n00:05\n\n\n52\n0.002510\n0.038204\n0.980769\n00:05\n\n\n53\n0.001762\n0.034696\n0.980769\n00:05\n\n\n54\n0.002121\n0.037318\n0.980769\n00:05\n\n\n55\n0.000552\n0.043198\n0.971154\n00:05\n\n\n56\n0.000387\n0.041396\n0.980769\n00:05\n\n\n57\n0.001379\n0.037345\n0.980769\n00:05\n\n\n58\n0.002729\n0.040222\n0.971154\n00:05\n\n\n59\n0.000564\n0.051966\n0.971154\n00:05\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\nOk, we have good results too. As it is a very simple dataset, it’s hard to say which technique is better. However, the CRNN training loss is much lower than validation loss, It might be a hint that we can tune it to get more accuracy or training faster? Please DM me if you have an idea about it. Thanks"
  },
  {
    "objectID": "posts/ts_dataloaders.html",
    "href": "posts/ts_dataloaders.html",
    "title": "Dynamic Size DataLoader",
    "section": "",
    "text": "Preparing the Dynamic size DataLoader for sequences with wide distributed length\nWhen you work with sequence data (sensors, music, …), you might encounter a case when the sequence length distribution is very large. For example, a song can last 30s to many minutes. Resampling to fixed size can lose details information. In this blog post, I will talk about how to create a dynamic size DataLoader.\nfrom fastai.vision.all import *\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n/home/hoa/miniconda3/envs/blog/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n  warn(f\"Failed to load image Python extension: {e}\")"
  },
  {
    "objectID": "posts/ts_dataloaders.html#model-works-with-dynamic-size-data",
    "href": "posts/ts_dataloaders.html#model-works-with-dynamic-size-data",
    "title": "Dynamic Size DataLoader",
    "section": "Model works with Dynamic Size data",
    "text": "Model works with Dynamic Size data\nBelow is some examples of layer/model which can deal with the dynamic size data.\n\nAdaptiveAvgPool layer which is used widely in the CNN model\nRNN model\n\n\nAdaptiveAvgPool\nAdaptiveAvgPool2D The output size is fixed no matter the input\n\n\n\nAdaptiveAvgPool2d\n\n\nYou can see below, no matter the input size ( [5,5], [6,4], [4,3] ), the output size is [2,2]\n\nx1 = torch.randn(8,16,5,5)\nx2 = torch.randn(8,16,6,4)\nx2 = torch.randn(8,16,4,3)\n\n\nnn.AdaptiveAvgPool2d(2)(x1).shape, nn.AdaptiveAvgPool2d(2)(x2).shape\n\n(torch.Size([8, 16, 2, 2]), torch.Size([8, 16, 2, 2]))\n\n\n\n\nRNN\nIn a sequence classification problem, we can use a RNN model, extracting the last timestep features and finally passing by a linear layer. So this kind of architecture doesn’t depend on the input either.\n\n\n\nrnn\n\n\nThe inputs below are 3 features and 512-244 steps, after the RNN model, both outputs are [16,100]\n\nx1 = torch.randn(16, 512, 3)\nx2 = torch.randn(16, 224, 3)\n\n\nrnn = nn.RNN(3,100)\n\n\nrnn(x1)[0].shape\n\ntorch.Size([16, 512, 100])\n\n\n\nrnn(x1)[0][:,-1].shape, rnn(x2)[0][:,-1].shape\n\n(torch.Size([16, 100]), torch.Size([16, 100]))\n\n\nThere are many models like that in various libraries. For example: timeseriesai. So the difficulty might not stay in building models but how to prepare the data to make use of their full power. Each problem is different so usually, you have to write the DataLoader yourself\nReference: Some codes are taken from @Ivan answer of the question pytorch-can-i-group-batches-by-length on stackoverflow"
  },
  {
    "objectID": "posts/ts_dataloaders.html#dynamic-dataloader",
    "href": "posts/ts_dataloaders.html#dynamic-dataloader",
    "title": "Dynamic Size DataLoader",
    "section": "Dynamic DataLoader",
    "text": "Dynamic DataLoader\nLet’s first create a dataset where each item is a 1d series with different lengths\n\nDataset\n\nclass DS(Dataset):\n    def __init__(self, signals):\n        super().__init__()\n        self.len = len(signals)\n        self.signals = signals\n\n    def __getitem__(self, i):\n        return torch.tensor(self.signals[i]), torch.tensor([random.randint(0,1)])\n\n    def __len__(self):\n        return self.len\n\n\nsignal_len = np.random.randint(0, 100, (16*6))\nsignals = [np.random.rand(s) for s in signal_len]\nds = DS(signals)\n\n\nsignal_len\n\narray([95, 77, 85, 87, 45, 47, 77, 93, 75,  2, 10, 69, 75, 61, 91, 13, 55,\n       31,  3, 51, 68, 94, 28, 99, 93, 18, 48, 16, 81, 11, 74, 89, 36,  3,\n       28, 92, 42, 36, 11, 44, 26, 63,  5,  4, 25, 54, 23, 23, 83, 33, 77,\n       45, 38, 52, 50, 75, 17, 29,  2, 69, 44, 83,  1, 47, 12, 17, 14, 35,\n       97, 53, 13, 49, 24, 78, 59,  1, 27, 43, 89, 37, 98,  8, 90, 77,  9,\n       29, 46, 44, 10, 25, 35, 37, 49, 98, 17, 25])\n\n\n\n\nCollate Function\nEven when you have a model that can handle input with different sizes, every item in a batch must have the same shape so they can be stacked together (the size of items in a batch are equal, but the sizes of different batches are different ). The collate_fn parameters in PyTorch DataLoader is responsible for this job\n\n\n\ncollate_fn\n\n\n\n\nFixed Size DataLoader with interpolation\n\n\n\n\n\n\nNote\n\n\n\nBefore trying out Dynamic Size DataLoader, you should test the fixed sized first by using: F.interpolate. It can be used as your baseline\n\n\nTake a look at the x-axis in 2 plots below\n\nx = torch.sin(torch.arange(0,5,0.01))\n\n\nplt.plot(x)\nplt.figure()\nplt.plot(np.array(F.interpolate(x[None,None,:], 200)[0].squeeze()))\n\n\n\n\n\n\n\nBelow is the simplest version of a collate_fn you should use in your baseline model\n\nbatch = [ds[i] for i in range(4)]\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s easier to write a function when you can experiment each line in a cell. I usually take out a list of item from the DataSet to experiment with my collate_fn.\n\n\n\nbatch = [ds[i] for i in range(4)]\n\n\ndef collate_fn(batch):\n    (xx, yy) = zip(*batch)\n    x_news = [F.interpolate(_x[None,None,:], (50))[0] for _x in xx]\n    return torch.stack(x_news), torch.concat(yy)\n\n\nxb, yb = collate_fn(batch)\n\n\nxb.shape, yb.shape\n\n(torch.Size([4, 1, 50]), torch.Size([4]))\n\n\nAny item passed through this DataLoader will be rescaled to 50 timesteps. A disadvantage of this technique is that you will have distortion or you might lose details information.\n\n\nDynamic size DataLoader\nIn this DataLoader, each batch will have a different size based on a reference (ex: maximum length of items). For padding, you can try padding with a constant, repeating signal, etc.\n\nthe F.pad function in Pytorch can only work with (padding value/2) &lt; dimension, so you might need to write the padding function yourself\n\nAn example of padding is by repeating using Tensor.repeat\n\nx = torch.sin(torch.arange(0,5,0.01))\n\n\nplt.plot(x)\nplt.figure()\nplt.plot(x.repeat(2))\n\n\n\n\n\n\n\n\ndef collate_padding(batch):\n    (xx, yy) = zip(*batch)\n    x_lens = [len(x) for x in xx]\n    idx_max = np.argmax(x_lens)\n    max_len = x_lens[idx_max]\n    \n    x_pads = []\n    for i, _x in enumerate(xx):\n        if i == idx_max:\n            x_pad = _x\n        else:\n            pad_nb = max_len//_x.shape[0]\n            x_pad = _x.repeat(1+pad_nb)[:max_len]\n        x_pads.append(x_pad)\n    return torch.stack(x_pads), torch.concat(yy)\n\n\nxb, yb = collate_padding(batch)\n\n\nxb.shape, yb.shape\n\n(torch.Size([4, 95]), torch.Size([4]))\n\n\n\ndl = DataLoader(dataset=ds, batch_size=16, shuffle=True, collate_fn=collate_padding)\n\n\nfor x in dl:\n    print(x[0].shape)\n\ntorch.Size([16, 93])\ntorch.Size([16, 98])\ntorch.Size([16, 99])\ntorch.Size([16, 97])\ntorch.Size([16, 94])\ntorch.Size([16, 74])\n\n\n\n\nGrouping batch by size\nIn the padding collate_fn version above, in a batch, if you have one very short item and one is very long, the short one will be repeated again and again and it is irrealistic in the real case (You can see the signal_len is distributed from 1 to 100 but most of the batches above are long). So you might want to group each batch based on their sizes\n\nsignal_len\n\narray([95, 77, 85, 87, 45, 47, 77, 93, 75,  2, 10, 69, 75, 61, 91, 13, 55,\n       31,  3, 51, 68, 94, 28, 99, 93, 18, 48, 16, 81, 11, 74, 89, 36,  3,\n       28, 92, 42, 36, 11, 44, 26, 63,  5,  4, 25, 54, 23, 23, 83, 33, 77,\n       45, 38, 52, 50, 75, 17, 29,  2, 69, 44, 83,  1, 47, 12, 17, 14, 35,\n       97, 53, 13, 49, 24, 78, 59,  1, 27, 43, 89, 37, 98,  8, 90, 77,  9,\n       29, 46, 44, 10, 25, 35, 37, 49, 98, 17, 25])\n\n\nHow do you do that? by using the batch_sampler parameter in DataLoader. Now it’s you to control which item in the Dataset is passed through which iteration in the DataLoader\n\n\n\nBatch grouping\n\n\n\nbatch_nb = 6\n\n\nsampler = np.split(signal_len.argsort()[::-1], batch_nb)\n\n\ndl = DataLoader(dataset=ds, batch_sampler=sampler, collate_fn=collate_padding)\n\n\nbatches\n\n[array([23, 80, 93, 68,  0, 21, 24,  7, 35, 14, 82, 78, 31,  3,  2, 61]),\n array([48, 28, 73,  1, 50,  6, 83, 55, 12,  8, 30, 59, 11, 20, 41, 13]),\n array([74, 16, 45, 69, 53, 19, 54, 92, 71, 26,  5, 63, 86, 51,  4, 60]),\n array([39, 87, 77, 36, 52, 79, 91, 32, 37, 67, 90, 49, 17, 85, 57, 34]),\n array([22, 76, 40, 95, 89, 44, 72, 47, 46, 25, 94, 65, 56, 27, 66, 15]),\n array([70, 64, 29, 38, 10, 88, 84, 81, 42, 43, 33, 18,  9, 58, 75, 62])]\n\n\n\nfor x in dl:\n    print(x[0].shape)\n\ntorch.Size([16, 99])\ntorch.Size([16, 83])\ntorch.Size([16, 59])\ntorch.Size([16, 44])\ntorch.Size([16, 28])\ntorch.Size([16, 13])\n\n\nFrom the shape above, you can see that the size’s length is distributed better in each batch.\n\n\n\n\n\n\nNote\n\n\n\nNow, items in each batch don’t change, so you might lose the randomness in your training process which might lead to an overfit -&gt; Use with caution"
  },
  {
    "objectID": "posts/finetune_clip.html#base-model",
    "href": "posts/finetune_clip.html#base-model",
    "title": "Why and How to Fine-tune CLIP",
    "section": "Base Model",
    "text": "Base Model\nAs mentioned above, the Image Encoder for our CLIP is clip-vit-base-patch32 and Text Encoder is roberta-base\n\nmodel = VisionTextDualEncoderModel.from_vision_text_pretrained(\n    \"openai/clip-vit-base-patch32\", \"roberta-base\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\nimage_processor = AutoImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n\nmodel.save_pretrained(\"clip-roberta\")\nprocessor.save_pretrained(\"clip-roberta\")\n\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe projection layer and logit scale weights `['visual_projection.weight', 'text_projection.weight', 'logit_scale']` are newly initialized. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nCould not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration."
  }
]