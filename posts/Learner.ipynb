{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0d68a1",
   "metadata": {},
   "source": [
    "---\n",
    "title: Flexible Deep Learning Training Loop\n",
    "from: markdown+emoji\n",
    "author: \"Dien-Hoa Truong\"\n",
    "date: 2023-01-15\n",
    "categories: [deeplearning]\n",
    "\n",
    "---\n",
    "\n",
    "Building a Flexible Training Loops via system of callbacks - fastai course part 2 2022 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e94407",
   "metadata": {},
   "source": [
    "![callback](https://iconarchive.com/download/i83699/custom-icon-design/mono-general-3/call-ringing.ico)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb2587",
   "metadata": {},
   "source": [
    "## Minimalist Training Loop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d422b3a7-8fed-4263-8ece-34dad0725ef2",
   "metadata": {},
   "source": [
    "```python\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc,count = 0.,0.,0\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(pred,yb).item()*n\n",
    "                tot_acc  += accuracy (pred,yb).item()*n\n",
    "        print(epoch, tot_loss/count, tot_acc/count)\n",
    "    return tot_loss/count, tot_acc/count\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872226d-6d7e-46a4-bad1-45905702b503",
   "metadata": {},
   "source": [
    "If you are a data scientist, you might be familiar with the code above - a minimalist training loop. It does couple of things:\n",
    "\n",
    "- Training through a number of epochs \n",
    "- In each epoch, loop through every batchs\n",
    "- In each batch, depending on if the model is training or validating, having different behaviours\n",
    "\n",
    "However, in most of the time, we won't stop here ... What if ðŸ¤” :\n",
    "\n",
    "- The dataset is unbalance, we should better adding more metrics than just accuracy, maybe `F1`, `ROCAUC`, ...\n",
    "- You want to log the result: in a file / show in a realtime graph / push it on `WandB`\n",
    "- `Early Stopping`, `Save Best Model`, and Much more ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f10e2-9133-47f8-b40d-0a9d3df603fe",
   "metadata": {},
   "source": [
    "Then you will add more and more ideas in the training loop and. To the point that changing anything becomes a headache. You want to reactivate an old feature that you've tried last week, combine several ideas together, ... Big chance that you will just create new notebook with Learner-Copy1, Learner-Copy2, Learner-CopyN, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bccd4e-b34a-4e22-971a-d3b875e98030",
   "metadata": {},
   "source": [
    "Let's today design a new Flexible Training Loop where we keep it as simple as possible but also having a full power of plugging new ideas via callbacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0cc4c8",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "Callback here is a not a python feature but a design concept. It's a just mean triggering a function when you've done something\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185e652",
   "metadata": {},
   "source": [
    "## Flexible Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4ce70-645e-418b-8ff2-bc3c4beed5c4",
   "metadata": {},
   "source": [
    "There are 3 core pieces of a training loop\n",
    "\n",
    "- Fit\n",
    "- Epoch\n",
    "- Batch\n",
    "\n",
    "We will wrap around each event here a `before` and `after` method (so `before_batch` `after_batch` `before_epoch` `after_epoch` `before_fit` `after_fit`) with full access to the `Learner` (which includes everything: the model, optimizer, dataloader, ... ). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ea746-afc0-451f-a86b-03e4fc8204c6",
   "metadata": {},
   "source": [
    "```python\n",
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.preds = self.model(self.batch[0])\n",
    "        self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        try:\n",
    "            self.callback('before_epoch')\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                try:\n",
    "                    self.callback('before_batch')\n",
    "                    self.one_batch()\n",
    "                    self.callback('after_batch')\n",
    "                except CancelBatchException: pass\n",
    "            self.callback('after_epoch')\n",
    "        except CancelEpochException: pass\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        try:\n",
    "            self.callback('before_fit')\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            self.callback('after_fit')\n",
    "        except CancelFitException: pass\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43d555-7e63-46a0-9114-81b92e213784",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "The `exception` here is for adding more control - exit an event when we need\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08426d7e-1cdc-471c-a85a-912efe886b22",
   "metadata": {},
   "source": [
    "Pause for a second and imagine if you want to write a simple `DeviceCallBack`, what will you do ? ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df964f9f-24ef-47c8-b085-1014d67ba691",
   "metadata": {},
   "source": [
    "It's putting to the device (CUDA):\n",
    "\n",
    "- Before_fit: Model -> CUDA\n",
    "- Before_batch: (Input, Label) -> CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3ba27-6ee3-4e4d-9fe2-0001d2124ec9",
   "metadata": {},
   "source": [
    "To define this Learner: ``` learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[DeviceCB()]) ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd77a6-498c-45d2-8f16-9e19d082ff7c",
   "metadata": {},
   "source": [
    "## How to go even further?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b91db-1b06-4c35-942e-d960521374ef",
   "metadata": {},
   "source": [
    "We don't want to repeat ourselves so the `before` and `after` in the previous `Learner` version can be refactored. We can use @decorator or @contextmanager for wrapping something before and after the event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec749d50-caf7-4baa-83ff-6cda3bc04b1e",
   "metadata": {},
   "source": [
    "The fastai course experiment this idea further with even considering ```'predict','get_loss','backward','step','zero_grad'``` as events. It means there are really nothing that we can not control here. An example of when it is useful is experimenting a Momentum Learner by rewritting the `zero_grad`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e52f6-073d-456d-b419-b2b9cf9ecc6c",
   "metadata": {},
   "source": [
    "```python\n",
    "#|export\n",
    "class MomentumLearner(TrainLearner):\n",
    "    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):\n",
    "        self.mom = mom\n",
    "        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters(): p.grad *= self.mom\n",
    "     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d77d30-5e12-44b6-9531-0595cdaa76a1",
   "metadata": {},
   "source": [
    "It means instead of forgetting all the previous gradient by asigning it to zero, we can multiply it by a number < 1. So in the next update, we also take into account what we've trained before ( or momentum )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d332d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
