{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d422b3a7-8fed-4263-8ece-34dad0725ef2",
   "metadata": {},
   "source": [
    "```python\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc,count = 0.,0.,0\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(pred,yb).item()*n\n",
    "                tot_acc  += accuracy (pred,yb).item()*n\n",
    "        print(epoch, tot_loss/count, tot_acc/count)\n",
    "    return tot_loss/count, tot_acc/count\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872226d-6d7e-46a4-bad1-45905702b503",
   "metadata": {},
   "source": [
    "Above is a minimalist training loop. It does couple of things as below:\n",
    "- Training through a number of epochs\n",
    "- In each epoch, loop through every batchs\n",
    "- In each batch, depending on if the model is training or validating, having different behaviours\n",
    "\n",
    "However, in most of the time, we won't stop here after finishing the 1st experiment. What if ... :\n",
    "- The dataset is unbalance, we should better adding more metrics than just accuracy, maybe F1, ROCAUC\n",
    "- You want to log the result in a file / show in a realtime graph put it on WandB\n",
    "- Early Stopping, Save Best Model, and Much more ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f10e2-9133-47f8-b40d-0a9d3df603fe",
   "metadata": {},
   "source": [
    "If we will just adding more and more things in the training loop, it will eventually become a big mess with thoudsand of lines, unmaintainable. And if you want to disable some features, then enable it again, there might be a big chance that you will just create new notebook with Learner-Copy1, Learner-Copy2, Learner-CopyN, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bccd4e-b34a-4e22-971a-d3b875e98030",
   "metadata": {},
   "source": [
    "Let's today design a new Flexible Training Loop where we keep it as simple as possible but also having a full power to adding things via callbacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978061fc-d73b-4b3d-ac33-c839365f3f93",
   "metadata": {},
   "source": [
    "Callback here is a not a python feature but a design concept. It's a just mean triggering a function when you've done something"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4ce70-645e-418b-8ff2-bc3c4beed5c4",
   "metadata": {},
   "source": [
    "So what is the core pieces of a training loop\n",
    "- Fit\n",
    "- Epoch\n",
    "- Batch\n",
    "\n",
    "We will wrap through each event here a `before` and `after` method with full access to the Learner (which includes everything: the model, optimizer, dataloader, ... ). `before_batch` `after_batch` `before_epoch` `after_epoch` `before_fit` `after_fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ea746-afc0-451f-a86b-03e4fc8204c6",
   "metadata": {},
   "source": [
    "```python\n",
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.preds = self.model(self.batch[0])\n",
    "        self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        try:\n",
    "            self.callback('before_epoch')\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                try:\n",
    "                    self.callback('before_batch')\n",
    "                    self.one_batch()\n",
    "                    self.callback('after_batch')\n",
    "                except CancelBatchException: pass\n",
    "            self.callback('after_epoch')\n",
    "        except CancelEpochException: pass\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        try:\n",
    "            self.callback('before_fit')\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            self.callback('after_fit')\n",
    "        except CancelFitException: pass\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43d555-7e63-46a0-9114-81b92e213784",
   "metadata": {},
   "source": [
    "The `exception` here is for adding more control / exitting an event when we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08426d7e-1cdc-471c-a85a-912efe886b22",
   "metadata": {},
   "source": [
    "Pause for a second and imagine if you want to write a DeviceCallBack, how do you do? ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df964f9f-24ef-47c8-b085-1014d67ba691",
   "metadata": {},
   "source": [
    "It's putting to the device:\n",
    "- The model before fitting\n",
    "- The input before each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3ba27-6ee3-4e4d-9fe2-0001d2124ec9",
   "metadata": {},
   "source": [
    "To define a this Learner: ``` learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[DeviceCB(), metrics]) ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd77a6-498c-45d2-8f16-9e19d082ff7c",
   "metadata": {},
   "source": [
    "How to go even further?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b91db-1b06-4c35-942e-d960521374ef",
   "metadata": {},
   "source": [
    "We don't want to repeat ourselves right? The `before` and `after` is repeating too much here. To solve this, we can use @decorator or @contextmanager for wrapping something before and after the event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec749d50-caf7-4baa-83ff-6cda3bc04b1e",
   "metadata": {},
   "source": [
    "The fastai course experiment this idea further with even considering ```'predict','get_loss','backward','step','zero_grad'``` as events. It means there are really nothing that we can not control here. An example of when it is useful is experimenting a Momentum Learner by rewritting the `zere_grad`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e52f6-073d-456d-b419-b2b9cf9ecc6c",
   "metadata": {},
   "source": [
    "```python\n",
    "#|export\n",
    "class MomentumLearner(TrainLearner):\n",
    "    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):\n",
    "        self.mom = mom\n",
    "        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters(): p.grad *= self.mom\n",
    "     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d77d30-5e12-44b6-9531-0595cdaa76a1",
   "metadata": {},
   "source": [
    "It means instead of forgetting all the previous gradient by asigning it to zero, we can multiply it by a number < 1. So in the next update, we also take into account what we've trained before ( or momentum ). I don't know but maybe we shoud rename the method from `zero_grad` to something less confusing here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:blog] *",
   "language": "python",
   "name": "conda-env-blog-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
