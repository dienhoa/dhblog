[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Some achievements: - fast.ai regular member - winner of community Kaggle competition"
  },
  {
    "objectID": "posts/SSD_base.html",
    "href": "posts/SSD_base.html",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "",
    "text": "Building an Object Detection from scratch with fastai v2\nRecently, I had a project that needs to modify an Object Detection Architecture. However, when I searched for related repositories, I found it quite difficult to understand. We have a lot of libraries for use out of the box but hard to make changes to the source code.\nThis blog is the implementation of Single Shot Detector Architecture using fast.ai in literate programming style so the readers can follow and run each line of code themselves in case needed to deepen their knowledge.\nThe original idea was taken from the fastai 2018 course. Readers are recommended to watch this lecture. 2018 Lecture\nSome useful notes taken by students: - Cedrick Note - Francesco Note\nDataset used: Pascal 2017\nWhat we can learn from this notebook:"
  },
  {
    "objectID": "posts/SSD_base.html#object-detection-dataloaders",
    "href": "posts/SSD_base.html#object-detection-dataloaders",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Object Detection Dataloaders",
    "text": "Object Detection Dataloaders\nFor objection detection, you have:\n\n1 independent variable (X): Image\n2 dependents variables (Ys): Bounding box and Class\n\nIn this part, we will use fastai DataBlock to build Object Detection Dataloaders. The idea is from each image file name, we will have:\n\nAn Image\nBounding Boxes getting from the annotations file\nLabels correspond to each bounding box\n\n\n\n\n\n\n\nNote\n\n\n\n\nZero padding: Each image have a different number of objects. Then, to make it possible to gather multiple images to one batch, the number of bounding boxes per image is the maximum in that batch (the padding value by default is 0) bb_pad\nBackground class: In Object Detection, we need to have a class that represents the background. fastai do it automatically for you by adding #na# at index 0\nThe coordinates of bounding box is rescaled to ~ -1 -> 1 in fastai/vision/core.py _scale_pnts\n\n\n\n( Check out some outputs below for details )\n\n\n\nList of Files to Data\n\n\n\npath = untar_data(URLs.PASCAL_2007)\n\n\npath.ls()\n\n(#8) [Path('/home/ubuntu/.fastai/data/pascal_2007/train.json'),Path('/home/ubuntu/.fastai/data/pascal_2007/test.json'),Path('/home/ubuntu/.fastai/data/pascal_2007/test'),Path('/home/ubuntu/.fastai/data/pascal_2007/train.csv'),Path('/home/ubuntu/.fastai/data/pascal_2007/segmentation'),Path('/home/ubuntu/.fastai/data/pascal_2007/valid.json'),Path('/home/ubuntu/.fastai/data/pascal_2007/train'),Path('/home/ubuntu/.fastai/data/pascal_2007/test.csv')]\n\n\n\nimgs, lbl_bbox = get_annotations(path/'train.json') \n\n\nimgs[0], lbl_bbox[0]\n\n('000012.jpg', ([[155, 96, 351, 270]], ['car']))\n\n\n\nimg2bbox = dict(zip(imgs, lbl_bbox))\n\n\nfirst = {k: img2bbox[k] for k in list(img2bbox)[:1]}; first\n\n{'000012.jpg': ([[155, 96, 351, 270]], ['car'])}\n\n\n\ngetters = [lambda o: path/'train'/o, lambda o: img2bbox[o][0], lambda o: img2bbox[o][1]]\n\n\nitem_tfms = [Resize(224, method='squish'),]\nbatch_tfms = [Rotate(), Flip(), Dihedral()]\n\n\npascal = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),\n                 splitter=RandomSplitter(),\n                 getters=getters,\n                 item_tfms=item_tfms,\n                 batch_tfms=batch_tfms,\n                 n_inp=1)\n\n\ndls = pascal.dataloaders(imgs, bs = 128)\n\n\n\n\n\n\n\nNote\n\n\n\n#na# is the background class as defined in BBoxLblBlock\n\n\n\ndls.vocab\n\n['#na#', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n\n\n\nlen(dls.vocab)\n\n21\n\n\n\ndls.show_batch()\n\n\n\n\n\none_batch = dls.one_batch()\n\n\n\n\n\n\n\nNote\n\n\n\nThe coordinates of boudning box is rescaled to ~ -1 -> 1 in fastai/vision/core.py\n\n\n\none_batch[1][0][0]\n\nTensorBBox([-0.0440, -0.2171,  0.2200,  0.5046], device='cuda:0')\n\n\n\n# Zero Padding\none_batch[2]\n\nTensorMultiCategory([[13, 15,  0,  ...,  0,  0,  0],\n                     [12, 15, 15,  ...,  0,  0,  0],\n                     [18,  5,  5,  ...,  0,  0,  0],\n                     ...,\n                     [15,  8,  8,  ...,  0,  0,  0],\n                     [ 7,  0,  0,  ...,  0,  0,  0],\n                     [ 8,  0,  0,  ...,  0,  0,  0]], device='cuda:0')"
  },
  {
    "objectID": "posts/SSD_base.html#model-architecture",
    "href": "posts/SSD_base.html#model-architecture",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Model Architecture",
    "text": "Model Architecture\n\n\n\nSSD Architecture\n\n\nIn a nutshell, Object Detection Model is a model that does 2 jobs at the same time:\n\na regressor with 4 outputs for bounding box\na classifier with c classes.\n\nTo handle multiple objects, here comes the grid cell. For each cell, you will have an atomic prediction for the object that dominates a part of the image ( This is the idea of the receptive field that you will see in the next part )\n\n\n\n\n\n\nMy Intuition\n\n\n\nIn Machine Learning, it is better to improve from something rather than start from scratch. You can see this in: Image Classification Architecture - Resnet with the Skip Connections, or Gradient Boosting in Tree-based Model. There is a common point in the grid-cell SSD architecture, the model will try to improve from an anchor box rather than searching through the whole image.\n\n\nWe should better leverage a well-known pretrained classification model to be used as a backbone / or body ( resnet in this tutorial ) if the object is similar to the Imagenet dataset. The head part will follow to adapt to the necessary dimension\nTo easily develop the idea - visualize and debug, we will start with a simple 4x4 grid\n\ndef flatten_conv(x,k):\n    # Flatten the 4x4 grid to dim16 vectors\n    bs,nf,gx,gy = x.size()\n    x = x.permute(0,2,3,1).contiguous()\n    return x.view(bs,-1,nf//k)\n\n\nclass OutConv(nn.Module):\n    # Output Layers for SSD-Head. Contains oconv1 for Classification and oconv2 for Detection\n    def __init__(self, k, nin, bias):\n        super().__init__()\n        self.k = k\n        self.oconv1 = nn.Conv2d(nin, (len(dls.vocab))*k, 3, padding=1)\n        self.oconv2 = nn.Conv2d(nin, 4*k, 3, padding=1)\n        self.oconv1.bias.data.zero_().add_(bias)\n        \n    def forward(self, x):\n        return [flatten_conv(self.oconv1(x), self.k),\n                flatten_conv(self.oconv2(x), self.k)]\n\n\nclass StdConv(nn.Module):\n    # Standard Convolutional layers \n    def __init__(self, nin, nout, stride=2, drop=0.1):\n        super().__init__()\n        self.conv = nn.Conv2d(nin, nout, 3, stride=stride, padding=1)\n        self.bn = nn.BatchNorm2d(nout)\n        self.drop = nn.Dropout(drop)\n        \n    def forward(self, x): return self.drop(self.bn(F.relu(self.conv(x))))\n\n\nclass SSD_Head(nn.Module):\n    def __init__(self, k, bias):\n        super().__init__()\n        self.drop = nn.Dropout(0.25)\n        self.sconv0 = StdConv(512,256, stride=1)\n        self.sconv2 = StdConv(256,256)\n        self.out = OutConv(k, 256, bias)\n        \n    def forward(self, x):\n        x = self.drop(F.relu(x))\n        x = self.sconv0(x)\n        x = self.sconv2(x)\n        return self.out(x)\n\nWe start with k = 1 which is the number of alterations for each anchor box ( we have a lot of anchor boxes later )\n\nk=1\n\n\nhead_reg4 = SSD_Head(k, -3.)\n\n\nbody = create_body(resnet34(True))\nmodel = nn.Sequential(body, head_reg4)\n\n/home/ubuntu/miniconda3/envs/blog/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n  warnings.warn(\n/home/ubuntu/miniconda3/envs/blog/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nTo understand and verify that everything works ok, you can take out a batch and run the model on it\n\nout0 = body(one_batch[0].cpu())\n\n\nout1 = head_reg4(out0)\n\n\nout1[0].shape, out1[1].shape\n\n(torch.Size([128, 16, 21]), torch.Size([128, 16, 4]))\n\n\nShape explanation:\n\n128: batch size\n16: number of anchor boxes\n21: number of classes\n4: number of bounding box coordinates"
  },
  {
    "objectID": "posts/SSD_base.html#x4-anchor-boxes-and-receptive-field",
    "href": "posts/SSD_base.html#x4-anchor-boxes-and-receptive-field",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "4x4 Anchor boxes and Receptive Field",
    "text": "4x4 Anchor boxes and Receptive Field\nAs mentioned before, we will start with a 4x4 grid to better visualize the idea. The size will be normalized to [0,1]\nThe idea of why, after the Body, we use Conv2d and not Linear Layer to make a 4x4x(4+c) output dimension instead of 16x(4+c) shape is - Receptive Field. This way, each cell will have information that comes directly from the location corresponding to the anchor box. The illustration is below.\n\n\n\nSSD vs YOLO\n\n\n\n\n\nReceptive Field\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe very careful about the bounding box format when working with Object Detection. There are many different formats out there. For example:\n\npascal_voc: [x_min, y_min, x_max, y_max]\ncoco: [x_min, y_min, width, height]\nYOLO: [x_center, y_center, width, height]\n\nThe bounding box format in this tutorial is [x_min, y_min, x_max, y_max]\n\n\nCheck out Bounding Boxes Augmentation for more details:\nWe define the anchors coordinates as below\n\nanc_grid = 4 # Start with only 4x4 grid and no variation for each cell\nk = 1 # Variation of each anchor box\nanc_offset = 1/(anc_grid*2)\nanc_x = np.repeat(np.linspace(anc_offset, 1-anc_offset, anc_grid), anc_grid) # Center of anc in x\nanc_y = np.tile(np.linspace(anc_offset, 1-anc_offset, anc_grid), anc_grid) # Center f anc in y\n\n\nanc_x\n\narray([0.125, 0.125, 0.125, 0.125, 0.375, 0.375, 0.375, 0.375, 0.625,\n       0.625, 0.625, 0.625, 0.875, 0.875, 0.875, 0.875])\n\n\n\nanc_y\n\narray([0.125, 0.375, 0.625, 0.875, 0.125, 0.375, 0.625, 0.875, 0.125,\n       0.375, 0.625, 0.875, 0.125, 0.375, 0.625, 0.875])\n\n\n\nanc_ctrs = np.tile(np.stack([anc_x,anc_y], axis=1), (k,1)) # Anchor centers\nanc_sizes = np.array([[1/anc_grid,1/anc_grid] for i in range(anc_grid*anc_grid)])\n\n\nanc_ctrs\n\narray([[0.125, 0.125],\n       [0.125, 0.375],\n       [0.125, 0.625],\n       [0.125, 0.875],\n       [0.375, 0.125],\n       [0.375, 0.375],\n       [0.375, 0.625],\n       [0.375, 0.875],\n       [0.625, 0.125],\n       [0.625, 0.375],\n       [0.625, 0.625],\n       [0.625, 0.875],\n       [0.875, 0.125],\n       [0.875, 0.375],\n       [0.875, 0.625],\n       [0.875, 0.875]])\n\n\n\nanc_sizes\n\narray([[0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25],\n       [0.25, 0.25]])\n\n\n\nanchors = torch.tensor(np.concatenate([anc_ctrs, anc_sizes], axis=1), requires_grad=False).cuda()\n# Coordinates with format: center_x, center_y, W, H\n\n\nanchors\n\ntensor([[0.1250, 0.1250, 0.2500, 0.2500],\n        [0.1250, 0.3750, 0.2500, 0.2500],\n        [0.1250, 0.6250, 0.2500, 0.2500],\n        [0.1250, 0.8750, 0.2500, 0.2500],\n        [0.3750, 0.1250, 0.2500, 0.2500],\n        [0.3750, 0.3750, 0.2500, 0.2500],\n        [0.3750, 0.6250, 0.2500, 0.2500],\n        [0.3750, 0.8750, 0.2500, 0.2500],\n        [0.6250, 0.1250, 0.2500, 0.2500],\n        [0.6250, 0.3750, 0.2500, 0.2500],\n        [0.6250, 0.6250, 0.2500, 0.2500],\n        [0.6250, 0.8750, 0.2500, 0.2500],\n        [0.8750, 0.1250, 0.2500, 0.2500],\n        [0.8750, 0.3750, 0.2500, 0.2500],\n        [0.8750, 0.6250, 0.2500, 0.2500],\n        [0.8750, 0.8750, 0.2500, 0.2500]], device='cuda:0',\n       dtype=torch.float64)\n\n\n\ngrid_sizes = torch.tensor(np.array([1/anc_grid]), requires_grad=False).unsqueeze(1).cuda()\n\n\ngrid_sizes\n\ntensor([[0.2500]], device='cuda:0', dtype=torch.float64)"
  },
  {
    "objectID": "posts/SSD_base.html#visualization-utils",
    "href": "posts/SSD_base.html#visualization-utils",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Visualization Utils",
    "text": "Visualization Utils\nIt is very helpful (to understand/ debug) when you can visualize data of every step. Many subtle tiny details happen in this Object Detection Problem. One careless implementation can lead to hours (or even days) to debug. Sometimes, you just wish that the code throws you some bugs that you can trackback.\n\n\n\n\n\n\nWarning\n\n\n\nThere are some details that you need to double check\n\nAre your ground truth bounding boxes, anchor boxes, bounding box activations are in the same scale ( -1 -> 1 or 0 -> 1 ) ?\nDo the background class is handled correctly? ( This is a bug when I develop this notebook that the old version of the fastai course set the index of background as number_of_classes but in the latest version, it is 0 )\nDo you map correctly each Anchor Box to the ground-true object? (This will be shown in the next session)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDon’t hesitate to take out one batch from your dataloader and verify every single detail. When I start to use fast.ai, I made a big mistake that thinking these data are already processed and we can not show things directly from there. This data is very important, it is the input of your model. It must be carefully double-checked.\n\n\nBelow we will try to plot some images from a batch with their bounding boxes and classes, to see that we did not missing anything\n\nimport matplotlib.colors as mcolors\nimport matplotlib.cm as cmx\nfrom matplotlib import patches, patheffects\n\n\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.set_xticks(np.linspace(0, 224, 8))\n    ax.set_yticks(np.linspace(0, 224, 8))\n    ax.grid()\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n    return ax\n\n\ndef draw_outline(o, lw):\n    o.set_path_effects([patheffects.Stroke(\n        linewidth=lw, foreground='black'), patheffects.Normal()])\n\n\ndef draw_text(ax, xy, txt, sz=14, color='white'):\n    text = ax.text(*xy, txt,\n        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n    draw_outline(text, 1)\n\n\ndef draw_rect(ax, b, color='white'):\n    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n    draw_outline(patch, 4)\n\n\ndef bb_hw(a): return np.array([a[1],a[0],a[3]-a[1]+1,a[2]-a[0]+1])\n\n\ndef get_cmap(N):\n    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba\n\n\nnum_colr = 12\ncmap = get_cmap(num_colr)\ncolr_list = [cmap(float(x)) for x in range(num_colr)]\n\n\ndef show_ground_truth(ax, im, bbox, clas=None, prs=None, thresh=0.3):\n    bb = [bb_hw(o) for o in bbox.reshape(-1,4)]\n    if prs is None:  prs  = [None]*len(bb)\n    if clas is None: clas = [None]*len(bb)\n    ax = show_img(im, ax=ax)\n    k=0\n    for i,(b,c,pr) in enumerate(zip(bb, clas, prs)):\n        if((b[2]>1) and (pr is None or pr > thresh)):\n            k+=1\n            draw_rect(ax, b, color=colr_list[i%num_colr])\n            txt = f'{k}: '\n            if c is not None: txt += ('bg' if c==0 else dls.vocab[c])\n            if pr is not None: txt += f' {pr:.2f}'\n            draw_text(ax, b[:2], txt, color=colr_list[i%num_colr])\n\n\ndef torch_gt(ax, ima, bbox, clas, prs=None, thresh=0.4):\n    return show_ground_truth(ax, ima, to_np((bbox*224).long()),\n         to_np(clas), to_np(prs) if prs is not None else None, thresh)\n\n\nShowing one batch\n\nidx = 5\n\n\nimg = one_batch[0][idx].permute(2,1,0).cpu()\n\n\nplt.imshow(img)\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\nExtracting one batch for your dataloader and see if the data is OK\n\nx = one_batch[0].permute(0,3,2,1).cpu()\n\n\ny = one_batch[1:]\n\nBecause the bounding box in the dataloader is scaled to -1 -> 1, it needs to be rescaled to 0 -> 1 for drawing by doing (bb+1)/2*Size\n\n## Bounding Box after dataloader should Rescale\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\nfor i,ax in enumerate(axes.flat):\n    show_ground_truth(ax, x[i].cpu(), ((y[0][i]+1)/2*224).cpu(), y[1][i].cpu())\nplt.tight_layout()\n\n\n\n\nEverything looks fine! We have correct bounding boxes and their corresponding classes"
  },
  {
    "objectID": "posts/SSD_base.html#map-to-ground-truth-and-loss-function",
    "href": "posts/SSD_base.html#map-to-ground-truth-and-loss-function",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Map to Ground-Truth and Loss function",
    "text": "Map to Ground-Truth and Loss function\nAs you might guess, There are 2 components forming the Object Detection Loss: Classification Loss (For the class) and Localization Loss (For the bounding box)\nThe idea is, for each image, we will: - Calculate the Intersection-over-Union (IoU) of each predefined Anchor Box with the Object Bounding Box. - Assign the label for each cell (Map to ground truth) according to the IoUs. Background will be assigned to Cell which overlaps with no object - Calculate the Classification Loss for all Cells - Calculate the Bounding Box Location Loss only for Cells responsible to Objects (no Background) - Take the sum of these 2 losses\n\n\n\n\n\n\nNote\n\n\n\nCurrently, we will loop for each image in a batch to calculate its loss and then sum them all. I think we might have a better way to vectorize these operations, or, calculate everything in one shot directly with a batch tensor\n\n\n\n\n\nMap to Grouth Truth\n\n\n\ndef get_y(bbox,clas):\n    \"\"\"\n    Remove the zero batching from a batch\n    \n    Because the number of object in each image are different so\n    we need to zero padding for batching \n    \"\"\"\n    bbox = bbox.view(-1,4)\n    clas = clas.view(-1,1)\n    bb_keep = ((bbox[:,2]-bbox[:,0])>0).nonzero()[:,0]\n    return TensorBase(bbox)[bb_keep],TensorBase(clas)[bb_keep]\n\n\none_batch[2][idx]\n\nTensorMultiCategory([16, 16, 16, 16, 14,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n                      0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n                      0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n\n\n\nget_y(one_batch[1][idx], one_batch[2][idx])\n\n(TensorBBox([[ 0.0966, -1.0172,  0.4870, -0.4764],\n             [-0.3311, -1.0029,  0.0835, -0.4559],\n             [-0.3511, -1.0028,  0.0783, -0.4872],\n             [ 0.1286, -1.0201,  0.5700, -0.5041],\n             [ 0.4902,  0.1488,  1.0261,  0.9663],\n             [-0.8546, -0.6447, -0.2425, -0.2718]], device='cuda:0'),\n TensorBBox([[16],\n             [16],\n             [16],\n             [16],\n             [14],\n             [ 7]], device='cuda:0'))\n\n\nWe can see that all the zero values are removed before continuing to process\n\ndef hw2corners(ctr, hw): \n    # Function to convert BB format: (centers and dims) -> corners\n    return torch.cat([ctr-hw/2, ctr+hw/2], dim=1)\n\nThe Activations are passed to a Tanh function to rescale their values to -1 -> 1. Then they are processed to make coherent with the Grid Coordinates:\n\nThe center of each cell’s prediction stays in the cell\nThe size of each cell’s prediction can be varied from 1/2 to 3/2 cell’s size to give more flexibility\n\n\n\n\n\n\n\nTip\n\n\n\nThe bounding box activations are in [x_center, y_center, width, height] format to easily define the min/max scale to the anchor box\n\n\n\ndef actn_to_bb(actn, anchors):\n    actn_bbs = torch.tanh(actn)\n    actn_centers = (actn_bbs[:,:2]/2 * grid_sizes) + anchors[:,:2]\n    actn_hw = (actn_bbs[:,2:]/2+1) * anchors[:,2:]\n    return hw2corners(actn_centers, actn_hw)\n\n\ndef one_hot_embedding(labels, num_classes):\n    return torch.eye(num_classes)[labels].cuda()\n\n\ndef intersect(box_a, box_b):\n    \"\"\"\n    Intersect area between to bounding boxes\n    \"\"\"\n    max_xy = torch.min(box_a[:, None, 2:], box_b[None, :, 2:])\n    min_xy = torch.max(box_a[:, None, :2], box_b[None, :, :2])\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef box_sz(b): return ((b[:, 2]-b[:, 0]) * (b[:, 3]-b[:, 1]))\n\n\ndef jaccard(box_a, box_b):\n    \"\"\"\n    Jaccard or Intersection over Union\n    \"\"\"\n    inter = intersect(box_a, box_b)\n    union = box_sz(box_a).unsqueeze(1) + box_sz(box_b).unsqueeze(0) - inter\n    return inter / union\n\nMap to Ground Truth (Visualization below). The idea is looping through all anchor boxes and calculating the overlaps with the Ground Truth bounding boxes, then assigning each Anchor Box to the corresponding class\n\ndef map_to_ground_truth(overlaps):\n    prior_overlap, prior_idx = overlaps.max(1) # 3\n    gt_overlap, gt_idx = overlaps.max(0) # 16\n    gt_overlap[prior_idx] = 1.99\n    for i,o in enumerate(prior_idx): gt_idx[o] = i\n    return gt_overlap,gt_idx\n\nFor calculating loss, we will loop through every images in a batch and calculate loss for each image (ssd_1_loss), then summing the result with ssd_loss. The Classification Loss (loss_f) currently is left empty as we will discussion it later in the next section.\n\ndef ssd_1_loss(b_c,b_bb,bbox,clas):\n    bbox,clas = get_y(bbox,clas)\n    bbox = (bbox+1)/2\n    a_ic = actn_to_bb(b_bb, anchors)\n    overlaps = jaccard(bbox.data, anchor_cnr.data)\n    gt_overlap,gt_idx = map_to_ground_truth(overlaps)\n    gt_clas = clas[gt_idx]\n    pos = gt_overlap > 0.4\n    pos_idx = torch.nonzero(pos)[:,0]\n    gt_clas[~pos] = 0  # Assign the background to idx 0\n    gt_bbox = bbox[gt_idx]\n    loc_loss = ((TensorBase(a_ic[TensorBase(pos_idx)]) - TensorBase(gt_bbox[TensorBase(pos_idx)])).abs()).mean()\n    clas_loss  = loss_f(b_c, gt_clas)\n    return loc_loss, clas_loss\n\n\nanchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:]).cuda()\n\n\nShowing Map To Ground Truth\nAs mentioned earlier, Map-to-Ground-Truth is a very important step for calculating loss. We should show it to make sure everything looks fine\n\nidx = 0\nbbox = one_batch[1][idx].cuda()\nclas = one_batch[2][idx].cuda()\n\n\nbbox,clas = get_y(bbox,clas)\nbbox = (bbox+1)/2\n# a_ic = actn_to_bb(b_bb, anchors)\noverlaps = jaccard(bbox.data, anchor_cnr.data)\ngt_overlap,gt_idx = map_to_ground_truth(overlaps)\ngt_clas = clas[gt_idx]\npos = gt_overlap > 0.4\npos_idx = torch.nonzero(pos)[:,0]\ngt_clas[~pos] = 0  # Assign the background to idx 0\ngt_bbox = bbox[gt_idx]\n\n\nima = one_batch[0][idx].permute(2,1,0).cpu()\n\n\nfig, ax = plt.subplots(figsize=(7,7))\ntorch_gt(ax, ima, bbox, clas)\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(7,7))\ntorch_gt(ax, ima, anchor_cnr, gt_clas)\n\n\n\n\n\nsz = 224\n\n\n\nClassificaton Loss: Binary Cross Entropy and why Focal Loss\n2 tricks can be used for Classification Loss:\n\nBinary Cross-Entropy Loss without background\nFurther improve Binary Cross-Entropy Loss with Focal Loss\n\n\nBinary Cross-Entropy\n\n\nIf we treat the Background Class as one class and ask the Model to understand what is a Background, it might be too difficult. We can translate it to a set of easier questions: Is it a Cat? Is it a Dog? … through all the classes, which is exactly what Binary Cross-Entropy does\n\n\nFocal Loss\n\n\nThe classification task in object detection is very imbalance that we have a lot of background objects (check the Match to Ground-Truth image above). If we just use Binary Cross-Entropy Loss function, it will try all efforts to improve background classification\n\n\n\n\nFocal Loss vs Binary Cross Entropy Loss\n\n\nQuote from fastai2018 course:\nThe blue line is the binary cross entropy loss. If the answer is not a motorbike, and I said “I think it’s not a motorbike and I am 60% sure” with the blue line, the loss is still about 0.5 which is pretty bad. So if we want to get our loss down, then for all these things which are actually back ground, we have to be saying “I am sure that is background”, “I am sure it’s not a motorbike, or a bus, or a person” — because if I don’t say we are sure it is not any of these things, then we still get loss.\nThat is why the motorbike example did not work. Because even when it gets to lower right corner and it wants to say “I think it’s a motorbike”, there is no payoff for it to say so. If it is wrong, it gets killed. And the vast majority of the time, it is background. Even if it is not background, it is not enough just to say “it’s not background” — you have to say which of the 20 things it is.\nSo the trick is to trying to find a different loss function that looks more like the purple line. Focal loss is literally just a scaled cross entropy loss. Now if we say “I’m .6 sure it’s not a motorbike” then the loss function will say “good for you! no worries”.\n\nclass BCE_Loss(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.num_classes = num_classes\n\n    def forward(self, pred, targ):\n        t = one_hot_embedding(targ.squeeze(), self.num_classes)\n        t = t[:,1:] # Start from 1 to exclude the Background\n        x = pred[:,1:]\n        w = self.get_weight(x,t)\n        return F.binary_cross_entropy_with_logits(x, t, w.detach(), reduction='sum')/self.num_classes\n    \n    def get_weight(self,x,t): return None\n\n\nclass FocalLoss(BCE_Loss):\n    def get_weight(self,x,t):\n        alpha,gamma = 0.25,1\n        p = x.sigmoid()\n        pt = p*t + (1-p)*(1-t)\n        w = alpha*t + (1-alpha)*(1-t)\n        return w * (1-pt).pow(gamma)\n\nThe ssd_loss will loop through every image in a batch and accumulate loss\n\ndef ssd_loss(pred, bbox, clas):\n    lcs, lls = 0., 0.\n    W = 30\n    for b_c, b_bb, bbox, clas in zip(*pred, bbox, clas):\n        loc_loss, clas_loss = ssd_1_loss(b_c, b_bb, bbox, clas)\n        lls += loc_loss\n        lcs += clas_loss\n    return lls + lcs\n\n\nloss_f = FocalLoss(len(dls.vocab))"
  },
  {
    "objectID": "posts/SSD_base.html#training-simple-model",
    "href": "posts/SSD_base.html#training-simple-model",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Training Simple Model",
    "text": "Training Simple Model\n\nmodel = nn.Sequential(body, head_reg4)\n\n\nlearner = Learner(dls, model, loss_func=ssd_loss)\n\n\nlearner.fit_one_cycle(5, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      34.889286\n      28.454723\n      00:25\n    \n    \n      1\n      32.127403\n      29.533695\n      00:23\n    \n    \n      2\n      30.588394\n      26.637667\n      00:23\n    \n    \n      3\n      29.455709\n      25.630453\n      00:23\n    \n    \n      4\n      28.651590\n      25.509596\n      00:23\n    \n  \n\n\n\nThe loss decreases, and the model can learn something. Looking at the results shown below, we can see that the predictions are not so bad but not particularly good either. In the next session, we can see how to improve the results with more anchor boxes\n\nShow Results\n\none_batch = dls.valid.one_batch()\nlearner.model.eval();\npred = learner.model(one_batch[0])\nb_clas, b_bb = pred\nx = one_batch[0]\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\nfor idx,ax in enumerate(axes.flat):\n    ima = x.permute(0,3,2,1).cpu()[idx]\n#     ima=md.val_ds.ds.denorm(x)[idx]\n    bbox,clas = get_y(y[0][idx], y[1][idx])\n    a_ic = actn_to_bb(b_bb[idx], anchors)\n    torch_gt(ax, ima, a_ic, b_clas[idx].max(1)[1], b_clas[idx].max(1)[0].sigmoid(), 0.21)\n#plt.tight_layout()\nplt.subplots_adjust(wspace=0.15, hspace=0.15)"
  },
  {
    "objectID": "posts/SSD_base.html#more-anchors",
    "href": "posts/SSD_base.html#more-anchors",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "More anchors",
    "text": "More anchors\nAs said earlier, the anchor box is a hint for the model to not go too far and focus on a part of the image. So obviously, 4x4 grid is not enough to predict an object of any size. In this part, by adding more Conv2d layers, we will have 3 grids: 4x4, 2x2, 1x1 and each cell will have 9 variations: 3-zooms and 3-ratios\nThe total number of anchors is: (16 + 4 + 1) x 9 = 189 anchors\n\n\n\nimage8.png\n\n\n\n# This is for release the GPU memrory while experimenting. I guess it is not enough. Please tell me if you know a better way\ndel learner\ndel model\nimport gc; gc.collect()\ntorch.cuda.empty_cache()\n\n\nanc_grids = [4,2,1]\nanc_zooms = [0.7, 1., 1.3]\nanc_ratios = [(1.,1.), (1.,0.5), (0.5,1.)]\nanchor_scales = [(anz*i,anz*j) for anz in anc_zooms for (i,j) in anc_ratios]\nk = len(anchor_scales)\nanc_offsets = [1/(o*2) for o in anc_grids]\nk\n\n9\n\n\n\nanc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)\n                        for ao,ag in zip(anc_offsets,anc_grids)])\nanc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)\n                        for ao,ag in zip(anc_offsets,anc_grids)])\nanc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)\n\n\nanc_x\n\narray([0.125, 0.125, 0.125, 0.125, 0.375, 0.375, 0.375, 0.375, 0.625,\n       0.625, 0.625, 0.625, 0.875, 0.875, 0.875, 0.875, 0.25 , 0.25 ,\n       0.75 , 0.75 , 0.5  ])\n\n\n\nanc_sizes  =   np.concatenate([np.array([[o/ag,p/ag] for i in range(ag*ag) for o,p in anchor_scales])\n               for ag in anc_grids])\ngrid_sizes = torch.tensor(np.concatenate([np.array([ 1/ag       for i in range(ag*ag) for o,p in anchor_scales])\n               for ag in anc_grids]), requires_grad=False).unsqueeze(1).cuda()\nanchors = torch.tensor(np.concatenate([anc_ctrs, anc_sizes], axis=1), requires_grad=False).float().cuda()\nanchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:]).cuda()\n\n\nanchor_cnr.shape\n\ntorch.Size([189, 4])\n\n\nWe need to adjust the SSD head a little bit. We will add more Conv2D layer with StdConv (to create 2x2 and 1x1 grids). After each StdConv is an OutConv to handle the Classification prediction and Localization prediction\n\nclass SSD_MultiHead(nn.Module):\n    def __init__(self, k, bias):\n        super().__init__()\n        self.drop = nn.Dropout(drop)\n        self.sconv0 = StdConv(512,256, stride=1, drop=drop)\n        self.sconv1 = StdConv(256,256, drop=drop)\n        self.sconv2 = StdConv(256,256, drop=drop)\n        self.sconv3 = StdConv(256,256, drop=drop)\n        self.out0 = OutConv(k, 256, bias)\n        self.out1 = OutConv(k, 256, bias)\n        self.out2 = OutConv(k, 256, bias)\n        self.out3 = OutConv(k, 256, bias)\n\n    def forward(self, x):\n        x = self.drop(F.relu(x))\n        x = self.sconv0(x)\n        x = self.sconv1(x)\n        o1c,o1l = self.out1(x)\n        x = self.sconv2(x)\n        o2c,o2l = self.out2(x)\n        x = self.sconv3(x)\n        o3c,o3l = self.out3(x)\n        return [torch.cat([o1c,o2c,o3c], dim=1),\n                torch.cat([o1l,o2l,o3l], dim=1)]\n\n\ndrop=0.4\n\n\nhead_reg4 = SSD_MultiHead(k, -4.)\n\n\nbody = create_body(resnet34(True))\nmodel = nn.Sequential(body, head_reg4)\n\n\nlearner = Learner(dls, model, loss_func=ssd_loss)\n\n\n# learner.lr_find()\n\n\nlearner.fit_one_cycle(20, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      79.482658\n      65.257332\n      00:24\n    \n    \n      1\n      77.919846\n      64.182114\n      00:24\n    \n    \n      2\n      75.337402\n      69.358673\n      00:24\n    \n    \n      3\n      70.927734\n      73.576935\n      00:24\n    \n    \n      4\n      65.866829\n      58.502281\n      00:24\n    \n    \n      5\n      61.796001\n      51.171406\n      00:24\n    \n    \n      6\n      58.571583\n      47.785007\n      00:24\n    \n    \n      7\n      55.809723\n      45.772766\n      00:24\n    \n    \n      8\n      53.606243\n      45.726265\n      00:25\n    \n    \n      9\n      51.751816\n      45.473743\n      00:24\n    \n    \n      10\n      49.946224\n      43.707134\n      00:24\n    \n    \n      11\n      48.457012\n      42.950340\n      00:25\n    \n    \n      12\n      46.938705\n      40.909351\n      00:24\n    \n    \n      13\n      45.661766\n      40.690815\n      00:24\n    \n    \n      14\n      44.419174\n      40.372437\n      00:25\n    \n    \n      15\n      43.232628\n      39.393692\n      00:24\n    \n    \n      16\n      42.119759\n      38.884872\n      00:24\n    \n    \n      17\n      41.290310\n      38.704178\n      00:24\n    \n    \n      18\n      40.546024\n      38.666664\n      00:24\n    \n    \n      19\n      39.970467\n      38.707432\n      00:24\n    \n  \n\n\n\n\nShow results\n\none_batch = dls.valid.one_batch()\nlearner.model.eval();\npred = learner.model(one_batch[0])\nb_clas, b_bb = pred\nx = one_batch[0]\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\nfor idx,ax in enumerate(axes.flat):\n    ima = x.permute(0,3,2,1).cpu()[idx]\n#     ima=md.val_ds.ds.denorm(x)[idx]\n    bbox,clas = get_y(y[0][idx], y[1][idx])\n    a_ic = actn_to_bb(b_bb[idx], anchors)\n    torch_gt(ax, ima, a_ic, b_clas[idx].max(1)[1], b_clas[idx].max(1)[0].sigmoid(), thresh=0.21)\n#plt.tight_layout()\nplt.subplots_adjust(wspace=0.15, hspace=0.15)\n\n\n\n\nThe result looks better than the simple version above"
  },
  {
    "objectID": "posts/SSD_base.html#non-maximum-suppression-nms",
    "href": "posts/SSD_base.html#non-maximum-suppression-nms",
    "title": "Object Detection from scratch - Single Shot Detector",
    "section": "Non Maximum Suppression (NMS)",
    "text": "Non Maximum Suppression (NMS)\nYou can see in the previous results, that having a lot of Anchor Boxes leads to many overlaps. You can use Non Maximum Suppression, a technique to choose one bounding box out of many overlapping ones\n\ndef nms(boxes, scores, overlap=0.5, top_k=100):\n    keep = scores.new(scores.size(0)).zero_().long()\n    if boxes.numel() == 0: return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1: break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n\n\ndef show_nmf(idx):\n    ima = one_batch[0][idx].permute(2,1,0).cpu()\n    bbox = one_batch[1][idx].cuda()\n    clas = one_batch[2][idx].cuda()\n    bbox,clas = get_y(bbox,clas)\n    \n    a_ic = actn_to_bb(b_bb[idx], anchors)\n    clas_pr, clas_ids = b_clas[idx].max(1)\n    clas_pr = clas_pr.sigmoid()\n\n    conf_scores = b_clas[idx].sigmoid().t().data\n\n    out1,out2,cc = [],[],[]\n    for cl in range(1, len(conf_scores)):\n        c_mask = conf_scores[cl] > 0.25\n        if c_mask.sum() == 0: continue\n        scores = conf_scores[cl][c_mask]\n        l_mask = c_mask.unsqueeze(1).expand_as(a_ic)\n        boxes = a_ic[l_mask].view(-1, 4)\n        ids, count = nms(boxes.data, scores, 0.4, 50)\n        ids = ids[:count]\n        out1.append(scores[ids])\n        out2.append(boxes.data[ids])\n        cc.append([cl]*count)\n    if not cc:\n        print(f\"{i}: empty array\")\n        return\n    cc = torch.tensor(np.concatenate(cc))\n    out1 = torch.cat(out1)\n    out2 = torch.cat(out2)\n\n    fig, ax = plt.subplots(figsize=(8,8))\n    torch_gt(ax, ima, out2, cc, out1, 0.1)\n\n\nfor i in range(25, 35): show_nmf(i)\n\n25: empty array\n28: empty array\n31: empty array\n32: empty array"
  },
  {
    "objectID": "posts/captcha.html",
    "href": "posts/captcha.html",
    "title": "Captcha prediction - From CNN to CRNN",
    "section": "",
    "text": "3 approaches predicting the captcha with > 95% accuracy\nDo you want a little bit more challenge than a traditional Image Classification? Let’s see if we can classify a sequence of classes rather than a single one ;).\nIn this blog post, I will try to break the captcha using 3 different approaches.\nIn this blog post:\nSpecial thanks to these references below for helping me out during this development: - Fastai Captcha Recognition by Augustas Macijauskas - CRNN-Pytorch repo by GitYCC"
  },
  {
    "objectID": "posts/captcha.html#mid-level-fastai-dataloaders",
    "href": "posts/captcha.html#mid-level-fastai-dataloaders",
    "title": "Captcha prediction - From CNN to CRNN",
    "section": "Mid-Level fastai Dataloaders",
    "text": "Mid-Level fastai Dataloaders\nIn this part, we will use the Mid-Level API fastai to load data. This tool will help us to create Dataloaders which compatible with all the fastai ecosystems\nIn brief, we will create a CaptchaTransform ( similar to a Pytorch Datasets ) which returns something showable (CaptchaImage in this case)\nTake a look at this fastai tutorial for more details\n\nfrom fastai.vision.all import *\nimport PIL\nfrom torch.nn import CTCLoss\nfrom scipy.special import logsumexp\n\n\npath = untar_data('https://github.com/AakashKumarNain/CaptchaCracker/raw/master/captcha_images_v2.zip')\n\n\nimgs = get_image_files(path)\n\n\nimgs\n\n(#1040) [Path('/home/ubuntu/.fastai/data/captcha_images_v2/by5y3.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/efb3f.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/76y6f.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/e2d66.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/c6we6.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/p2m6n.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/d66cn.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/2yggg.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/cffp4.png'),Path('/home/ubuntu/.fastai/data/captcha_images_v2/5npdn.png')...]\n\n\n\n\n\n\n\n\nNote\n\n\n\nBelow is the mapping from label to index and vice-versa. The index starts from 1 because we save the 0 for UNKNOWN class which is use in the last section CNN + RNN\n\n\n\n# Find all the unique labels\nld = set()\nfor f in imgs:\n    for l in f.stem:\n        ld.add(l)\n\nlabel_mapper = \"\".join(sorted(ld))\nl2i = { label_mapper[i]: i+1 for i in range(len(label_mapper)) } # labels to int + BLANK LABEL\ni2l = { v: k for k, v in l2i.items() } # int to labels\n\n\nl2i, i2l\n\n({'2': 1,\n  '3': 2,\n  '4': 3,\n  '5': 4,\n  '6': 5,\n  '7': 6,\n  '8': 7,\n  'b': 8,\n  'c': 9,\n  'd': 10,\n  'e': 11,\n  'f': 12,\n  'g': 13,\n  'm': 14,\n  'n': 15,\n  'p': 16,\n  'w': 17,\n  'x': 18,\n  'y': 19},\n {1: '2',\n  2: '3',\n  3: '4',\n  4: '5',\n  5: '6',\n  6: '7',\n  7: '8',\n  8: 'b',\n  9: 'c',\n  10: 'd',\n  11: 'e',\n  12: 'f',\n  13: 'g',\n  14: 'm',\n  15: 'n',\n  16: 'p',\n  17: 'w',\n  18: 'x',\n  19: 'y'})\n\n\n\ndef label_func(path): return tensor([l2i[l] for l in path.stem])\n\n\ndef open_image(fname):\n    img = PIL.Image.open(fname).convert('RGB')\n    t = torch.Tensor(np.array(img))\n    return t.permute(2,0,1).double()/255.0\n\n\nclass CaptchaImage(fastuple):\n    def show(self, ctx=None, **kwargs): \n        img,labels = self\n        t = tensor(img)\n        return show_image(t, title=''.join(str(i2l[i.item()]) for i in labels), ctx=ctx, **kwargs)\n\n\nclass CaptchaTransform(Transform):\n    def __init__(self, files):\n        self.files = files\n        \n    def encodes(self, i):\n        file = self.files[i]\n        label = label_func(file)\n        img = open_image(file)\n        return CaptchaImage(TensorImage(img), label)\n\n\nbs = 8\n\n\nsplitter = RandomSplitter(valid_pct=0.1) \ntrain_idx , valid_idx = splitter(imgs) \ntrain_files = imgs[train_idx]\nvalid_files = imgs[valid_idx]\ntrain_tl= TfmdLists(range(len(train_files)), CaptchaTransform(train_files))\nvalid_tl= TfmdLists(range(len(valid_files)), CaptchaTransform(valid_files))\ndls = DataLoaders.from_dsets(train_tl, \n                             valid_tl, \n                             after_item=Resize((50,200), method=ResizeMethod.Squish),\n                             after_batch=[Rotate(max_deg=10),\n                                          Brightness(max_lighting=0.5, p=0.8, batch=False),\n                                          Contrast(max_lighting=0.5, p=0.8, batch=False)], \n                             bs=bs)\n\n\n@typedispatch\ndef show_batch(x:CaptchaImage, y, samples, ctxs=None, max_n=6, nrows=None, ncols=2, figsize=None, **kwargs):\n    if figsize is None: figsize = (ncols*6, max_n//ncols * 3)\n    if ctxs is None: ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize)\n    for i,ctx in enumerate(ctxs): CaptchaImage(x[0][i],x[1][i]).show(ctx=ctx)\n\n\none_batch = dls.one_batch()\n\n\ndls.show_batch(max_n=10)\n\n\n\n\n\nn_chars = len(i2l)\n\n\nn_chars * 5\n\n95"
  },
  {
    "objectID": "posts/captcha.html#an-image-classification-model-with-a-tweak-on-output-dimension",
    "href": "posts/captcha.html#an-image-classification-model-with-a-tweak-on-output-dimension",
    "title": "Captcha prediction - From CNN to CRNN",
    "section": "An Image Classification Model with a tweak on output dimension",
    "text": "An Image Classification Model with a tweak on output dimension\nA simple image classification model is used here with the output dimension: Number_of_vocab x Number_of_classes. Then, while calculating the loss function, we reshape the dimension and calculating the Cross-Entropy loss for each class (The fastai CrossEntropyLossFlat can help you to specify which axis that you want to calculate the Cross-Entropy loss on)\n\n\n\nsimple tweak on output\n\n\n\nn_class = n_chars + 1\n\n\nmodel = create_cnn_model(xresnet34, n_class*5)\n\n\ncrit = LabelSmoothingCrossEntropyFlat()\n\n\ndef loss_captcha(output, target):\n    output = output.view(-1, 5, n_class)\n    return crit(output, target)\n\n\ndef char_accu(inp, targ, axis=-1):\n    inps = inp.reshape(-1, 5, n_class)\n    pred = inps.argmax(dim=-1)\n    return (pred == targ).sum()/(pred.shape[0]*pred.shape[1])\n\n\ndef captcha_accu(inp, targ, axis=-1):\n    inps = inp.reshape(-1, 5, n_class)\n    pred = inps.argmax(dim=-1)\n    return ((pred == targ).all(axis=1)).sum()/targ.shape[0]\n\n\nlearn = Learner(dls, model, loss_captcha, metrics=[char_accu, captcha_accu])\n\n\nmodel = model.cuda()\ndls = dls.cuda()\n\n\nlearn.fit_one_cycle(60, 3e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      char_accu\n      captcha_accu\n      time\n    \n  \n  \n    \n      0\n      4.417060\n      3.768889\n      0.065385\n      0.000000\n      00:05\n    \n    \n      1\n      4.238056\n      5.701259\n      0.055769\n      0.000000\n      00:05\n    \n    \n      2\n      3.933014\n      3.674879\n      0.088462\n      0.000000\n      00:05\n    \n    \n      3\n      3.719078\n      5.090882\n      0.075000\n      0.000000\n      00:05\n    \n    \n      4\n      3.548473\n      3.824108\n      0.132692\n      0.000000\n      00:05\n    \n    \n      5\n      3.366105\n      3.826407\n      0.113462\n      0.000000\n      00:05\n    \n    \n      6\n      3.136746\n      3.577558\n      0.115385\n      0.000000\n      00:05\n    \n    \n      7\n      2.936203\n      3.092070\n      0.194231\n      0.000000\n      00:05\n    \n    \n      8\n      2.729806\n      3.936151\n      0.196154\n      0.000000\n      00:05\n    \n    \n      9\n      2.521348\n      4.372227\n      0.161538\n      0.000000\n      00:05\n    \n    \n      10\n      2.263230\n      3.843181\n      0.184615\n      0.000000\n      00:05\n    \n    \n      11\n      2.063784\n      6.484088\n      0.094231\n      0.000000\n      00:05\n    \n    \n      12\n      1.915519\n      5.548406\n      0.196154\n      0.000000\n      00:05\n    \n    \n      13\n      1.772406\n      2.161972\n      0.426923\n      0.000000\n      00:05\n    \n    \n      14\n      1.659181\n      2.289798\n      0.411538\n      0.000000\n      00:05\n    \n    \n      15\n      1.480252\n      6.103688\n      0.165385\n      0.000000\n      00:05\n    \n    \n      16\n      1.440136\n      1.914036\n      0.509615\n      0.009615\n      00:05\n    \n    \n      17\n      1.336665\n      3.500629\n      0.332692\n      0.009615\n      00:05\n    \n    \n      18\n      1.275441\n      1.713563\n      0.590385\n      0.076923\n      00:05\n    \n    \n      19\n      1.219574\n      1.325677\n      0.738462\n      0.240385\n      00:05\n    \n    \n      20\n      1.165449\n      1.249834\n      0.771154\n      0.298077\n      00:05\n    \n    \n      21\n      1.132226\n      1.372405\n      0.711538\n      0.163462\n      00:05\n    \n    \n      22\n      1.072073\n      1.138467\n      0.823077\n      0.403846\n      00:05\n    \n    \n      23\n      1.045686\n      1.143437\n      0.832692\n      0.432692\n      00:05\n    \n    \n      24\n      1.027944\n      1.020135\n      0.890385\n      0.605769\n      00:05\n    \n    \n      25\n      0.980347\n      0.953016\n      0.905769\n      0.673077\n      00:05\n    \n    \n      26\n      0.955928\n      0.885346\n      0.938461\n      0.769231\n      00:05\n    \n    \n      27\n      0.938319\n      0.880523\n      0.940385\n      0.778846\n      00:05\n    \n    \n      28\n      0.912880\n      0.909565\n      0.926923\n      0.692308\n      00:05\n    \n    \n      29\n      0.888791\n      0.846983\n      0.957692\n      0.836538\n      00:05\n    \n    \n      30\n      0.877755\n      0.829840\n      0.959615\n      0.855769\n      00:05\n    \n    \n      31\n      0.854199\n      0.870150\n      0.946154\n      0.788462\n      00:05\n    \n    \n      32\n      0.838330\n      0.819317\n      0.959615\n      0.855769\n      00:05\n    \n    \n      33\n      0.824045\n      0.791964\n      0.978846\n      0.951923\n      00:05\n    \n    \n      34\n      0.821468\n      0.794070\n      0.969231\n      0.894231\n      00:05\n    \n    \n      35\n      0.803675\n      0.774516\n      0.969231\n      0.913462\n      00:05\n    \n    \n      36\n      0.800759\n      0.771277\n      0.975000\n      0.932692\n      00:05\n    \n    \n      37\n      0.784726\n      0.774723\n      0.971154\n      0.913462\n      00:05\n    \n    \n      38\n      0.776317\n      0.733620\n      0.986538\n      0.971154\n      00:05\n    \n    \n      39\n      0.767470\n      0.736605\n      0.982692\n      0.951923\n      00:05\n    \n    \n      40\n      0.757092\n      0.729704\n      0.978846\n      0.942308\n      00:05\n    \n    \n      41\n      0.755353\n      0.721849\n      0.984615\n      0.961538\n      00:05\n    \n    \n      42\n      0.754742\n      0.728808\n      0.978846\n      0.951923\n      00:05\n    \n    \n      43\n      0.748297\n      0.719470\n      0.982692\n      0.961538\n      00:05\n    \n    \n      44\n      0.742404\n      0.716713\n      0.986538\n      0.971154\n      00:05\n    \n    \n      45\n      0.735511\n      0.707033\n      0.984615\n      0.961538\n      00:05\n    \n    \n      46\n      0.735871\n      0.699644\n      0.984615\n      0.961538\n      00:06\n    \n    \n      47\n      0.728447\n      0.696534\n      0.984615\n      0.961538\n      00:05\n    \n    \n      48\n      0.723877\n      0.706259\n      0.984615\n      0.961538\n      00:05\n    \n    \n      49\n      0.728841\n      0.698290\n      0.984615\n      0.961538\n      00:05\n    \n    \n      50\n      0.721174\n      0.697634\n      0.984615\n      0.961538\n      00:05\n    \n    \n      51\n      0.720327\n      0.697556\n      0.986538\n      0.971154\n      00:05\n    \n    \n      52\n      0.714673\n      0.690376\n      0.988462\n      0.971154\n      00:06\n    \n    \n      53\n      0.717317\n      0.693437\n      0.984615\n      0.961538\n      00:05\n    \n    \n      54\n      0.714745\n      0.691280\n      0.986538\n      0.971154\n      00:05\n    \n    \n      55\n      0.715825\n      0.685640\n      0.984615\n      0.961538\n      00:06\n    \n    \n      56\n      0.711018\n      0.691839\n      0.986538\n      0.971154\n      00:05\n    \n    \n      57\n      0.713582\n      0.688716\n      0.986538\n      0.971154\n      00:06\n    \n    \n      58\n      0.713092\n      0.687872\n      0.986538\n      0.971154\n      00:06\n    \n    \n      59\n      0.711346\n      0.691615\n      0.988462\n      0.971154\n      00:05\n    \n  \n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\nWow! With this simple trick, we can reach 97% accuracy for the prediction 5-digits captcha. Very impressive! Let’s see if we can do it better with other techniques"
  },
  {
    "objectID": "posts/captcha.html#remove-the-adaptiveavgpool2d-to-reserve-spatial-information",
    "href": "posts/captcha.html#remove-the-adaptiveavgpool2d-to-reserve-spatial-information",
    "title": "Captcha prediction - From CNN to CRNN",
    "section": "Remove the AdaptiveAvgPool2d to reserve spatial information",
    "text": "Remove the AdaptiveAvgPool2d to reserve spatial information\nIn the model used above, between the body and head, there is an AdaptiveAvgPool2d layer, which blurs all essential spatial information. So let’s remove it and create our own head\n\n\n\nReceptive field with and without Adaptive Average Pooling\n\n\n\n\n\n\n\n\nIntuition\n\n\n\nFrom the Illustration above, we can see that, with AdaptiveAvgPool2d, each element in the feature vector must understand the whole Captcha, to classify correctly. To facilitate the work, by removing the Pooling layer, each feature needs to represent only a part of the Captcha, or in the best case, a letter. Combining all the letter’s features together can give us a Captcha prediction\n\n\n\nbody = create_body(xresnet34)\n\n\nhead = nn.Sequential(\n    Flatten(),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(7168,1000),\n    nn.ReLU(),\n    nn.BatchNorm1d(1000),\n    nn.Dropout(0.5),\n    nn.Linear(1000,n_class*5),\n)\n\n\nmodel = nn.Sequential(body, head)\n\n\nmodel.cuda()\ndls.cuda()\n\n<fastai.data.core.DataLoaders>\n\n\n\nlearn = Learner(dls, model, loss_captcha, metrics=[char_accu, captcha_accu])\n\n\nlearn.fit_one_cycle(60, 3e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      char_accu\n      captcha_accu\n      time\n    \n  \n  \n    \n      0\n      3.056647\n      2.770755\n      0.221154\n      0.000000\n      00:06\n    \n    \n      1\n      2.707805\n      2.455260\n      0.351923\n      0.019231\n      00:06\n    \n    \n      2\n      2.307795\n      2.130244\n      0.434615\n      0.048077\n      00:06\n    \n    \n      3\n      1.944143\n      1.862135\n      0.544231\n      0.067308\n      00:06\n    \n    \n      4\n      1.663925\n      1.757791\n      0.603846\n      0.076923\n      00:06\n    \n    \n      5\n      1.499317\n      1.516997\n      0.653846\n      0.153846\n      00:06\n    \n    \n      6\n      1.436849\n      1.711082\n      0.590385\n      0.048077\n      00:06\n    \n    \n      7\n      1.486420\n      1.472152\n      0.676923\n      0.153846\n      00:06\n    \n    \n      8\n      1.389985\n      1.436426\n      0.725000\n      0.192308\n      00:06\n    \n    \n      9\n      1.344463\n      1.969319\n      0.588462\n      0.067308\n      00:06\n    \n    \n      10\n      1.500018\n      1.610145\n      0.661538\n      0.105769\n      00:06\n    \n    \n      11\n      1.370128\n      1.287863\n      0.821154\n      0.451923\n      00:06\n    \n    \n      12\n      1.320343\n      1.476295\n      0.742308\n      0.240385\n      00:06\n    \n    \n      13\n      1.276946\n      3.231544\n      0.340385\n      0.000000\n      00:06\n    \n    \n      14\n      1.580945\n      1.563601\n      0.763462\n      0.307692\n      00:06\n    \n    \n      15\n      1.444904\n      1.174396\n      0.834615\n      0.432692\n      00:06\n    \n    \n      16\n      1.276948\n      1.223626\n      0.817308\n      0.403846\n      00:06\n    \n    \n      17\n      1.145310\n      1.043204\n      0.901923\n      0.653846\n      00:06\n    \n    \n      18\n      1.079106\n      1.131395\n      0.909615\n      0.701923\n      00:06\n    \n    \n      19\n      1.054410\n      1.013260\n      0.901923\n      0.634615\n      00:06\n    \n    \n      20\n      1.059869\n      1.099213\n      0.892308\n      0.634615\n      00:06\n    \n    \n      21\n      1.031296\n      0.993608\n      0.905769\n      0.663462\n      00:06\n    \n    \n      22\n      0.994466\n      0.873521\n      0.944231\n      0.788462\n      00:06\n    \n    \n      23\n      1.009694\n      0.865026\n      0.948077\n      0.817308\n      00:06\n    \n    \n      24\n      0.932827\n      1.105105\n      0.951923\n      0.875000\n      00:06\n    \n    \n      25\n      0.887062\n      0.852510\n      0.950000\n      0.836538\n      00:06\n    \n    \n      26\n      0.884278\n      0.817573\n      0.959615\n      0.875000\n      00:06\n    \n    \n      27\n      0.862702\n      0.815369\n      0.963462\n      0.865385\n      00:06\n    \n    \n      28\n      0.853542\n      0.814780\n      0.969231\n      0.903846\n      00:06\n    \n    \n      29\n      0.852293\n      0.793552\n      0.971154\n      0.903846\n      00:06\n    \n    \n      30\n      0.832754\n      1.187254\n      0.892308\n      0.605769\n      00:06\n    \n    \n      31\n      0.824426\n      0.905626\n      0.971154\n      0.923077\n      00:06\n    \n    \n      32\n      0.802780\n      0.747221\n      0.973077\n      0.913462\n      00:06\n    \n    \n      33\n      0.807151\n      1.262912\n      0.957692\n      0.913462\n      00:06\n    \n    \n      34\n      0.788464\n      0.744047\n      0.980769\n      0.942308\n      00:06\n    \n    \n      35\n      0.772782\n      0.724258\n      0.982692\n      0.932692\n      00:06\n    \n    \n      36\n      0.764042\n      0.716053\n      0.980769\n      0.951923\n      00:06\n    \n    \n      37\n      0.755922\n      0.726421\n      0.980769\n      0.942308\n      00:06\n    \n    \n      38\n      0.755969\n      0.716299\n      0.984615\n      0.951923\n      00:06\n    \n    \n      39\n      0.742237\n      0.709827\n      0.988461\n      0.971154\n      00:06\n    \n    \n      40\n      0.741012\n      0.700651\n      0.986538\n      0.961538\n      00:06\n    \n    \n      41\n      0.739579\n      0.746366\n      0.975000\n      0.923077\n      00:06\n    \n    \n      42\n      0.734317\n      0.740812\n      0.978846\n      0.942308\n      00:06\n    \n    \n      43\n      0.728524\n      0.689804\n      0.984615\n      0.951923\n      00:06\n    \n    \n      44\n      0.721895\n      0.686216\n      0.984615\n      0.961538\n      00:06\n    \n    \n      45\n      0.718292\n      0.680776\n      0.984615\n      0.961538\n      00:06\n    \n    \n      46\n      0.711961\n      0.675663\n      0.988462\n      0.971154\n      00:06\n    \n    \n      47\n      0.711802\n      0.678798\n      0.988461\n      0.971154\n      00:06\n    \n    \n      48\n      0.712833\n      0.678948\n      0.986538\n      0.961538\n      00:06\n    \n    \n      49\n      0.711009\n      0.678042\n      0.984615\n      0.961538\n      00:06\n    \n    \n      50\n      0.705792\n      0.671570\n      0.986538\n      0.961538\n      00:06\n    \n    \n      51\n      0.703747\n      0.669645\n      0.986538\n      0.961538\n      00:06\n    \n    \n      52\n      0.700930\n      0.668842\n      0.988462\n      0.971154\n      00:06\n    \n    \n      53\n      0.700268\n      0.667778\n      0.988461\n      0.971154\n      00:06\n    \n    \n      54\n      0.698437\n      0.673563\n      0.988461\n      0.971154\n      00:06\n    \n    \n      55\n      0.702755\n      0.665972\n      0.988462\n      0.961538\n      00:06\n    \n    \n      56\n      0.699661\n      0.668209\n      0.988461\n      0.971154\n      00:06\n    \n    \n      57\n      0.695964\n      0.666184\n      0.988461\n      0.971154\n      00:06\n    \n    \n      58\n      0.697385\n      0.665079\n      0.992308\n      0.980769\n      00:06\n    \n    \n      59\n      0.702616\n      0.668129\n      0.988462\n      0.961538\n      00:06\n    \n  \n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\nWe have a quite similar result to the previous model after 60 epochs. However, this one learns much faster. After 15 epochs, it attains already 43% captcha accuracy while the With AdaptiveAvgPool2d is still at 0%"
  },
  {
    "objectID": "posts/captcha.html#crnn-ctc-loss",
    "href": "posts/captcha.html#crnn-ctc-loss",
    "title": "Captcha prediction - From CNN to CRNN",
    "section": "CRNN + CTC Loss",
    "text": "CRNN + CTC Loss\nOne can imagine, from the intuition of the last section, if we can extract features from letters and then predict the captcha, How about using a Recurrent Neural Network (RNN)? Is it for solving sequence problems right?\nYes, yes, It is the CRNN.\n\n\n\nCRNN Model\n\n\n\n\n\n\n\n\nNote\n\n\n\nFeel free to run the model step by step through each layer to understand better the dimension\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Sequence Length of the output doesn’t necessarily equal to the Captcha Length (which is 5 in our case) because our loss function CTC Loss knows how to handle it\n\n\nThe CNN-Body model I use here is resnet34, but not the entire one. We cut it after some layers. The reason is, the deeper the image passes through the CNN the more its Width shrink, and it can not be smaller than our captcha length (which is 5). Below you can see I choose to cut after 7 layers so the feature’s Width is still 13 (the last dimension of the tensor)\n\nbody = create_body(resnet34, cut=7)\n\n\nn_class = n_chars + 1\n\n\nn_class\n\n20\n\n\nBy running the CNN body manually, I can know the number of output features, height and width which will be used later for building the CRNN model\n\nbody(one_batch[0].cpu()).shape\n\ntorch.Size([8, 256, 4, 13])\n\n\n\nclass CRNN(nn.Module):\n    def __init__(self, output_channel, H, W, n_class, map_to_seq_hidden=64, rnn_hidden_1=256, rnn_hidden_2=128):\n        super(CRNN, self).__init__()\n        self.body = create_body(resnet34, cut=7)\n        self.map_to_seq = LinBnDrop(output_channel * H, map_to_seq_hidden, p=0.1, bn=False, lin_first=False)\n        self.rnn1 = nn.LSTM(map_to_seq_hidden, rnn_hidden_1, bidirectional=True)\n        self.rnn2 = nn.LSTM(2 * rnn_hidden_1, rnn_hidden_2, bidirectional=True)\n        self.dense = LinBnDrop(2 * rnn_hidden_2, n_class, p=0.1, bn=False, lin_first=False)\n        \n    def forward(self, images):\n        # shape of images: (batch, channel, height, width)\n\n        conv = self.body(images)\n        batch, channel, height, width = conv.size()\n\n        conv = conv.view(batch, channel * height, width)\n        conv = conv.permute(2, 0, 1)  # (width, batch, feature)\n        seq = self.map_to_seq(conv)\n\n        recurrent, _ = self.rnn1(seq)\n        recurrent, _ = self.rnn2(recurrent)\n\n        output = self.dense(recurrent)\n        return output  # shape: (seq_len, batch, num_class)\n\n\noutput_channel = 256\nH = 4\nW = 13\n\n\nmodel = CRNN(output_channel, H, W, n_class)\n\n\nsteps = W\n\nThe loss function we use here is CTC Loss. In brief, it is a loss function that can handle a sequence classification without a specific alignment (Because we don’t have a character-level dataset and the span of each character is random). An Illustration is below (Taken from https://sid2697.github.io/Blog_Sid/algorithm/2019/10/19/CTC-Loss.html). In CTC Loss, it allows a repeated prediction with a character that spans through multiple positions and also a blank character. However, we need a decoder to decode later the output. I will talk about it in the next section\n\n\n\nCTC Loss Illustration\n\n\n\ncriterion = nn.CTCLoss()\n\n\ndef loss_captcha_ctc(output, target):\n    batch_size = target.shape[0]\n    input_lengths = torch.LongTensor([steps] * batch_size)\n    target_lengths = torch.LongTensor([5] * batch_size)\n    log_probs = torch.nn.functional.log_softmax(output, dim=2)\n    return criterion(log_probs, target, input_lengths, target_lengths)\n\n\nDecoder and Metrics\nAs the output of CRNN model is not exactly corresponding to the Groud-Truth, we must have something to decode it and get the final prediction. Check this tutorial for more details https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7. Below there is code for the greedy and beam-search technique\n\nblank = 0\nbeam_size = 10\n\n\nNINF = -1 * float('inf')\nDEFAULT_EMISSION_THRESHOLD = 0.01\n\n\ndef _reconstruct(labels, blank=0):\n    new_labels = []\n    # merge same labels\n    previous = None\n    for l in labels:\n        if l != previous:\n            new_labels.append(l)\n            previous = l\n    # delete blank\n    new_labels = [l for l in new_labels if l != blank]\n    return new_labels\n\n\ndef greedy_decode(emission_log_prob, blank=0, **kwargs):\n    labels = np.argmax(emission_log_prob, axis=-1)\n    labels = _reconstruct(labels, blank=blank)\n    return labels\n\n\ndef beam_search_decode(emission_log_prob, blank=0, **kwargs):\n    beam_size = kwargs['beam_size']\n    emission_threshold = kwargs.get('emission_threshold', np.log(DEFAULT_EMISSION_THRESHOLD))\n\n    length, class_count = emission_log_prob.shape\n\n    beams = [([], 0)]  # (prefix, accumulated_log_prob)\n    for t in range(length):\n        new_beams = []\n        for prefix, accumulated_log_prob in beams:\n            for c in range(class_count):\n                log_prob = emission_log_prob[t, c]\n                if log_prob < emission_threshold:\n                    continue\n                new_prefix = prefix + [c]\n                # log(p1 * p2) = log_p1 + log_p2\n                new_accu_log_prob = accumulated_log_prob + log_prob\n                new_beams.append((new_prefix, new_accu_log_prob))\n\n        # sorted by accumulated_log_prob\n        new_beams.sort(key=lambda x: x[1], reverse=True)\n        beams = new_beams[:beam_size]\n\n    # sum up beams to produce labels\n    total_accu_log_prob = {}\n    for prefix, accu_log_prob in beams:\n        labels = tuple(_reconstruct(prefix, blank))\n        # log(p1 + p2) = logsumexp([log_p1, log_p2])\n        total_accu_log_prob[labels] = \\\n            logsumexp([accu_log_prob, total_accu_log_prob.get(labels, NINF)])\n\n    labels_beams = [(list(labels), accu_log_prob)\n                    for labels, accu_log_prob in total_accu_log_prob.items()]\n    labels_beams.sort(key=lambda x: x[1], reverse=True)\n    labels = labels_beams[0][0]\n    return labels\n\nAs we can have a prediction from the decoder that has length > 5, we don’t have character level accuracy for the metrics but only whole captcha accuracy\n\ndef captcha_accu_ctc(pred, targ, axis=-1):\n    log_probs = torch.nn.functional.log_softmax(pred, dim=2)\n    emission_log_probs = np.transpose(log_probs.detach().cpu().numpy(), (1, 0, 2))\n    decoder = beam_search_decode\n    label2char = i2l\n    \n    decoded_list = []\n    for emission_log_prob in emission_log_probs:\n        decoded = decoder(emission_log_prob, blank=blank, beam_size=beam_size)\n        decoded_list.append(decoded)\n    \n    count_ok = 0\n    for decode, gt in zip(decoded_list, targ):\n        if len(decode) == len(gt):\n            count_ok += (torch.tensor(decode).cuda() == gt).all().item()\n            \n    return count_ok/targ.shape[0]\n\n\n# loss_captcha_ctc(pred, one_batch[1])\n\n\ndls = dls.cuda()\nmodel = model.cuda()\n\n\nlearn = Learner(dls, model, loss_func=loss_captcha_ctc, metrics=[captcha_accu_ctc])\n# learn = Learner(dls, model, loss_func=loss_captcha_ctc)\n\n\nlearn.fit_one_cycle(1,1e-6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      captcha_accu_ctc\n      time\n    \n  \n  \n    \n      0\n      0.001245\n      0.070678\n      0.961538\n      00:06\n    \n  \n\n\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0014454397605732083)\n\n\n\n\n\n\nlearn.fit_one_cycle(60, 3e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      captcha_accu_ctc\n      time\n    \n  \n  \n    \n      0\n      3.364362\n      3.221772\n      0.000000\n      00:05\n    \n    \n      1\n      3.239760\n      3.222240\n      0.000000\n      00:05\n    \n    \n      2\n      3.198520\n      3.179216\n      0.000000\n      00:05\n    \n    \n      3\n      3.068399\n      3.082180\n      0.000000\n      00:05\n    \n    \n      4\n      2.935494\n      2.940528\n      0.000000\n      00:05\n    \n    \n      5\n      2.778541\n      2.834429\n      0.000000\n      00:05\n    \n    \n      6\n      2.595863\n      2.414017\n      0.000000\n      00:05\n    \n    \n      7\n      2.398474\n      2.306193\n      0.000000\n      00:05\n    \n    \n      8\n      2.133149\n      2.030112\n      0.009615\n      00:05\n    \n    \n      9\n      1.933357\n      2.035026\n      0.000000\n      00:05\n    \n    \n      10\n      1.740602\n      3.510546\n      0.000000\n      00:05\n    \n    \n      11\n      1.571230\n      1.606472\n      0.000000\n      00:05\n    \n    \n      12\n      1.481388\n      1.380839\n      0.028846\n      00:05\n    \n    \n      13\n      1.363306\n      1.311289\n      0.028846\n      00:05\n    \n    \n      14\n      1.196686\n      1.012929\n      0.125000\n      00:05\n    \n    \n      15\n      0.932915\n      0.713615\n      0.192308\n      00:05\n    \n    \n      16\n      0.730192\n      0.596288\n      0.278846\n      00:05\n    \n    \n      17\n      0.589729\n      0.484889\n      0.442308\n      00:05\n    \n    \n      18\n      0.393728\n      0.315344\n      0.567308\n      00:05\n    \n    \n      19\n      0.335306\n      0.282954\n      0.663462\n      00:05\n    \n    \n      20\n      0.253060\n      0.164563\n      0.826923\n      00:05\n    \n    \n      21\n      0.192399\n      0.202485\n      0.778846\n      00:05\n    \n    \n      22\n      0.152099\n      0.170849\n      0.788462\n      00:05\n    \n    \n      23\n      0.165517\n      0.182350\n      0.769231\n      00:05\n    \n    \n      24\n      0.143058\n      0.116091\n      0.875000\n      00:05\n    \n    \n      25\n      0.140082\n      0.098341\n      0.884615\n      00:05\n    \n    \n      26\n      0.107128\n      0.153724\n      0.846154\n      00:05\n    \n    \n      27\n      0.104198\n      0.064582\n      0.903846\n      00:05\n    \n    \n      28\n      0.075773\n      0.164378\n      0.826923\n      00:05\n    \n    \n      29\n      0.073986\n      0.095392\n      0.942308\n      00:05\n    \n    \n      30\n      0.121426\n      0.098069\n      0.875000\n      00:05\n    \n    \n      31\n      0.061526\n      0.085097\n      0.913462\n      00:05\n    \n    \n      32\n      0.050841\n      0.033660\n      0.961538\n      00:05\n    \n    \n      33\n      0.041742\n      0.113056\n      0.884615\n      00:05\n    \n    \n      34\n      0.037825\n      0.035997\n      0.980769\n      00:05\n    \n    \n      35\n      0.042165\n      0.057766\n      0.923077\n      00:05\n    \n    \n      36\n      0.040758\n      0.056158\n      0.961538\n      00:05\n    \n    \n      37\n      0.013569\n      0.063323\n      0.951923\n      00:05\n    \n    \n      38\n      0.017145\n      0.041725\n      0.951923\n      00:05\n    \n    \n      39\n      0.016637\n      0.050826\n      0.951923\n      00:05\n    \n    \n      40\n      0.015019\n      0.039701\n      0.971154\n      00:05\n    \n    \n      41\n      0.007477\n      0.036252\n      0.980769\n      00:05\n    \n    \n      42\n      0.017243\n      0.036068\n      0.980769\n      00:05\n    \n    \n      43\n      0.011176\n      0.045191\n      0.951923\n      00:05\n    \n    \n      44\n      0.010634\n      0.045656\n      0.971154\n      00:05\n    \n    \n      45\n      0.005593\n      0.048920\n      0.961538\n      00:05\n    \n    \n      46\n      0.002363\n      0.049634\n      0.961538\n      00:05\n    \n    \n      47\n      0.005349\n      0.049498\n      0.961538\n      00:05\n    \n    \n      48\n      0.012180\n      0.036904\n      0.971154\n      00:05\n    \n    \n      49\n      0.001877\n      0.035886\n      0.980769\n      00:05\n    \n    \n      50\n      0.002923\n      0.043079\n      0.971154\n      00:05\n    \n    \n      51\n      0.002380\n      0.034152\n      0.980769\n      00:05\n    \n    \n      52\n      0.002510\n      0.038204\n      0.980769\n      00:05\n    \n    \n      53\n      0.001762\n      0.034696\n      0.980769\n      00:05\n    \n    \n      54\n      0.002121\n      0.037318\n      0.980769\n      00:05\n    \n    \n      55\n      0.000552\n      0.043198\n      0.971154\n      00:05\n    \n    \n      56\n      0.000387\n      0.041396\n      0.980769\n      00:05\n    \n    \n      57\n      0.001379\n      0.037345\n      0.980769\n      00:05\n    \n    \n      58\n      0.002729\n      0.040222\n      0.971154\n      00:05\n    \n    \n      59\n      0.000564\n      0.051966\n      0.971154\n      00:05\n    \n  \n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\nOk, we have good results too. As it is a very simple dataset, it’s hard to say which technique is better. However, the CRNN training loss is much lower than validation loss, It might be a hint that we can tune it to get more accuracy or training faster? Please DM me if you have an idea about it. Thanks"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dhblog",
    "section": "",
    "text": "computer-vision\n\n\ndeeplearning\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\nDien-Hoa Truong\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2022\n\n\nDien-Hoa Truong\n\n\n\n\n\n\nNo matching items"
  }
]